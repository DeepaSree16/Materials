{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M2_NB_MiniProject_1_Calculus_for_Linear_Regression.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"gOQv69UdcrIK"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Mini-Project: Calculus for Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"sNCzT95pcrIN"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"IMBrLtAtcrIO"},"source":["At the end of the mini-project, you will be able to :\n","\n","* perform Linear Regression using different optimization algorithms - full batch gradient descent, RMSProp, Adam, Momentum"]},{"cell_type":"markdown","metadata":{"id":"4Pped-3NPaDV"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"u4lUpJv8Pjj9"},"source":["### Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"UZvjd9p3PfRO"},"source":["Linear regression assumes a linear or straight line relationship between the input variables (X) and the single output variable (y). More specifically, that output (y) can be calculated from a linear combination of the input variables (X). When there is a single input variable, the method is referred to as a simple linear regression; for more than one, the process is called multiple linear regression.\n","\n","Here we limit to 2-dimensional space, thus a Cartesian plane. Let us develop gradually from the ground up starting with y=mx format and then y=mx+c regression."]},{"cell_type":"markdown","metadata":{"id":"R7ZV5ryMP07f"},"source":["##  Grading = 10 Points"]},{"cell_type":"markdown","metadata":{"id":"qpLOQr6gu-rR"},"source":["Marks:\n","\n","Exercise 1, 2, 5: Total marks = 0.5 x 3 = 1.5\n","\n","Exercise 3, 4, 6, 7: Total marks = 1 x 4 = 4\n","\n","Exercise 8, 9, 10: Total marks = 1.5 x 3 = 4.5"]},{"cell_type":"markdown","metadata":{"id":"PKJUrIYnJ33S"},"source":["#### Import required packages"]},{"cell_type":"code","metadata":{"id":"k-jOtWe9_reB"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XG-iVLKSr1QO"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"TNu_AbYxr1QO"},"source":["We will use Sweden insurance dataset to demonstrate simple linear regression. The dataset is called the “Auto Insurance in Sweden” dataset and involves predicting the **total payment for all the claims in thousands of Swedish Kronor (y)** given the total **number of claims (x)**."]},{"cell_type":"markdown","metadata":{"id":"BZ59LrKRr1QP"},"source":["**Exercise 1: Read the swedish_insurance.csv dataset and visualize total payment (y) vs number of claims (x).**\n","\n","**Hint:** pd.read_csv()"]},{"cell_type":"code","metadata":{"id":"hvpwpuX9r1QQ","cellView":"form"},"source":["#@title Download Dataset\n","!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/swedish_insurance.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kA_wJ4qTr1QQ"},"source":["# Read dataset\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N3IuJPuWr1QQ"},"source":["# Visualize y vs x\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lq54j_42Q2VA"},"source":["### Simplified Scenario of $y = mx$"]},{"cell_type":"markdown","metadata":{"id":"z8XDUlc-qdDI"},"source":["In this case, we are going to fit a line to data that passes through the origin. Let's develop the loss functions and see how it behaves.\n","\n","$$y = mx \\rightarrow h_{\\theta}(x) = \\theta x$$\n","\n","$$J(\\theta) = \\frac{1}{2n} \\sum_1^n (h_{\\theta}(x_i) - y_i)^2$$\n","\n","Note that we use the squared error divided by 2n where n is the number of data points. Therefore, we can consider this as a mean value. Precisely, this is the half of mean squared error. The intuition behind the division by 2 helps to have a more simplified derivate for the loss function.\n","\n","Now if we plot the loss function for varying θ values we would get the plot shown below."]},{"cell_type":"markdown","metadata":{"id":"M2w0j2Y3XeAM"},"source":["**Exercise 2: Create and plot the loss function for varying θ values.**\n","\n","* create a function to compute mean squared error loss\n","$$J(\\theta) = \\frac{1}{2n} \\sum_1^n (\\theta x_i - y_i)^2$$\n","* compute loss for different theta values (50 points between 0 and 10)\n","* plot the computed loss corresponding to theta values\n","\n","**Hint:** np.linspace()"]},{"cell_type":"code","metadata":{"id":"CipqnwxIuwq4"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZQc3b3uBqyvg"},"source":["Now we have an approximate value for the minimum of the loss function. Let us see how we can arrive at this particular minima computationally."]},{"cell_type":"markdown","metadata":{"id":"SlQKSfNRq1GD"},"source":["### The Gradient Descent\n","\n","Gradient descent is an optimization algorithm that is used when training a machine learning model. It is based on a convex function and tweaks its parameters iteratively to minimize a given function to its local minimum.\n","\n","The learning rate represents the step size and is denoted as $\\alpha$. Smaller $\\alpha$ value denotes the smaller step and slower the algorithm. However, taking a larger step could make us miss the minima. \n","\n","So we can formulate the change of θ as follows:\n","\n","$$\\theta = \\theta - \\alpha \\frac{∂}{∂\\theta}J(\\theta)$$\n","\n","where, \n","$$\\frac{∂}{∂\\theta}J(\\theta) = \\frac{∂}{∂\\theta} \\frac{1}{2n} \\sum_1^n (h_{\\theta}(x_i) - y_i)^2$$\n","\n","$$\\frac{∂}{∂\\theta}J(\\theta) = \\frac{∂}{∂\\theta} \\frac{1}{2n} \\sum_1^n (\\theta x_i - y_i)^2$$\n","\n","$$\\frac{∂}{∂\\theta}J(\\theta) = \\frac{1}{n} \\sum_1^n (\\theta x_i - y_i)x_i$$\n"]},{"cell_type":"markdown","metadata":{"id":"ExemBfWl4wiG"},"source":["**Exercise 3: Create a function to evaluate the derivative of the loss function with respect to θ then perform gradient descent.**\n","\n","* create a gradient function\n","* perform gradient descent for n number of iterations\n","* visualize the updates of theta\n","* tune learning rate and number of iterations for faster convergence\n","\n","Hint: [Calculus behind Linear Regression](https://towardsdatascience.com/calculus-behind-linear-regression-1396cfd0b4a9)."]},{"cell_type":"code","metadata":{"id":"g3qEANoVrKee"},"source":["# Create a gradient function\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9Ka8Wzu3rNYt"},"source":["Perform gradient descent iteratively until we meet minima or the number of iterations is reached."]},{"cell_type":"code","metadata":{"id":"o9NZYVBprPTY"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAt4YTYNrRUc"},"source":["Visualize the gradient traversal:"]},{"cell_type":"code","metadata":{"id":"hMSYSAcerTqS"},"source":["# Visualize the updates of theta\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSKYCC-c7NtQ"},"source":["**Exercise 4: Visualize the final regression line along with the intermediate lines.**\n","\n","* plot a regression line for every updated theta\n","* plot final regression line using the last theta value"]},{"cell_type":"code","metadata":{"id":"yE1dXfZW1lyu"},"source":["# Visualize updates of Full Batch Gradient descent\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AaGym69p80dk"},"source":["### Complete Scenario $y = mx + c$"]},{"cell_type":"markdown","metadata":{"id":"eTVxgoHcrVpq"},"source":["This is the extension of the previous case and we can model the estimated equation and the loss function as follows:\n","\n","$$y = mx + c \\rightarrow h_{\\theta}(x) = \\theta_1 + \\theta_2 x$$\n","\n","$$J(\\theta_1, \\theta_2) = \\frac{1}{2n} \\sum_1^n (h_{\\theta}(x_i) - y_i)^2$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"-vryoI2KrjUm"},"source":["Now that we have a loss function with 2 variables, the loss function will be a 3D plot with the third axis corresponding to the loss value."]},{"cell_type":"markdown","metadata":{"id":"n9wBbYfx-EVT"},"source":["**Exercise 5: Create and plot the loss function for varying θ1 and θ2 values.**"]},{"cell_type":"markdown","metadata":{"id":"mBM9oxJX1C4A"},"source":["**Hint:** np.arange(), np.meshgrid(), plt.axes(projection='3d'), ax.contour3D()"]},{"cell_type":"code","metadata":{"id":"KpUNicV_roeL"},"source":["# Create loss function\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hDkjIOwrr1QU"},"source":["# Visualize loss function\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ofOmQXRro-m"},"source":["The gradient descent can be similarly derived to reach the following set of equations:\n","\n","$$\\theta_1 = \\theta_1 - \\alpha \\frac{∂}{∂\\theta_1}J(\\theta_1,\\theta_2) $$\n","\n","$$\\theta_2 = \\theta_2 - \\alpha \\frac{∂}{∂\\theta_2}J(\\theta_1,\\theta_2) $$\n","\n","$$\\frac{∂}{∂\\theta_1}J(\\theta_1, \\theta_2) = \\frac{1}{n} \\sum_1^n (\\theta_1 + \\theta_2 x_i - y_i)$$\n","\n","$$\\frac{∂}{∂\\theta_2}J(\\theta_1, \\theta_2) = \\frac{1}{n} \\sum_1^n (\\theta_1 + \\theta_2 x_i - y_i)x_i$$\n","\n","Note that the gradients must be updated simultaneously so that update of one θ value will not affect the other."]},{"cell_type":"markdown","metadata":{"id":"Hi6p5VXqbEFd"},"source":["**Exercise 6: Create a function to evaluate the derivative of the loss function with respect to θ1 and θ2 then perform gradient descent.**\n","\n","* create one gradient function for each theta1 and theta2 \n","* perform gradient descent for n number of iterations\n","* visualize the updates of theta1 and theta2\n","* tune learning rate and number of iterations for faster convergence"]},{"cell_type":"code","metadata":{"id":"7LqofkIlbr6N"},"source":["# Create gradient functions w.r.t theta1 and theta2\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4KVzQGN2j4Im"},"source":["Run gradient descent iteratively until we meet minima or the number of iterations is reached."]},{"cell_type":"code","metadata":{"id":"tmaBtDjodPa9"},"source":["# Perform gradient descent\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cw2kBcN5r_0D"},"source":["Visualize the descent over the gradient of the loss function as follows:"]},{"cell_type":"code","metadata":{"id":"oiOxLE16sCgW"},"source":["# Visualize the updates of theta1, theta2\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tA-IBoH2kdS1"},"source":["**Exercise 7: Visualize the final regression line along with the intermediate lines.**\n","\n","* plot a regression line for every updated theta1 and theta2\n","* plot final regression line using the last theta1 and theta2 values"]},{"cell_type":"code","metadata":{"id":"8-4QjQ_asJCZ"},"source":["# Visualize updates of Gradient descent\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0OxVawyNuVC"},"source":["# Parameter values\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SC4rnoN9FlJ-"},"source":["### RMSProp \n","\n","Using RMSProp optimization to find the minima of the cost function.\n","\n","Weights update is given as: \n","\n","$$E[g^2]_t = \\beta E[g^2]_{t-1} + (1-\\beta)(\\frac{∂c}{∂w})^2$$\n","\n","$$w_t = w_{t-1} - \\frac{\\eta}{\\sqrt{E[g^2]_t + \\epsilon}}(\\frac{∂c}{∂w})$$\n","\n","where $E[g^2]$ is the moving average of squared gradients, $∂c/∂w$ is gradient of the cost function with respect to the weight, $\\eta$ is the learning rate and $\\beta$ is moving average parameter (default value 0.9). The $\\epsilon$ is a small scalar (e.g. $10^{-8}$) used to prevent division by 0."]},{"cell_type":"markdown","metadata":{"id":"3gYnS6yzlwsi"},"source":["**Exercise 8: Perform the optimization steps using RMSProp optimization.**\n","\n","* perform rmsprop optimization for n number of iterations\n","* visualize the updates of theta1 and theta2\n","* tune learning rate and number of iterations for faster convergence\n","* plot a regression line for every updated theta1 and theta2\n","* plot final regression line using the last theta1 and theta2 values"]},{"cell_type":"code","metadata":{"id":"Cs_rUCKHyY05"},"source":["# RMSProp optimization\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLMXjMQpscAu"},"source":["# Visualize the updates of theta1, theta2\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OK7DNo8osb91"},"source":["# Visualize updates of RMSProp\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s72FIgpi1iBX"},"source":["# Parameter values\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P-i_tcw6HpQc"},"source":["### Adam\n","\n","Using Adam optimization to find the minima of the cost function.\n","\n","Weights update is given as: \n","\n","$$m_w^{(t+1)} \\leftarrow \\beta_1m_w^{(t)} + (1-\\beta_1)∇_wL^{(t)},$$\n","\n","$$v_w^{(t+1)} \\leftarrow \\beta_2v_w^{(t)} + (1-\\beta_2)(∇_wL^{(t)})^2,$$\n","\n","$$\\hat{m}_w = \\frac{m_w^{(t+1)}}{1-\\beta_1^{t+1}},$$  \n","\n","$$\\hat{v}_w = \\frac{v_w^{(t+1)}}{1-\\beta_2^{t+1}},$$\n","\n","$$w^{(t+1)} \\leftarrow w^{(t)} - \\eta\\frac{\\hat{m}_w}{\\sqrt{\\hat{v}_w}+\\epsilon}$$\n","\n","where $\\epsilon$  is a small scalar (e.g. $10^{-8}$) used to prevent division by 0, and $\\beta _{1}$ (e.g. 0.9) and $\\beta _{2}$ (e.g. 0.999) are the forgetting factors for gradients and second moments of gradients, respectively. Squaring and square-rooting is done elementwise."]},{"cell_type":"markdown","metadata":{"id":"orqdM8ros3fo"},"source":["**Exercise 9: Perform the optimization steps using Adam optimization.**\n","\n","* perform adam optimization for n number of iterations\n","* visualize the updates of theta1 and theta2\n","* tune learning rate and number of iterations for faster convergence\n","* plot a regression line for every updated theta1 and theta2\n","* plot final regression line using the last theta1 and theta2 values"]},{"cell_type":"code","metadata":{"id":"1fTVs0zP2AM0"},"source":["# Adam optimization\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nK-wTurB14wo"},"source":["# Visualize the updates of theta1, theta2\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWjB3b8T2D4Z"},"source":["# Visualize updates of Adam\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JR8e7Q7O1kH1"},"source":["# Parameter values\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BYIeVxiFr1QX"},"source":["### Momentum optimization\n","\n","Using Momentum optimization to find the minima of the cost function.\n","\n","Momentum involves maintaining the change in the position and using it in the subsequent calculation of the change in position.\n","\n","If we think of updates over time, then the update at the current iteration or time (t) will add the change used at the previous time (t-1) weighted by the momentum hyperparameter, as follows:\n","\n","$$change\\_w_t = step\\_size * f'(w_{t-1})\\ +\\ momentum * change\\_w_{t-1}$$\n","\n","The update to the position is then performed as before.\n","\n","$$w_t = w_{t-1}\\ –\\ change\\_w_t$$\n","\n","The change in the position accumulates the magnitude and direction of changes over the iterations of the search, proportional to the size of the momentum hyperparameter."]},{"cell_type":"markdown","metadata":{"id":"m5K2tHj_r1QX"},"source":["**Exercise 10: Perform the optimization steps using Momentum optimization.**\n","\n","* perform momentum optimization for n number of iterations\n","* visualize the updates of theta1 and theta2\n","* tune learning rate and number of iterations for faster convergence\n","* plot a regression line for every updated theta1 and theta2\n","* plot final regression line using the last theta1 and theta2 values"]},{"cell_type":"code","metadata":{"id":"CAXFjKvbr1QX"},"source":["# Momentum optimization\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RYfLMIjxr1QX"},"source":["# Visualize the updates of theta1, theta2\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AgHbZxTcr1QX"},"source":["# Visualize updates of Momentum optimizer\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IPlkNUlcr1QY"},"source":["# Parameter values\n","\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWzqUV2jr1QY"},"source":["Discussions:\n","\n","* Compare the RMSProp and Adam optimizers.\n","* What will happen if we take the different learning rates for theta1 and theta2 for all optimizers?\n","* What is the significance of momentum factor in Momentum optimization?"]}]}