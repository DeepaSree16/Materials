{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M2_NB_(ungraded)_Numpy_autograd_Demo.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ik5OVg0Xf8h2"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Demo Notebook: Autograd"]},{"cell_type":"markdown","metadata":{"id":"rubber-supplement"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"cShVnSEof74m"},"source":["At the end of the experiment, you will be able to\n","\n","* understand and implement linear regression model and calculate gradient loss function using autograd"]},{"cell_type":"markdown","metadata":{"id":"2zUMjagYf7hG"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"O2sP8Kwe7L9Z"},"source":["**Goal:** Fit the parameters (slope and intercept) of a simple linear regression model via gradient descent (GD), using Autograd \n","\n","**Dataset:** The Dataset consists of age vs systolic blood pressure measurements of 33 American women \n","\n","**Task-flow:**\n","\n","* fit a linear model via the sklearn machine learning library of python to get the fitted values of the intercept and slope as reference. \n","\n","* use the autograd library and the contained *grad* function to fit the parameters of the simple linear model via GD with the objective to minimize the MSE loss. \n","    * define the mse loss function \n","    * determine the gradients of the loss w.r.t. the parameters via automatic differentiation\n","    * use these gradients to update the parameter values via the update formula\n","    * iterate over the two former steps for many steps and check the current values of the estimated model parameters and the loss after each update step \n","    * verify that the estimated parameter values converge to the values which you got from the sklearn fit.  "]},{"cell_type":"code","metadata":{"id":"RMlU4PJJ5QO7"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","plt.style.use('default')\n","# from sklearn importing linear regression model\n","from sklearn.linear_model import LinearRegression"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p3c9bh7zMVhP"},"source":["Here we read in the systolic blood pressure and the age of the 33 American women in our dataset. Then we use the sklearn library to find the optimal values for the slope a and the intercept b."]},{"cell_type":"code","metadata":{"id":"RvINwW1vydo9"},"source":["# Blood Pressure data\n","x = [22, 41, 52, 23, 41, 54, 24, 46, 56, 27, 47, 57, 28, 48, 58,  9, \n","     49, 59, 30, 49, 63, 32, 50, 67, 33, 51, 71, 35, 51, 77, 40, 51, 81]\n","y = [131, 139, 128, 128, 171, 105, 116, 137, 145, 106, 111, 141, 114, \n","     115, 153, 123, 133, 157, 117, 128, 155, 122, 183,\n","     176,  99, 130, 172, 121, 133, 178, 147, 144, 217] \n","# Convert the input to an array\n","x = np.asarray(x, np.float32) \n","y = np.asarray(y, np.float32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OurbybEoydpB"},"source":["# A scatter plot of y (Blood Pressure) vs. x (age)\n","plt.scatter(x=x,y=y)\n","plt.title(\"blood pressure vs age\")\n","plt.xlabel(\"x (age)\")\n","plt.ylabel(\"y (sbp)\")\n","\n","# importing linear regression model\n","model = LinearRegression()\n","# fit training data, for supervised learning applications, this accepts two arguments: the data X and the labels y\n","res = model.fit(x.reshape((len(x),1)), y)         # here .reshape() changes the data shape to a 1D matrix or column   \n","predictions = model.predict(x.reshape((len(x),1)))          \n","plt.plot(x, predictions)\n","plt.show()\n","print(\"intercept = \",res.intercept_,\"slope = \", res.coef_[0],)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qDre_3MNSFtJ"},"source":["## Autograd"]},{"cell_type":"markdown","metadata":{"id":"Pu3bqKzRDQqw"},"source":["Now we want to use Autograd, a library for automatic differentiation. First we need to install it. Then we again can define our mse loss and calculate the minimal loss with the optimal values for the slope a and the inercept b from above."]},{"cell_type":"code","metadata":{"id":"gas_UQxrDOFh"},"source":["!pip install autograd        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"syWi2amOR-pE"},"source":["import autograd.numpy as np  # automatically differentiate native Python and Numpy code\n","from autograd import grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jyunhm7wR-9G"},"source":["# defining a loss function as Mean Square Error\n","def loss(a,b):\n","  y_hat = a*x + b\n","  return np.sum((y_hat - y)**2) / len(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yaLCW1AcX6pr"},"source":["loss(1.1050216,87.67143)    #minimal loss for the optimal values for slope a and intercept b"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OU3rc92DEZmv"},"source":["Now we define that we want to have the gradients of the loss w.r.t to our two model parameters, the slope a and the intercept b. In the next cell we print the gradient of the loss w.r.t to a and gradient of the loss w.r.t to b. Note that we calculated the loss for all data points and therefore we get diffrent gradients then in nb_04, where we only used one datapoint. Autograds *grad* function, takes a function as input and returns a function that computes its derivative. You can use the derivative function to compute the gradient at a specific position of the loss function."]},{"cell_type":"code","metadata":{"id":"lltyH-eRUPSn"},"source":["# calculating gradient of loss function\n","grad_loss_a = grad(loss,0)\n","grad_loss_b = grad(loss,1)\n","print(grad_loss_a(0.,139.))\n","print(grad_loss_b(0.,139.))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nrmK9PP0El3L"},"source":["Now, let's use gradient descent to optimize the slope a and the intercept b. The start values are a=0 and b=139  (139 is the mean of the blood pressure and slope a=0 implies that the model predicts the mean for each age). Our learning rate eta is 0.0004 and we do 80000 update steps with all 33 observations. "]},{"cell_type":"code","metadata":{"id":"8odVy_28U39q"},"source":["eta = 0.0004    # learning rate\n","a = 0.0         # inital guess for slope\n","b = 139.0       # initial guess for intercept\n","\n","# making a for loop with 80000 iterations for finding the optimal values of a and b \n","for i in range(80000):\n","    grad_a, grad_b  = grad_loss_a(a,b),grad_loss_b(a,b)\n","    a = a - eta * grad_a\n","    b = b - eta * grad_b\n","    if (i % 5000 == 0):\n","      print(\"Epoch:\",i, \"slope=\",a,\"intercept=\",b,\"gradient_a\", grad_a, \"gradient_b\",grad_b, \"mse=\", loss(a,b))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nBEvvFxtFvg6"},"source":["Let's look at the final values for the slope a, the intercept b and the mse loss. We know form the closed formula solution that:\n","\n","1.   optimal value for a: 1.1050216\n","2.   optimal value for b: 87.67143\n","3.   minimal loss: 349.200787168560\n","\n","After 80000 update steps we are very close to the optimal values\n"]},{"cell_type":"code","metadata":{"id":"omFXVvFIW3aT"},"source":["# display a, b and loss function\n","print(a,b, loss(a,b))"],"execution_count":null,"outputs":[]}]}