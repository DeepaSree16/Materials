{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7rc1"},"colab":{"name":"M1_Case_Study_InformationGain_Mutual_Information.ipynb","provenance":[{"file_id":"11Iv7MwU0Wk06SE6A0wC5GmhXqypK6sKc","timestamp":1616663108432}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"accepting-emergency"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Case Study: Information Gain & Mutual Information"],"id":"accepting-emergency"},{"cell_type":"markdown","metadata":{"id":"developmental-messenger"},"source":["## Learning Objectives"],"id":"developmental-messenger"},{"cell_type":"markdown","metadata":{"id":"detected-positive"},"source":["At the end of the case study, you will be able to\n","\n","* understand the significance of information gain\n","* calculate the entropy of a probability distribution\n","* understand the mutual information and its importance\n","* find the feature with the best information gain from the given set of features"],"id":"detected-positive"},{"cell_type":"markdown","metadata":{"id":"sublime-edwards"},"source":["## Dataset"],"id":"sublime-edwards"},{"cell_type":"markdown","metadata":{"id":"upper-richmond"},"source":["### Dataset description\n","\n","The dataset chosen for this assignment is **Mushroom Dataset**. This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified as definitely edible, definitely poisonous, or of unknown edibility and not recommended.\n","\n","source: https://archive.ics.uci.edu/ml/datasets/mushroom"],"id":"upper-richmond"},{"cell_type":"markdown","metadata":{"id":"handy-lawrence"},"source":["## Information"],"id":"handy-lawrence"},{"cell_type":"markdown","metadata":{"id":"secondary-internship"},"source":["**Information Gain**\n","\n","Information Gain measures the reduction in entropy by splitting a dataset according to a given value of a random variable. The largest information gain is equivalent to the smallest entropy.\n","\n","Entropy quantifies how much information there is in a random variable, or more specifically, its probability distribution. A skewed distribution has low entropy, whereas a distribution where events have equal probability has a larger entropy.\n","\n","Information Gain is applied to quantify which feature provides maximal information about the classification based on the notion of entropy, i.e., by quantifying the size of uncertainty, disorder, or impurity, in general, with the intention of decreasing the amount of entropy.\n","\n","**Mutual Information**\n","\n","Mutual information is a quantity that measures a relationship between two\n","random variables that are sampled simultaneously. In particular, it measures\n","how much information is communicated, on average, in one random variable\n","about another.\n","\n","For example, suppose X represents the roll of a fair 6-sided die, and Y\n","represents whether the roll is even. Clearly, the value of Y\n","tells us something about the value of X and vice versa. That is, these variables\n","share mutual information."],"id":"secondary-internship"},{"cell_type":"markdown","metadata":{"id":"general-fifteen"},"source":["#### Importing required packages"],"id":"general-fifteen"},{"cell_type":"code","metadata":{"id":"phantom-timing"},"source":["import numpy as np\n","import pandas as pd \n","import math                    # Math Functions\n","from math import log2          # log base 2 function\n","from math import log           # log function\n","from collections import Counter                      # Keeps track of elements and count\n","from sklearn.preprocessing import LabelEncoder       # Encoding the labels\n","from matplotlib import pyplot as plt                 # Visualization"],"id":"phantom-timing","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"placed-integrity"},"source":["### Entropy"],"id":"placed-integrity"},{"cell_type":"markdown","metadata":{"id":"equivalent-siemens"},"source":["It is a metric to measure the uncertainty of a probability distribution. Low entropy means the distribution varies (peaks and valleys). High entropy means the distribution is uniform.\n","\n","![img](https://miro.medium.com/max/400/1*M15RZMSk8nGEyOnD8haF-A.png)\n","\n","From the above figure, the x-axis measures the proportion of data points belonging to the positive class in each bubble, and the y-axis axis measures their respective entropies. Right away, you can see the inverted ‘U’ shape of the graph. Entropy is lowest at the extremes when the bubble either contains no positive instances or only positive instances. That is, when the bubble is pure, the disorder is 0. Entropy is highest in the middle when the bubble is evenly split between positive and negative instances. Extreme disorder because there is no majority."],"id":"equivalent-siemens"},{"cell_type":"markdown","metadata":{"id":"unnecessary-network"},"source":["#### Calculating the entropy\n","A dataset with a 50/50 split of samples for the two classes would have a maximum entropy of 1, whereas an imbalanced dataset with a split of 10/90 would have a smaller entropy for a randomly drawn example from the dataset. In a binary classification problem (two classes), we can calculate the entropy of the data sample as follows:\n","\n","Entropy = -(P(0) * log(P(0)) + P(1) * log(P(1)))\n","\n","We can demonstrate this with an example of calculating the entropy for this imbalanced dataset in Python."],"id":"unnecessary-network"},{"cell_type":"code","metadata":{"id":"terminal-director"},"source":["# proportion of examples in each class\n","class0 = 10/100\n","class1 = 90/100\n","# calculate entropy\n","entropy = -(class0 * log2(class0) + class1 * log2(class1))\n","# print the result\n","print('entropy: %.3f' % entropy)"],"id":"terminal-director","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"electronic-elite"},"source":["We define a function to calculate the entropy of a group of samples based on the ratio of samples that belong to class 0 and class 1."],"id":"electronic-elite"},{"cell_type":"code","metadata":{"id":"viral-headset"},"source":["# lets define a function for entropy\n","def class_entropy(class0, class1):\n","    return -(class0 * log2(class0) + class1 * log2(class1))"],"id":"viral-headset","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cubic-ready"},"source":["Now, consider a dataset with 20 examples, 13 for class 0 and 7 for class 1. We can calculate the entropy for this dataset."],"id":"cubic-ready"},{"cell_type":"code","metadata":{"id":"fallen-bookmark"},"source":["# split of the main dataset \n","class0 = 13 / 20\n","class1 = 7 / 20\n","# calculate entropy before the change\n","s_entropy = class_entropy(class0, class1)"],"id":"fallen-bookmark","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"thorough-summer"},"source":["Let’s assume that we have a group of eight samples, seven for class 0 and one for class 1. We can then calculate the entropy of this group of samples."],"id":"thorough-summer"},{"cell_type":"code","metadata":{"id":"sweet-edgar"},"source":["# split 1: group 1\n","s1_class0 = 7 / 8\n","s1_class1 = 1 / 8\n","# calculate the entropy of the first group\n","s1_entropy = class_entropy(s1_class0, s1_class1)\n","print('Group1 Entropy: %.3f' % s1_entropy)"],"id":"sweet-edgar","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"understanding-departure"},"source":["Now, let’s assume that we have a group of 12 samples with six in each group. We would expect this group to have an entropy of 1."],"id":"understanding-departure"},{"cell_type":"code","metadata":{"id":"assumed-aberdeen"},"source":["# split 2: group 2\n","s2_class0 = 6 / 12\n","s2_class1 = 6 / 12\n","# calculate the entropy of the second group\n","s2_entropy = class_entropy(s2_class0, s2_class1)\n","print('Group2 Entropy: %.3f' % s2_entropy)"],"id":"assumed-aberdeen","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"marine-remedy"},"source":["### Information gain"],"id":"marine-remedy"},{"cell_type":"markdown","metadata":{"id":"hollywood-founder"},"source":["Finally, we can calculate the information gain based on the groups created for each value and the calculated entropy.\n","\n","The first group has eight samples, and the second group has the remaining 12 samples in the data set. Therefore, we have everything we need to calculate the information gain.\n","\n","In this case, information gain can be calculated as:\n","\n","$Entropy(Dataset) – ({\\frac{Count(Group1)}{Count(Dataset)})*Entropy(Group1)}  +  ({\\frac{Count(Group2)}{Count(Dataset)}) * Entropy(Group2)}$\n","\n","Entropy($\\frac{13}{20}$, $\\frac{7}{20}$) – ($\\frac{8}{20}$ * Entropy($\\frac{7}{8}$, $\\frac{1}{8}$) + $\\frac{12}{20}$ * Entropy($\\frac{6}{12}$, $\\frac{6}{12}$))"],"id":"hollywood-founder"},{"cell_type":"code","metadata":{"id":"living-contribution"},"source":["# calculate the information gain\n","gain = s_entropy - (8/20 * s1_entropy + 12/20 * s2_entropy)\n","print('Information Gain: %.3f bits' % gain)"],"id":"living-contribution","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hindu-awareness"},"source":["#### Calculating entropy of a probability distribution\n","\n","\n","entropy(p) = − SUM (Pi * log(Pi) )\n","\n","To know more about entropy click [here](http://www.cs.csi.cuny.edu/~imberman/ai/Entropy%20and%20Information%20Gain.htm)"],"id":"hindu-awareness"},{"cell_type":"code","metadata":{"id":"chronic-purse"},"source":["# below function returns the Entropy of a probability distribution\n","def entropy_cal(pi):\n","    total = 0\n","    # Iterating the Pi\n","    for p in pi:\n","        p = p / sum(pi)\n","        # calculating the total i.e. sum(pi*log(pi))\n","        if p != 0:\n","            total += p * log(p, 2)\n","    # applying the negative sign\n","    total *= -1\n","    return total"],"id":"chronic-purse","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"constant-annual"},"source":["the more **impure** a dataset, the higher the entropy and the less **impure** a dataset, the lower the entropy."],"id":"constant-annual"},{"cell_type":"code","metadata":{"id":"dress-drawing"},"source":["plt.plot(np.linspace(0.01,1),np.log(np.linspace(0.01,1)))\n","plt.xlabel(\"P(x)\")\n","plt.ylabel(\"log2(P(x))\")\n","plt.show()"],"id":"dress-drawing","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"regional-commons"},"source":["Information gain in above case\n","\n","gain(D, A) = entropy(D)− SUM ( |Di| / |D| * entropy(Di) ), where D is the dataset and A (Di) is the group of the dataset\n","\n","To know more about Information gain click [here](https://datacadamia.com/data_mining/information_gain)"],"id":"regional-commons"},{"cell_type":"code","metadata":{"id":"buried-siemens"},"source":["# below function returns the information gain\n","def gain(d, a):\n","    total = 0\n","    for v in a:\n","        total += sum(v) / sum(d) * entropy_cal(v)\n","    gain = entropy_cal(d) - total\n","    return gain"],"id":"buried-siemens","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"portuguese-winning"},"source":["Testing with an example\n","\n","Consider playing tennis under different conditions"],"id":"portuguese-winning"},{"cell_type":"code","metadata":{"id":"pointed-brisbane"},"source":["# example (playTennis)\n","\n","# set of example of the dataset\n","playTennis = [9, 5] # Yes, No"],"id":"pointed-brisbane","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"discrete-colon"},"source":["# attribute, number of members (feature)\n","outlook = [\n","    [4, 0],  # overcase\n","    [2, 3],  # sunny\n","    [3, 2]   # rain\n","]\n","print(gain(playTennis, outlook))"],"id":"discrete-colon","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bigger-cement"},"source":["temperature = [\n","    [2, 2],  # hot\n","    [3, 1],  # cool\n","    [4, 2]   # mild\n","]\n","print(gain(playTennis, temperature))"],"id":"bigger-cement","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"everyday-thriller"},"source":["humidity = [\n","    [3, 4],  # high\n","    [6, 1]   # normal\n","]\n","print(gain(playTennis, humidity))"],"id":"everyday-thriller","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bigger-highlight"},"source":["wind = [\n","    [6, 2],  # weak\n","    [3, 3]   # strong\n","]\n","print(gain(playTennis, wind))"],"id":"bigger-highlight","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"seeing-judgment"},"source":["### Finding the best split using Information gain\n","\n","Consider the mushroom dataset having 22 features for calculating the Information gain and finding the best feature that has the highest gain"],"id":"seeing-judgment"},{"cell_type":"code","metadata":{"cellView":"form","id":"N-oY18gwYzO7"},"source":["#@title Download Dataset\n","!wget https://cdn.iisc.talentsprint.com/CDS/Datasets/mushrooms.csv"],"id":"N-oY18gwYzO7","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"saving-supplier"},"source":["data = pd.read_csv('mushrooms.csv')\n","data.head()"],"id":"saving-supplier","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"diverse-duncan"},"source":["The idea behind building trees is, finding the best feature to split on that generates the largest information gain or provides the least uncertainty in the following leafs. The 2 most known ways to find these features are:\n","\n","1. Gini Impurity\n","\n","2. Entropy"],"id":"diverse-duncan"},{"cell_type":"markdown","metadata":{"id":"conventional-crisis"},"source":["#### Gini Impurity\n","\n","We start at 1 (maximum impurity) and subtract the squared percentage of each label in the data set. As an example, if a data set had 5 items of class (1) and 5 items of class (0), the Gini impurity of the set would be $G=1−(5/10)^2−(5/10)^2$ \n","\n","That is the impurity at any given instance/leaf, to find the Weighted Information Gain, you start with the old (root) Gini Impurity and subtract the sum of all weighted Gini Impurities. The weighted Gini Impurity is the same as the Gini Impurity but multiplied by a ratio (weight) which is the number of data points in the new leaf divided by the number of points in the original root."],"id":"conventional-crisis"},{"cell_type":"markdown","metadata":{"id":"effective-repeat"},"source":["![gini_impurity_Example](https://cdn.iisc.talentsprint.com/CDS/Images/Gini_Impurity_Example.JPG)\n","\n","The Weighted Gain for this example  =0.5−(2/10)∗0.5−(5/10)∗0.48−(4/10)∗0.44=0.026"],"id":"effective-repeat"},{"cell_type":"markdown","metadata":{"id":"brown-surface"},"source":["Entropy: Similar to the Gini Impurity but follows a different formula for the calculation:\n","![entropy_equation](https://cdn.iisc.talentsprint.com/CDS/Images/Entropy_Equation.JPG)\n","\n","We will still find the information gain, using weighted entropies, and pick the attribute which provided the maximum information gain.\n","\n","![entropy_equation](https://cdn.iisc.talentsprint.com/CDS/Images/Gain_Equation.JPG)"],"id":"brown-surface"},{"cell_type":"markdown","metadata":{"id":"deadly-fashion"},"source":["Let's start by creating 2 functions, one that calculates the entropy and one that calculates the information gain."],"id":"deadly-fashion"},{"cell_type":"code","metadata":{"id":"curious-counter"},"source":["# below function to calculate the entropy\n","def entropy(labels):\n","    entropy=0\n","    # get the element counts of labels\n","    label_counts = Counter(labels)\n","    for label in label_counts:\n","        prob_of_label = label_counts[label] / len(labels)\n","        entropy -= prob_of_label * math.log2(prob_of_label)\n","    return entropy\n","\n","# below function to calculate the information gain\n","def information_gain(starting_labels, split_labels):\n","    info_gain = entropy(starting_labels)\n","    for branched_subset in split_labels:\n","        info_gain -= len(branched_subset) * entropy(branched_subset) / len(starting_labels)\n","    return info_gain"],"id":"curious-counter","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"registered-portrait"},"source":["Now let's define a function that takes one column at a time and returns list of variants in that column"],"id":"registered-portrait"},{"cell_type":"code","metadata":{"id":"statutory-seventh"},"source":["# below function to split each variant records of one column i.e. feature\n","def split(dataset, column):\n","    split_data = []\n","    col_vals = data[column].unique() # This tree generation method only works with discrete values\n","    for col_val in col_vals:\n","        split_data.append(dataset[dataset[column] == col_val])\n","    return split_data"],"id":"statutory-seventh","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"plastic-smoke"},"source":["Now Let's define a function that integrates all the above defined functions and returns the best feature having the highest information gain"],"id":"plastic-smoke"},{"cell_type":"code","metadata":{"id":"norman-apollo"},"source":["def find_best_split(dataset):\n","    best_gain = 0\n","    best_feature = 0\n","    features = list(dataset.columns)\n","    # remove the target\n","    features.remove('class')\n","    for feature in features:\n","        split_data = split(dataset, feature)\n","        # extract labels of each variants in corresponding feature\n","        split_labels = [dataframe['class'] for dataframe in split_data]\n","        gain = information_gain(dataset['class'], split_labels)\n","        if gain > best_gain:\n","            best_gain, best_feature = gain, feature\n","    print(\"best feature: {}  \\ninformation gain: {}\".format(best_feature, best_gain))\n","    return best_feature, best_gain\n","\n","new_data = split(data, find_best_split(data)[0]) # contains a list of dataframes after splitting"],"id":"norman-apollo","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"still-haven"},"source":["Now, a recursive call is needed to keep finding the best split for every new data subset."],"id":"still-haven"},{"cell_type":"markdown","metadata":{"id":"super-spoke"},"source":["### Mutual Information"],"id":"super-spoke"},{"cell_type":"markdown","metadata":{"id":"faced-laser"},"source":["It provides a measure of the mutual dependence between two random variables. Let $X$ and $Y$ be random variables with probability distributions $pX$ and $pY$ respectively, and $pX,Y$ the joint probability distribution over $(X,Y)$. The base-b mutual information between $X$ and $Y$ is defined as\n","\n","\\begin{split}I(X;Y) &= \\sum_{x,y} p_{X,Y}(x,y) \\log_b \\frac{p_{X,Y}(x,y)}{p_X(x)p_Y(y)}\\\\\n","       &= H(X) + H(Y) - H(X,Y).\\end{split}"],"id":"faced-laser"},{"cell_type":"markdown","metadata":{"id":"rough-disability"},"source":["#### Conditional Entropy\n","\n","The conditional entropy is used to measure the relationship between variables. The following formula gives this measurement:\n","\n","$H(Y \\mid X) = - \\sum_{x} \\sum_{y} p(x, y) \\log_{2} p(y \\mid x)$\n","\n","Let investigate how conditional entropy is related to entropy. Using the above formula, we can conclude that:\n","\n","$ H(Y \\mid X) = H(X, Y) - H(X)$\n","\n","meaning that the information contained in $Y$ given $X$ equals information jointly contained in $X$ and $Y$ minus the amount of information only contained in $X$.\n"],"id":"rough-disability"},{"cell_type":"code","metadata":{"id":"arabic-vatican"},"source":["def conditional_entropy(p_xy, p_x):\n","    p_y_given_x = p_xy / p_x\n","    out = np.nansum(-p_xy * np.log2(p_y_given_x))\n","    return out\n","\n","print(conditional_entropy(np.array([[0.1, 0.5], [0.2, 0.3]]), np.array([0.2, 0.8])))"],"id":"arabic-vatican","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lucky-wayne"},"source":["To know more about `np.nansum` click [here](https://numpy.org/doc/stable/reference/generated/numpy.nansum.html)"],"id":"lucky-wayne"},{"cell_type":"markdown","metadata":{"id":"attended-victory"},"source":["Knowing conditional entropy means knowing the amount of information contained in $Y$ but not in $X$. Now let see how much information is shared between $X$ and $Y$."],"id":"attended-victory"},{"cell_type":"markdown","metadata":{"id":"bronze-illustration"},"source":["To find the **mutual information** between two random variables $X$ and $Y$, let start the process by finding all the information in both $X$ and $Y$ together and then subtract the part which is not shared. The information both in $X$ and $Y$ is $H(X,Y)$. Subtracting two conditional entropies gives:  $I(X, Y) = H(X, Y) - H(Y \\mid X) − H(X \\mid Y)$\n","\n","This means that we have to subtract the information only contained in X and Y from all the information at hand. This relationship is perfectly described by this picture from the d2l.\n","\n","![mutual_inf](https://cdn.iisc.talentsprint.com/CDS/Images/JointEntropy.JPG)\n","\n","The concept of mutual information likewise correlation coefficient, allows us to measure the linear relationship between two random variables as well as the amount of maximum information shared between them."],"id":"bronze-illustration"},{"cell_type":"code","metadata":{"id":"continued-musical"},"source":["def mutual_information(p_xy, p_x, p_y):\n","    p = p_xy / (p_x * p_y)\n","    out = np.nansum(p_xy * np.log2(p))\n","    return out\n","\n","print(mutual_information(np.array([[0.1, 0.5], [0.1, 0.3]]),\n","                        np.array([0.2, 0.8]),\n","                        np.array([[0.75, 0.25]])))"],"id":"continued-musical","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"binding-recovery"},"source":["We can interpret the mutual information I(X,Y) as the average amount of surprisal by seeing two outcomes happening together compared to what we would expect if they were independent."],"id":"binding-recovery"},{"cell_type":"markdown","metadata":{"id":"boolean-monte"},"source":["Mutual information measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n","\n","**Note:** We use `mutual_info_classif()` from sklearn.feature_selection"],"id":"boolean-monte"},{"cell_type":"code","metadata":{"id":"integral-farmer"},"source":["# preprocessing - changing the values to numbers with label encoder.\n","def Label_enc(features):\n","    enc = LabelEncoder()\n","    return enc.fit_transform(features)"],"id":"integral-farmer","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"deluxe-boring"},"source":["# encoding the all the columns\n","for col in data.columns:\n","    data[str(col)] = Label_enc(data[str(col)])"],"id":"deluxe-boring","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"effective-logan"},"source":["Extract features and target from the dataset"],"id":"effective-logan"},{"cell_type":"code","metadata":{"id":"recorded-champion"},"source":["target = data['class']\n","features = data.iloc[:,1:]\n","features"],"id":"recorded-champion","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gross-green"},"source":["Applying `mutual_info_classif` on the data"],"id":"gross-green"},{"cell_type":"code","metadata":{"id":"engaging-commission"},"source":["from sklearn.feature_selection import mutual_info_classif\n","mi = mutual_info_classif(features, target) \n","len(mi)"],"id":"engaging-commission","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"legal-humidity"},"source":["mi"],"id":"legal-humidity","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unique-triple"},"source":["Add columns to the mutual information values"],"id":"unique-triple"},{"cell_type":"code","metadata":{"id":"million-street"},"source":["mi = pd.Series(mi)\n","mi.index = features.columns"],"id":"million-street","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"first-withdrawal"},"source":["Let’s observe the Mutual Information with respect to features from the following bar plot."],"id":"first-withdrawal"},{"cell_type":"code","metadata":{"id":"adjusted-collectible"},"source":["# sort the values in the descending order.\n","mi.sort_values(ascending=False, inplace = True)\n","plt.title('Mutual information with respect to features')\n","mi.plot.bar(figsize = (16,5))\n","plt.show()"],"id":"adjusted-collectible","execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"polyphonic-relay"},"source":["**Conclusion**\n","\n","In decision trees, at each stage of the process, we split on the variable that maximizes mutual information with the target value. That is, the variable that reduces the uncertainty about the target the most. Say $Y$ is the target value and we have $X_1,\\cdots,X_p$. Then we check $I(Y;X_1),\\cdots,I(Y;X_p)$. We generally can’t compute these in closed form without knowing the distributions, but we can approximate them using sampling and in some cases other methods: sklearn’s `mutual_info_classif` is one implementation. Once we know the $X_i$ that we want to split on, we need to choose a threshold. We again choose the threshold that maximizes mutual information by minimizing conditional entropy.\n","\n","To know more about the essence of Information gain in Decision tree, click [here](https://victorzhou.com/blog/information-gain/)"],"id":"polyphonic-relay"}]}