{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CDS-PRB-02-AS-4-5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vAYtrPrfY2Bp"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 4: Text Classification using Naive Bayes Classifier"]},{"cell_type":"markdown","metadata":{"id":"y_b5pYsPY7f3"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"S7sU_4_aZASD"},"source":["At the end of the experiment, you will be able to:\n","\n","* perform different text processing techniques\n","* train and evaluate NaiveBayes classification model\n","* predict the sentiment ('positive' or 'negative') for a movie reviews dataset using the sklearn machine learning library\n","* generate customizable UI using gradio library\n"]},{"cell_type":"markdown","metadata":{"id":"qEYhGIYoTN-a"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"nA2VTJIOMb1n"},"source":["**Naïve Bayes’ Classifier**\n","\n","Bayesian network classifiers are a popular supervised\n","classification paradigm. The Naïve Bayes’ classifier is a probabilistic\n","classifier based on the Bayes’ theorem, considering\n","Naïve (Strong) independence assumption.\n","It is a popular(baseline) method for text categorizing, with word frequencies as the feature. \n","\n","An advantage of Naïve Bayes’ is that it\n","only requires a small amount of training data to\n","estimate the parameters necessary for classification. Despite its simplicity and strong assumptions, the\n","Naïve Bayes’ classifier has been proven to work\n","satisfactorily in many domains. Bayesian classification\n","provides practical learning algorithms and prior knowledge and observed data can be combined. In Naïve Bayes’ technique, the basic idea is to find the\n","probabilities of categories given a text document by\n","using the joint probabilities of words and categories. It is based on the assumption of word independence. "]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Download Dataset  \n","!wget https://cdn.iisc.talentsprint.com/CDS/Datasets/IMDB_Dataset.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F5VyFvjMT5O-"},"source":["### Importing Required Packages"]},{"cell_type":"code","metadata":{"id":"wl0ngdnKjKHH"},"source":["!pip -qq install gradio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZWtHdmPYMOvT"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.feature_extraction.text import CountVectorizer                        # to transform a given text into a vector on the basis of the frequency (count) of each word that occurs in the entire text\n","from sklearn.feature_extraction.text import TfidfVectorizer                        # to transform text into a term and document frequency based representation of numbers\n","import nltk                                                                        # platform for building Python programs to process natural language\n","nltk.download('stopwords')                                                         # to download the stop words\n","nltk.download('punkt')                                                             # tokenizer divides a text into a list of sentences, by using an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences\n","from nltk.corpus import stopwords                                                  # importing the NTLK stopwords to remove articles, preposition and other words that are not actionable\n","from nltk.stem.porter import PorterStemmer                                         # process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words\n","from wordcloud import WordCloud                                                    # visualization of words based on their frequency\n","from nltk.tokenize import word_tokenize                                            # allows to create individual objects from a bag of words\n","from bs4 import BeautifulSoup                                                      # Python library for pulling data from HTML and XML files\n","import re                                                                          # regular expression (or RE) specifies a set of strings that matches it                                                    \n","from sklearn.naive_bayes import MultinomialNB                                      # to import multinomial naive bayes which is suitable for classification with discrete features (e.g., word counts for text classification)\n","from sklearn.metrics import classification_report,confusion_matrix,accuracy_score  # to import metrics for evaluating the classification model \n","from sklearn.model_selection import train_test_split\n","import gradio\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nJ9aiXQDBQ-r"},"source":["### Dataset description"]},{"cell_type":"markdown","metadata":{"id":"bEarccunBsDs"},"source":["The [IMDB Movie Reviews dataset](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) is a binary sentiment analysis dataset consisting of 50,000 reviews from the Internet Movie Database (IMDb) labeled as positive or negative. The dataset contains an even number of positive and negative reviews. Only highly polarizing reviews are considered. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. No more than 30 reviews are included per movie. The dataset contains additional unlabeled data."]},{"cell_type":"code","metadata":{"id":"v_fAB1U0fqJ7"},"source":["# read the dataset\n","df = pd.read_csv('IMDB_Dataset.csv')\n","print(df.shape)\n","df.head(10)      # first 10 rows "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wy2WsjbRDCJr"},"source":["### Exploratory Data Analysis"]},{"cell_type":"code","metadata":{"id":"z8mLwZe5CWSN"},"source":["# summary of the dataset\n","df.describe()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twI5ntekDNrs"},"source":["Now, we will look at the sentiment count."]},{"cell_type":"code","metadata":{"id":"6izwMrUVDQ6f"},"source":["# sentiment count\n","df['sentiment'].value_counts()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tyeDUn36DVRn"},"source":["We can see that the dataset is balanced.\n","\n","Now, we will do the text cleaning of the reviews.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"on2XCsxtI0-b"},"source":["### Text Cleaning\n","\n","The data scraped from the website is mostly in the raw text form. This data needs to be cleaned before analyzing it or fitting a model to it. Cleaning up the text data is necessary to highlight the attributes that we are going to want our machine learning system to pick up on."]},{"cell_type":"markdown","metadata":{"id":"tupdRoUSGYrw"},"source":["**Removing html strips and noise text**\n","\n","Sample noise removal tasks could include:\n","\n","* removing text file headers, footers\n","* removing HTML, XML, etc. markup and metadata\n","* extracting valuable data from other formats, such as JSON"]},{"cell_type":"code","metadata":{"id":"x56E-c6KFvZH"},"source":["# removing the html strips\n","def strip_html(text):\n","    # BeautifulSoup is a useful library for extracting data from HTML and XML documents\n","    soup = BeautifulSoup(text, \"html.parser\")\n","    return soup.get_text()\n","\n","# removing the square brackets\n","def remove_between_square_brackets(text):\n","    return re.sub('\\[[^]]*\\]', '', text)\n","\n","# removing the noisy text\n","def denoise_text(text):\n","    text = strip_html(text)\n","    text = remove_between_square_brackets(text)\n","    return text\n","# apply function on review column\n","df['review'] = df['review'].apply(denoise_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WQzlTJ-VGwVu"},"source":["**Removing special characters**\n","\n","Special characters typically include any character that is not a letter or number, such as punctuation and whitespace. Removing special characters from a string results in a string containing only letters and numbers."]},{"cell_type":"code","metadata":{"id":"IBWV2c7oGpVL"},"source":["# define function for removing special characters\n","def remove_special_characters(text, remove_digits=True):\n","    pattern = r'[^a-zA-z0-9\\s]'\n","    text = re.sub(pattern,'',text)\n","    return text\n","# apply function on review column\n","df['review'] = df['review'].apply(remove_special_characters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IGsYJGmLG9bM"},"source":["**Text Stemming**\n","\n","Stemming, also called suffix stripping, is a technique used to reduce text dimensionality. Stemming is also a type of text normalization that enables you to standardize some words into specific expressions also called stems."]},{"cell_type":"code","metadata":{"id":"bmBWpKUhG5F7"},"source":["# stemming the text\n","def simple_stemmer(text):\n","    ps=nltk.porter.PorterStemmer()\n","    text= ' '.join([ps.stem(word) for word in text.split()])\n","    return text\n","# apply function on review column\n","df['review'] = df['review'].apply(simple_stemmer)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"omeXdqsFHm1M"},"source":["**Removing stopwords**\n","\n","Stopwords are English words that do not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc."]},{"cell_type":"code","metadata":{"id":"mgIjbfZgEgNH"},"source":["# setting english stopwords\n","stopword_list = nltk.corpus.stopwords.words('english')\n","print(stopword_list)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H_OBq2crHXU5"},"source":["# set stopwords to english\n","stop = set(stopwords.words('english'))\n","print(stop)\n","\n","# removing the stopwords\n","def remove_stopwords(text, is_lower_case=False):\n","    # splitting strings into tokens (list of words)\n","    tokens = word_tokenize(text)\n","    tokens = [token.strip() for token in tokens]\n","    if is_lower_case:\n","        # filtering out the stop words\n","        filtered_tokens = [token for token in tokens if token not in stopword_list]\n","    else:\n","        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n","    filtered_text = ' '.join(filtered_tokens)    \n","    return filtered_text\n","# apply function on review column\n","df['review'] = df['review'].apply(remove_stopwords)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TxWPUIt3QrBY"},"source":["After cleaning the reviews, we now split the clean dataset into training and testing set."]},{"cell_type":"code","metadata":{"id":"gwGbAazvDVuB"},"source":["X_train, X_test, y_train, y_test = train_test_split(df.review, df.sentiment, test_size = 0.2, random_state = 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"juVRVeO6i3Xp"},"source":["**Parameters in train_test_split**\n","\n","* arrays sequence of indexables with same length / shape (here, df.review and df.sentiment): \n","Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n","\n","* test_size (float or int, default=None): The proportion of the dataset to include in the test split If train_size is also None, it will be set to 0.25.\n","\n","\n","* RandomState:\n","Controls the shuffling applied to the data before applying the split."]},{"cell_type":"code","metadata":{"id":"k_LwQzG3Ec3i"},"source":["print(X_train.shape)\n","print(X_test.shape)\n","print(y_train.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eG_BKf2kT9Ui"},"source":["Let us see positive and negative words by using **WordCloud**.\n","\n","A word cloud (also called tag cloud or weighted list) is a visual representation of text data. Words are usually single words, and the importance of each is shown with font size or color.\n","\n"]},{"cell_type":"code","metadata":{"id":"upS9PJwNTYF5"},"source":["# Word cloud for positive review words\n","\n","plt.figure(figsize=(10,10))\n","df_positive_review =  df[df['sentiment']=='positive']\n","positive_text = ' '.join(review for review in df_positive_review.review)\n","WC = WordCloud(width=1000, height=500, max_words=500, min_font_size=5)\n","positive_words = WC.generate(positive_text)\n","plt.imshow(positive_words, interpolation='bilinear')\n","plt.show"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XRkYhLgjULMH"},"source":["# Word cloud for negative review words\n","\n","plt.figure(figsize=(10,10))\n","df_negative_review =  df[df['sentiment']=='negative']\n","negative_text = ' '.join(review for review in df_negative_review.review)\n","WC = WordCloud(width=1000, height=500, max_words=500, min_font_size=5)\n","negative_words = WC.generate(negative_text)\n","plt.imshow(negative_words, interpolation='bilinear')\n","plt.show"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hO52_AYgaMZa"},"source":["#### Count Vectorizer\n","\n","The `CountVectorizer` provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.\n","\n","An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document. The vectors returned from a call to transform() will be sparse vectors, and we can transform them back to numpy arrays to look and better understand what is going on by calling the `toarray()` function.\n","\n","To know more about CountVectorizer, click [here](https://towardsdatascience.com/basics-of-countvectorizer-e26677900f9c)."]},{"cell_type":"code","metadata":{"id":"ilmOsl0ciO88"},"source":["# Count vectorizer\n","cv = CountVectorizer(min_df=0, max_df=1, binary=False, ngram_range=(1,3))\n","# transformed train reviews\n","cv_train_reviews = cv.fit_transform(X_train)\n","# transformed test reviews\n","cv_test_reviews = cv.transform(X_test)\n","print('CV_train:', cv_train_reviews.shape)\n","print('CV_test:', cv_test_reviews.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dAavZUbLPNoY"},"source":["#### Term Frequency-Inverse Document Frequency (TF-IDF)\n","\n","It is used to convert text documents to matrix of tf-idf features.\n","\n","`TfidfVectorizer` is the base building block of many NLP pipelines. It is a simple technique to vectorize text documents — i.e. transform sentences into arrays of numbers — and use them in subsequent tasks.\n","\n","To know more about tf-idf, click [here]( https://cdn.iisc.talentsprint.com/CDS/TF-IDF.pdf)."]},{"cell_type":"code","metadata":{"id":"Fm4CdvwfLbLi"},"source":["# tfidf vectorizer\n","tv = TfidfVectorizer(min_df=0, max_df=1, use_idf=True, ngram_range = (1,3))\n","#transformed train reviews\n","tfidf_train_reviews = tv.fit_transform(X_train)\n","#transformed test reviews\n","tfidf_test_reviews = tv.transform(X_test)\n","print('Tfidf_train:', tfidf_train_reviews.shape)\n","print('Tfidf_test:', tfidf_test_reviews.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ne_S1VjfRH_S"},"source":["### Modeling the dataset"]},{"cell_type":"markdown","metadata":{"id":"n9oco2hkRXP3"},"source":["Naive Bayes classifier for multinomial models.\n","\n","The multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work.\n","\n","To know more, click [here](https://scikit-learn.org/stable/modules/naive_bayes.html#multinomial-naive-bayes)."]},{"cell_type":"markdown","metadata":{"id":"LKq5SAD9kKOp"},"source":["#### **`sklearn library`**\n","\n","Scikit-learn is a free machine learning library for Python. It features various algorithms like support vector machine, random forests, k-nearest neighbors, NaiveBayes and it also supports Python numerical and scientific libraries like NumPy and SciPy."]},{"cell_type":"code","metadata":{"id":"xkqhHOOpjxni"},"source":["# training the model\n","mnb=MultinomialNB()\n","# fitting the NaiveBayes for count vectorizer\n","mnb_cv = mnb.fit(cv_train_reviews, y_train)\n","print('MultinomialNB for Count Vectorizer :',mnb_cv)\n","# fitting the NaiveBayes for tfidf features\n","mnb_tfidf = mnb.fit(tfidf_train_reviews, y_train)\n","print('MultinomialNB for tf-idf :',mnb_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dU0rl3M8n3_d"},"source":["Here, the model parameters are as follows: \n","\n","* alpha: \n","Additive smoothing parameter (0 for no smoothing). In statistics, additive smoothing, also called Laplace smoothing, is a technique used to smooth categorical data. Given a set of observation counts ${\\textstyle {\\mathbf {x} \\ =\\ \\left\\langle x_{1},\\,x_{2},\\,\\ldots ,\\,x_{d}\\right\\rangle }}$ from a $d$-dimensional multinomial distribution with $N$ trials, a \"smoothed\" version of the counts gives the estimator:\n","\n","${\\hat {\\theta }}_{i}={\\frac {x_{i}+\\alpha }{N+\\alpha d}}\\qquad (i=1,\\ldots ,d)$,\n","\n","where the smoothed count ${\\textstyle {{\\hat {x}}_{i}=N{\\hat {\\theta }}_{i}}}$ and the \"pseudocount\" $α > 0$ is a smoothing parameter. $α = 0$ corresponds to no smoothing. \n","\n","* fit_prior:\n","Whether to learn class prior probabilities or not. If false, a uniform prior will be used.\n","\n","* class_prior:\n","Prior probabilities of the classes. If specified the priors are not adjusted according to the data."]},{"cell_type":"markdown","metadata":{"id":"KDp3YW0fSAtO"},"source":["**Model performance on test data**"]},{"cell_type":"code","metadata":{"id":"TnXIgKkLR2uM"},"source":["# predicting the model for CountVectorizer\n","mnb_cv_predict = mnb.predict(cv_test_reviews)\n","print('predictions for Count Vectorizer :', mnb_cv_predict)\n","\n","# predicting the model for tfidf features\n","mnb_tfidf_predict = mnb.predict(tfidf_test_reviews)\n","print('predictions for tf-idf :', mnb_tfidf_predict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CUnQOGpoShcz"},"source":["**Accuracy of Model**\n","\n","It is the ratio of number of correct classifications to the total number of input samples."]},{"cell_type":"code","metadata":{"id":"k1EHCAnWSRZ1"},"source":["# accuracy score for count vectorizer\n","mnb_cv_score = accuracy_score(y_test, mnb_cv_predict)\n","print(\"mnb_cv_score :\", mnb_cv_score)\n","\n","# accuracy score for tf-idf\n","mnb_tfidf_score = accuracy_score(y_test, mnb_tfidf_predict)\n","print(\"mnb_tfidf_score :\", mnb_tfidf_score)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEih5xUNTFX3"},"source":["**Plot the confusion matrix**\n","\n","A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. \n","\n","A confusion matrix, in predictive analytics, is a square matrix that tells us the rate of false positives, false negatives, true positives and true negatives for a test or predictor. We can make a confusion matrix if we know both the predicted values and the true values for a sample set."]},{"cell_type":"code","metadata":{"id":"Kitq1vu7S_3A"},"source":["# confusion matrix for count vectorizer\n","cm_cv = confusion_matrix(y_test, mnb_cv_predict, labels=['positive', 'negative'])\n","print('confusion matrix for count vectorizer :\\n', cm_cv)\n","\n","# confusion matrix for tf-idf\n","cm_tfidf = confusion_matrix(y_test,mnb_tfidf_predict, labels=['positive','negative'])\n","print('confusion matrix for tf-idf :\\n', cm_tfidf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DXQYvbik134w"},"source":["### Gradio Implementation"]},{"cell_type":"markdown","metadata":{"id":"f9LGFxtQjwxv"},"source":["Gradio is an open-source python library that allows us to quickly create easy-to-use, customizable UI components for our ML model, any API, or any arbitrary function in just a few lines of code. We can integrate the GUI directly into the Python notebook, or we can share the link with anyone."]},{"cell_type":"code","metadata":{"id":"U41iWl1H14kt"},"source":["# Function for preprocessing of text\n","\n","def preprocess_text(text):\n","\n","    text = denoise_text(text)\n","    text = remove_special_characters(text)\n","    text = simple_stemmer(text)\n","    text = remove_stopwords(text)\n","\n","    return text"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lTEAHUtr0vm1"},"source":["# Function to predict label for a review\n","\n","def predict_review_label(text, vectorizer_method):\n","    \n","    processed_text = preprocess_text(text)\n","\n","    if vectorizer_method == 'CountVectorizer':\n","        review = cv.transform([processed_text])\n","        pred = mnb_cv.predict(review)\n","    if vectorizer_method == 'TFIDFVectorizer':\n","        review = tv.transform([processed_text])\n","        pred = mnb_tfidf.predict(review)\n","    else:\n","        review = cv.transform([processed_text])\n","        pred = mnb_cv.predict(review)\n","\n","    return pred[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZG4q5-9I6n0x"},"source":["# Testing a review\n","predict_review_label(\"It was a good movie, really enjoyed it a lot.\", 'CountVectorizer')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b17nuGAm-hym"},"source":["# Testing a review\n","predict_review_label(\"Was very bad, I was barely able to understand the concepts.\", 'TFIDFVectorizer')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iAWKOcY25sDA"},"source":["# Dropdown choices\n","in_vectorizer_dropdown = gradio.inputs.Dropdown(['CountVectorizer', 'TFIDFVectorizer'], type=\"value\", default=None, label='Choose a Method to Vectorize')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oy_0yXHI7fs4"},"source":["# Input from user\n","in_review = gradio.inputs.Textbox(lines=2, placeholder=None, default=\"review\", label='Enter Review Text')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TlLlAp4E8pew"},"source":["# Output prediction\n","out_label = gradio.outputs.Textbox(type=\"auto\", label='Predicted Review Label')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bpRnUY5w551-"},"source":["# Gradio interface to generate UI link\n","\n","iface = gradio.Interface(\n","  fn = predict_review_label, \n","  inputs = [in_review, in_vectorizer_dropdown],\n","  outputs = [out_label])\n","iface.launch(share=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j2eQkcvnvntC"},"source":["Click on the link generated above to see UI."]}]}