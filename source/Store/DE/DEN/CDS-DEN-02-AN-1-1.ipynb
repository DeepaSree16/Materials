{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M5_Case_Study_(ungraded)_PageRank_MapReduce.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","## Case Study : PageRank on MapReduce\n","(ungraded)"],"metadata":{"id":"j2LG7BITuc6w"}},{"cell_type":"markdown","source":["## Learning Objectives\n"],"metadata":{"id":"DdhpaVdWuwOd"}},{"cell_type":"markdown","source":["At the end of the case study, you will be able to\n","\n","* Obtain a brief overview of MapReduce\n","* Understand the concept of Page Rank used by Google\n","* Implement Page Rank algorithm by using the approach of MapReduce:\n","  * Perform transformation operations using MapReduce\n","  * Calculate the rank of a webpage using pyspark and MapReduce"],"metadata":{"id":"Z1lVwUv6v6fU"}},{"cell_type":"markdown","source":["## Information"],"metadata":{"id":"Tfiy2tTkwAMv"}},{"cell_type":"markdown","source":["**Parallel Processing:-** To deal with massive data or Big data as it is popularly known. The idea is to reduce the run time, cost and memory constraints while dealing with this huge data.\n","\n","In this process, the task is broken down into multiple parts with a help of a tool. Each part is assigned to a different computational unit for processing.\n","Once the computation is done by different units, the solution is re-assembled to give a final solution."],"metadata":{"id":"eogRxAMH1rcJ"}},{"cell_type":"markdown","source":["**Map Reduce:-** This is a programming model that allows the user to perform parallel processing on Big data.\n","\n","To read more about MapReduce [Click Here](https://taylankabbani96.medium.com/mapreduce-programming-model-a7534aca599b)"],"metadata":{"id":"drRvh-_bEceM"}},{"cell_type":"markdown","source":["### **Page Rank**"],"metadata":{"id":"4jY-pTPdGJy_"}},{"cell_type":"markdown","source":["**A brief history:-** Early searching engines used to search through the web and create an inverted index of all the terms found in each page. While querying, an old search engine would return the pages that contain the term the user entered and rank the pages based on the frequency of occurrence of the term entered by the user, in each page. \n","\n","The naivety of this simple approach, gave spammers an opportunity to exploit the searching engines. By exploiting it, they could lead people to their pages (spam).\n","\n","  - **For example:-** A spammer would write an irrelevant term multiple times, say \"movie\", where the page actually sells clothes!! \n","To prevent the term \"movie\" from appearing on the spam page, they could give it the same color as the background. \n","By using this technique, spammers managed to put their pages on the top of the search results of the search engine.This paralyzed Web searching engines and made them almost useless.\n","\n","  To prevent those spam pages from having so much influence, Google proposed **Page Rank** as a way to define the importance of a page. It is not wrong to say that this algorithm is what made **Google** then a very powerful engine."],"metadata":{"id":"VeQLYEXQGQSl"}},{"cell_type":"markdown","source":["### What is Page Rank?"],"metadata":{"id":"No-RTVZ7I8BK"}},{"cell_type":"markdown","source":["**Page rank or PR**, is a recursive algorithm developed by Google, founded by Larry Page to assign a real number to each page in the web so that they can be ranked based on this score. \n","The higher the score of a page, the more importance assigned to it."],"metadata":{"id":"AFpkTtRhGMTJ"}},{"cell_type":"markdown","source":["**Importance:-** The importance score of a page directly depends on the number of other pages pointing to it. \n","\n","To understand the concept of pointing, suppose if a movie-page’s link is being displayed on many other pages, it is said that these pages are pointing or voting to that particular movie-page, and thus:"],"metadata":{"id":"prqVXPuHWZf6"}},{"cell_type":"markdown","source":["**Understanding \"Importance\" with the help of an example:-**\n"],"metadata":{"id":"7ZRCRcS_Xgha"}},{"cell_type":"markdown","source":["How does somebody decide if a person on twitter is important or not? \n","  1. The first thing to check is the number of followers, the more followers a particular person has, the higher likelihood of that person being famous. \n","  2. The second step is to check the importance of that person’s followers, if the president for example is following him/her, the higher the importance of that person."],"metadata":{"id":"lPdGaeheXtHK"}},{"cell_type":"markdown","source":["### **Page Rank Algorithm**"],"metadata":{"id":"wvGI9Du3ZBj0"}},{"cell_type":"markdown","source":["Consider a tiny version of the web with only five pages. To rank these pages based on their importance using page rank algorithm. Consider the graph to understand the concept, where nodes represent pages and arrows represent links between pages.\n","\n","\n","To calculate the importance score of page C, let R denote the importance score of a page, then the importance score of the page C can be calculated as:\n","$R_{c} = R_{B}/3 + R_{A}/4$\n","\n","Page C’s own importance is the sum of the votes on its in-links, and if page $A$ with importance $R_{A}$ has $n$ out-links, each link gets $R_{A}/n$ votes."],"metadata":{"id":"33SXTWJAZhqM"}},{"cell_type":"markdown","source":["![img](https://miro.medium.com/max/526/1*Mgnh6M7DUhJIuO1_btIU1A.png)"],"metadata":{"id":"3QJdQFnga9U3"}},{"cell_type":"markdown","source":["**Steps Involved**"],"metadata":{"id":"SQeFOibEakoa"}},{"cell_type":"markdown","source":["1. Initialize each page’s importance or rank as 1/number of pages.\n","2. Update each page’s rank according to the following formulation:\n","   $$R_j = \\sum_{i}^{j} \\frac{r_i}{d_i}$$\n","   \n","\n","3. Repeat above steps until the page ranks stabilize."],"metadata":{"id":"VVygt7jIaolo"}},{"cell_type":"markdown","source":["### Implementation of Page Rank using MapReduce\n","\n","\n","The number of pages on the Web is enormously huge and using a simple approach to recursively update ranking of millions of pages will be very expensive and time consuming as as well. MapReduce tackles the problem by taking the advantage of running on a cluster (parallelization) "],"metadata":{"id":"HspQ0A_LgGmJ"}},{"cell_type":"markdown","source":["### Installing the pyspark package"],"metadata":{"id":"lEHmdHG1dEt6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"f968njsRtt4g"},"outputs":[],"source":["!pip install pyspark"]},{"cell_type":"markdown","source":["### Creating a spark session"],"metadata":{"id":"M7poK8AddKQM"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import col, udf  # User Defined Functions\n","from pyspark.sql.types import StringType\n","spark = SparkSession.builder.appName('MapReduce').getOrCreate()\n","spark"],"metadata":{"id":"h3MZeCKddM-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accessing sparkContext from sparkSession instance.\n","sc = spark.sparkContext"],"metadata":{"id":"bf5yxPDkdSdl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Creating links.txt file"],"metadata":{"id":"un5ebKq2dVlW"}},{"cell_type":"markdown","source":["An adjacency list representation, info: where ‘A B C’ means that A node (page) out-links to B & C.\n","\n","\n"],"metadata":{"id":"Jx3csMmudY_V"}},{"cell_type":"markdown","source":["Consider this as a mini internet where A, B, C, D are web pages.\n","\n","![img](https://miro.medium.com/max/344/1*7C9YQPLxjVk_oGlnOq185w.png)"],"metadata":{"id":"OQEx_nEb3ylH"}},{"cell_type":"code","source":["# Create/Open the text file\n","f = open(\"links.txt\",\"w+\")\n","\n","# Enter data into the file\n","lst = [\"A B C\", \"B C D\", \"C D\", \"D A\"]          # links \n","for i in lst:\n","    f.write(i + \"\\r\\n\")\n","\n","# Close the file instance  \n","f.close()"],"metadata":{"id":"nDfAoR7cdWL1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Open the links.txt file back and read the contents"],"metadata":{"id":"kdnLwcuhdgIq"}},{"cell_type":"code","source":["f = open(\"links.txt\", \"r\")\n","if f.mode == 'r':\n","    contents = f.read()\n","    print(contents)"],"metadata":{"id":"ulK5jTd4dgr_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Adjacency list\n","links = sc.textFile('links.txt')\n","links.collect()"],"metadata":{"id":"6iDOru8Ldle1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Key/value pairs: the key is the name of the page and the value consists of out-links from the page\n","links = links.map(lambda x: (x.split(' ')[0], x.split(' ')[1:]))\n","print(links.collect())\n","\n","# Find node count\n","N = links.count()\n","print(N)\n","\n","# İnitiate PageRank values (ranks) as 1/(number of pages).\n","ranks = links.map(lambda node: (node[0],1.0/N))\n","print(ranks.collect())"],"metadata":{"id":"R62ttcudd3zm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Perform Map and Reduce steps"],"metadata":{"id":"gBJlW2SWd-jJ"}},{"cell_type":"code","source":["# Map: For each node i, calculate vote for each out-link of i and propagate to adjacent nodes\n","# Reduce: For each node i, sum the upcoming votes and update the Rank value\n","# Repeat this map reduce step until rank values converge\n","\n","ITERATIONS=20\n","for i in range(ITERATIONS):\n","    # Join graph info with rank info and propogate to all neighbors rank scores (rank/(number of neighbors)\n","    # And add up ranks from all in-coming edges\n","    ranks = links.join(ranks).flatMap(lambda x : [(i, float(x[1][1])/len(x[1][0])) for i in x[1][0]])\\\n","    .reduceByKey(lambda x,y: x+y)\n","    print(ranks.sortByKey().collect())"],"metadata":{"id":"ei_u_owgd_NE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* It is evident that out of four dummy web pages under consideration, D has the highest importance or rank.\n","* Web page B has the lowest rank.\n","* It will be best to trust web page D rather than B, for information on it "],"metadata":{"id":"XFcOYGY-326f"}},{"cell_type":"markdown","source":["**References**:\n","1. Taylan Kabbani, 2020. PageRank on MapReduce. *Medium*."],"metadata":{"id":"hp-2_n4_IWZt"}}]}