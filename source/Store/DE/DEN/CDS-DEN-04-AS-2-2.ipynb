{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"CDS-DEN-04-AS-6-8.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"LM_GMDQ-J1uN"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 6: ETL pipeline for Text Mining and Analytics"]},{"cell_type":"markdown","metadata":{"id":"pAZ01Z5tJ1uV"},"source":["At the end of the experiment, you will be able to:\n","\n","* perform text mining and analytics using Spark SQL functions\n","* use Spark’s built-in and external data sources to write data in different file formats as part of the extract, transform, and load (ETL) tasks\n"]},{"cell_type":"markdown","metadata":{"id":"J93wmCVyJ1uW"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"MyHY_53qJ1uX"},"source":["The basic terminology related to text analytics are\n","\n","* **Text**: a sequence of words and punctuation\n","* **Corpus**: a large body of text\n","* **Frequency distribution**: the frequency of words in a text object\n","* **Collocation**: a sequence of words that occur together unusually often\n","* **Bigrams**: word pairs. High frequent bigrams are collocations\n","* **Text normalization**: the process of transforming text into a single canonical form, e.g., converting text to lowercase, removing punctuations and stop words."]},{"cell_type":"markdown","metadata":{"id":"UexUWZb2J1uX"},"source":["### Introduction"]},{"cell_type":"markdown","metadata":{"id":"F2dNzmmEJ1uX"},"source":["Text analytics is the process of deriving information from text. It usually involves information retrieval, lexical analysis to study word frequency distributions, pattern recognition, tagging, information extraction, visualization, and predictive analytics. The overarching goal is, essentially, to turn text into data for analysis, via application of natural language processing (NLP), different types of algorithms and analytical methods.\n","\n","Here we will consider `milton-paradise.txt` text file from Gutenberg corpus to do text mining and analytics. Starting from data extraction, we will perform various transformations on text including tokenization, the number of words counting, POS tagging, chunking and then store it in different file formats."]},{"cell_type":"markdown","metadata":{"id":"GsODE6yLJ1uY"},"source":["### Install Pyspark"]},{"cell_type":"code","metadata":{"id":"PbVweILTJ1uY"},"source":["!pip install pyspark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gjdtWmUaJ1uY"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"i2qaeOmyJ1uZ"},"source":["from pyspark.sql import SparkSession\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import string\n","from pyspark.ml.feature import NGram\n","from pyspark.ml import Pipeline\n","from pyspark.sql.types import *\n","from pyspark.sql.functions import *\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f8ZWzA-VJ1uZ"},"source":["### Start a Spark Session"]},{"cell_type":"markdown","metadata":{"id":"RKKkyz0IJ1uZ"},"source":["Spark session is a combined entry point of a Spark application, which came into implementation from Spark 2.0. Instead of having various context, everything is now encapsulated in a Spark session."]},{"cell_type":"code","metadata":{"id":"pAeBc9XpJ1uZ"},"source":["# Start spark session\n","spark = SparkSession.builder.appName('ETL text data').getOrCreate()\n","spark"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_w1rS1nRJ1ua"},"source":["### Text Analytics"]},{"cell_type":"markdown","metadata":{"id":"QUO-MOywJ1ua"},"source":["#### Get the text data\n","\n","The raw text is from the Gutenberg corpus from the nltk package. Get file ids in Gutenberg corpus."]},{"cell_type":"code","metadata":{"id":"r8JRZaZPJ1ua"},"source":["nltk.download('gutenberg')\n","\n","# Download dependencies for sent_tokenize()\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bkEWZrVWJ1ua"},"source":["from nltk.corpus import gutenberg\n","gutenberg_fileids = gutenberg.fileids()\n","gutenberg_fileids"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0_vRcgahJ1ua"},"source":["The file id is `milton-paradise.txt`. Use the nltk.sent_tokenize() function to split text into sentences."]},{"cell_type":"code","metadata":{"id":"XtvBSiuFJ1ub"},"source":["milton_paradise = gutenberg.raw('milton-paradise.txt')\n","\n","pdf = pd.DataFrame({'sentences': nltk.sent_tokenize(milton_paradise)})\n","d = spark.createDataFrame(pdf)\n","d.show(1, truncate= False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1wiCzYxgJ1ub"},"source":["From above it can be seen that empty spaces are present in the data."]},{"cell_type":"markdown","metadata":{"id":"EW2NfTKfJ1ub"},"source":["#### Transform Data\n","\n","* Remove trailing spaces"]},{"cell_type":"code","metadata":{"id":"iearnIOYJ1ub"},"source":["# Transform data\n","d1 = d.withColumn(\"sentences\", regexp_replace(col(\"sentences\"), \"\\\\s+\",\"_\"))       # replace all spaces with underscore\n","d1 = d1.withColumn(\"sentences\", regexp_replace(col(\"sentences\"), \"_\",\" \"))         # replace all underscores with one space\n","d1 = d1.withColumn(\"sentences\", trim(col(\"sentences\")))                            # remove trailing spaces"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9hVrcNODJ1ub"},"source":["d1.show(5, truncate= False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yrO3vZ67J1uc"},"source":["# Check for empty lines\n","d1.where(col(\"sentences\")==\"\").count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iUTbYzXSJ1uc"},"source":["##### Word Tokenization\n","\n","It is the process of breaking down a paragraph, a sentence or a complete text corpus into an array of words."]},{"cell_type":"code","metadata":{"id":"5TaiZDtkJ1uc"},"source":["from nltk.tokenize import word_tokenize\n","\n","word_udf = udf(lambda x: word_tokenize(x), ArrayType(StringType()))\n","d2 = d1.withColumn(\"words\", word_udf(\"sentences\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VX4130tAJ1uc"},"source":["d2.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F4N9SIngJ1uc"},"source":["From above it can be seen that data has punctuations in it.\n","\n","* **Remove punctuation and stopwords**"]},{"cell_type":"code","metadata":{"id":"DxR9eooPJ1uc"},"source":["# Download stopwords\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rRHaCN0J1ud"},"source":["from nltk.corpus import stopwords\n","stop_words = set(stopwords.words('english'))\n","print(stop_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vSSCh2QeJ1ud"},"source":["\n","punctuation = string.punctuation\n","print(punctuation)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3gKHU3-bJ1ud"},"source":["# Transform data\n","punct_udf = udf(lambda x: [w for w in x if not w in punctuation if not w in stop_words])\n","d3 = d2.withColumn(\"words\", punct_udf(\"words\"))\n","d3.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x7CN5QwlJ1ud"},"source":["# Convert dataframe column to arraytype for further processing\n","\n","array_udf = udf(lambda x: x, ArrayType(StringType()))\n","d4 = d3.withColumn(\"words\", array_udf(\"words\"))\n","d4.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0HgsKRUDJ1ud"},"source":["##### Ngrams and collocations\n","\n","Collocation is a sequence of words that occur together unusually often.\n","Bigrams: word pairs. High frequent bigrams are collocations.\n","\n","Let's see how we transform texts to 2-grams, 3-grams, and 4-grams collocations."]},{"cell_type":"code","metadata":{"id":"oSeoJeoMJ1ue"},"source":["ngrams = [NGram(n=n, inputCol='words', outputCol=str(n)+'-grams') for n in [2,3,4]]\n","\n","# build pipeline model\n","pipeline = Pipeline(stages=ngrams)\n","\n","# transform data\n","texts_ngrams = pipeline.fit(d4).transform(d4)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yxhhYXXrJ1ue"},"source":["# display result\n","texts_ngrams.select('2-grams').show(6, truncate=False)\n","texts_ngrams.select('3-grams').show(6, truncate=False)\n","texts_ngrams.select('4-grams').show(6, truncate=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BozJZvZVJ1ue"},"source":["* Add the number of words column"]},{"cell_type":"code","metadata":{"id":"gnW-5PvwJ1ue"},"source":["# Transform data\n","len_udf = udf(lambda x: len(x), IntegerType())\n","\n","d5 = d4.withColumn(\"no_of_words\", len_udf(\"words\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byxmFkQxJ1ue"},"source":["d5.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iw8-o6gMJ1ue"},"source":["##### **POS (part-of-speech) tagging**\n","\n","It is a process of converting a sentence to forms – list of words, list of tuples (where each tuple is having a form (word, tag)). The tag is a part-of-speech tag and signifies whether the word is a noun, adjective, verb, and so on.\n","\n","To know more about POS tagging click [here](https://medium.com/greyatom/learning-pos-tagging-chunking-in-nlp-85f7f811a8cb)."]},{"cell_type":"code","metadata":{"id":"U8JhY1RqJ1uf"},"source":["# Download dependencies for pos_tag()\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2M-3UpFRJ1uf"},"source":["## define schema for returned result from the udf function\n","## the returned result is a list of tuples\n","schema = ArrayType(StructType([\n","            StructField('f1', StringType()),\n","            StructField('f2', StringType())    ]))\n","\n","sent_to_tag_words_udf = udf(lambda x: nltk.pos_tag(x), schema)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PkckMu1MJ1uf"},"source":["# Transform data\n","d6 = d5.withColumn(\"tagged_words\", sent_to_tag_words_udf(\"words\"))\n","d6.show(5) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1EYWgCKWJ1uf"},"source":["##### **Frequency Distribution Plot**\n","\n","It gives us information about the number of times a word has occurred within a sentence."]},{"cell_type":"code","metadata":{"id":"NWhKkH9-J1uf"},"source":["from nltk.probability import FreqDist\n","\n","row = d6.select('words').toPandas().iloc[0,0]\n","fd = FreqDist(row) \n","fd.plot(30, cumulative= False)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"36AvWojEJ1uf"},"source":["From the above plot it can be seen that in the first row, the word 'Man' has occurred twice."]},{"cell_type":"markdown","metadata":{"id":"IwWhqb88J1uf"},"source":["##### **Chunking**\n","Chunking is the process of grouping similar words together based on the nature of the word. It is the process of segmenting and labeling multitokens. Let's see how to do a noun phrase chunking on the tagged words data frame from the previous step.\n","\n","First we need to define a udf function that chunks noun phrases from a list of pos-tagged words."]},{"cell_type":"code","metadata":{"id":"g08SQc6XJ1ug"},"source":["# define a udf function to chunk noun phrases from pos-tagged words\n","grammar = \"NP: {<DT>?<JJ>*<NN>}\"\n","chunk_parser = nltk.RegexpParser(grammar)\n","chunk_parser_udf = udf(lambda x: str(chunk_parser.parse(x)), StringType())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PrAbDbOGJ1ug"},"source":["# Transform data\n","d7 = d6.withColumn(\"NP_chunk\", chunk_parser_udf(\"tagged_words\"))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v0bVIAQOJ1uh"},"source":["d7.select('NP_chunk').show(1, truncate= False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5l8mSsydJ1uh"},"source":["#### Load data"]},{"cell_type":"markdown","metadata":{"id":"8wG_GKthJ1uh"},"source":["**Use Parquet file to store data**"]},{"cell_type":"code","metadata":{"id":"Jwz7yHbcJ1uh"},"source":["d7.write.format(\"parquet\").mode(\"overwrite\").save(\"transformed_text_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Vn8a8aVKJ1uh"},"source":["**Read data from Parquet file**"]},{"cell_type":"code","metadata":{"id":"zU67l4gUJ1uh"},"source":["df_text_parquet = spark.read.format(\"parquet\").load(\"transformed_text_parquet_data\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yonfCe5wJ1ui"},"source":["df_text_parquet.show(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IsSrVl6bJ1ui"},"source":["**Store the data as a `json file`**"]},{"cell_type":"code","metadata":{"id":"B5Jobed2J1ui"},"source":["d7.write.format(\"json\").mode(\"overwrite\").save('transformed_text_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IxhbRkgOJ1ui"},"source":["**Read data from `json` to spark dataframe**"]},{"cell_type":"code","metadata":{"id":"IXldw5HBJ1ui"},"source":["df_text_json = spark.read.format(\"json\").load('transformed_text_json_data.json')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jcb-LBjAJ1ui"},"source":["df_text_json.show(5)"],"execution_count":null,"outputs":[]}]}