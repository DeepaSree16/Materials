%\tolerance=10000
%\documentclass[prl,twocoloumn,preprintnumbers,amssymb,pla]{revtex4}
\documentclass[prl,twocolumn,showpacs,preprintnumbers,superscriptaddress]{revtex4}
\documentclass{article}
\usepackage{graphicx}
\usepackage{color}
\usepackage{dcolumn}
%\linespread{1.7}
\usepackage{bm}
%\usepackage{eps2pdf}
\usepackage{graphics}
\usepackage{pdfpages}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage[demo]{graphicx} % omit 'demo' for real document
%\usepackage{times}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{subfig}
\usepackage{amsbsy}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{float}
\documentclass{article}
\usepackage{amsmath,systeme}
\usepackage{tikz}

\sysalign{r,r}

% \textheight = 8.5 in
% \topmargin = 0.3 in

%\textwidth = 6.5 in
% \textheight = 8.5 in
%\oddsidemargin = 0.0 in
%\evensidemargin = 0.0 in

%\headheight = 0.0 in
%\headsep = 0.0 in
%\parskip = 0.2in
%\parindent = 0.0in

% \newcommand{\ket}[1]{\left|#1\right\rangle}
% \newcommand{\bra}[1]{\left\langle#1\right|}
\newcommand{\ket}[1]{| #1 \rangle}
\newcommand{\bra}[1]{\langle #1 |}
\newcommand{\braket}[2]{\langle #1 | #2 \rangle}
\newcommand{\ketbra}[2]{| #1 \rangle \langle #2 |}
\newcommand{\proj}[1]{| #1 \rangle \langle #1 |}
\newcommand{\al}{\alpha}
\newcommand{\be}{\beta}
\newcommand{\op}[1]{ \hat{\sigma}_{#1} }
\def\tred{\textcolor{red}}
\def\tgre{\textcolor{green}}


\theoremstyle{plain}
\newtheorem{theorem}{Theorem}

\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{conjecture}[theorem]{Conjecture}

\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}


\begin{document}
\begin{widetext}
\\
\\
\\

\begin{wrapfigure}
\centering
%\includegraphics[\textwidth]{TS_IISc.png}
\end{wrapfigure}
\begin{figure}[h!]
 \begin{right}
  \hfill\includegraphics[\textwidth, right]{TS_IISc.png}
 \end{right}
\end{figure}
%\noindent \textbf{1. Match the following TensorflowLite Model Optimization techniques with ther functions.}
%\\
%\\
%\\
%\begin{figure}[H]
%\begin{center}
%    \includegraphics[width=1.25\textwidth,centering]{cloud_model.pdf}
%\end{center}
%    %\caption{}
%\end{figure}
%\noindent\textbf{1. We can formally describe a Markov Decision Process %as a tuple $(X, A, p, r)$ where:}
%\\
%\\
%A. \\ \\
%$X$ represents the set of all action space (finite, countable, continuous)\\
%\\
%$A$ represents the set of all state space (finite, countable, continuous)
%\\
%\\
%$p$ represents transition probability (probability of observing a next state $y$ when action $a$ is taken while at state $x$
%\\
%\\
%$r$ represents reinforcement reward obtained when taking
%action $a$, while a transition from $x$ to $y$ is observed $r(x, a, y)$
%\\
%\\
%\\
%B. \\ \\
%$X$ represents the set of all action space (finite, countable, continuous)\\
%\\
%$A$ represents the set of all state space (finite, countable, continuous)
%\\
%\\
%$p$ represents reinforcement reward obtained when taking
%action $a$, while a transition from $x$ to $y$ is observed $r(x, a, y)$
%\\
%\\
%$r$ represents transition probability (probability of observing a next %state $y$ when action $a$ is taken while at state $x$
%\\
%\\
%\\
%C. \\ \\
%$X$ represents the set of all action space (finite, countable, continuous)\\
%\\
%$A$ represents transition probability (probability of observing a next state $y$ when action $a$ is taken while at state $x$
%\\
%\\
%$p$ represents the set of all state space (finite, countable, continuous)
%\\
%\\
%$r$ represents reinforcement reward obtained when taking
%action $a$, while a transition from $x$ to $y$ is observed $r(x, a, y)$
%\\
%\\
%\\
%D. \\ \\
%$X$ represents the set of all state space (finite, countable, continuous)\\
%\\
%$A$ represents the set of all action space (finite, countable, continuous)
%\\
%\\
%$p$ represents transition probability (probability of observing a next state $y$ when action $a$ is taken while at state $x$
%\\
%\\
%$r$ represents reinforcement reward obtained when taking
%action $a$, while a transition from $x$ to $y$ is observed $r(x, a, y)$
%\\
%\\
%\\
%\textbf{Answer: D}
\noindent\textbf{1. In a Markov Decision Process, the state and reward at time $t$ depend on which of the following:}
\\
\\
\\
A. Agent Dynamics
\\
\\
B. State-action pair for all time instances before $t$
\\
\\
C. Cumulative reward at time $t$
\\
\\
D. State-action pair for time $(t-1)$
\\
\\
\\
\textbf{Answer: B}
\\
\\
\\
\\
\textbf{2. Consider an MDP $M: (X, A, p,r, \gamma)$, where\\ \\
$X$ is the set of all possible states that environment can be in \\ \\
$A$ is the set of actions that agent can act on environment \\ \\
$p$ represents transition probability (probability of observing a next state $y$ when action $a$ is taken while at state $x$)
\\
\\
$r$ represents reinforcement reward obtained when taking
action $a$, while a transition from $x$ to $y$ is observed $r(x, a, y)$
\\
\\
$\gamma$ is the discount factor, $\gamma \in (0, 1]$}
\\
\\
\\
Which of the following statement(s) gives the interpretation of $\gamma?$
\\
\\
\\
$i$. At each time step there is a $\gamma$ chance that agent dies and does not receive rewards afterwards.
\\
\\
$ii$. One would expect that the agents get better and better. 
\\
\\
$iii$. At each time step there is a $1 - \gamma$ chance that agent dies and does not receive rewards afterwards.
\\
\\
\\
\\
A. $i$ and $ii$ only
\\
\\
B. $ii$ only
\\
\\
C. $ii$ and $iii$
\\
\\
D. $iii$ only
\\
\\
\\
\textbf{Answer: C}
\\
\\
\\
\\
\textbf{3. Which of the following statement(s) is/are TRUE regarding Markov Decision Processes (MDPs):}
\\
\\
\\
$i$. MDPs provide a framework for modelling sequential decision making.
\\
\\
$ii$. At time $t$, the agent (learning systems) observes a state $x_{t} \in X$ of the environment and chooses our action $a_{t} \in A$.
\\
\\
$iii$. Then at $t$ + 1, it receives reward $r_{t+1}$ from the environment $x_{t}$ and environment changes its state to $x_{t+1}$.
\\
\\
$iv$. MDP models the environment and policy function specifies the agent to take an action.
\\
\\
\\
A. $ii$, $iii$ and $iv$
\\
\\
B. $i$, $ii$, $iii$ and $iv$
\\
\\
C. $i$, $ii$, $iii$
\\
\\
D. $i$, $iii$, $iv$
\\
\\
\\
\textbf{Answer: B}
\\
\\
\\
\\
\textbf{4. Reinforcement learning is a branch of machine learning dedicated to training agents to operate in an environment, in order to maximize their utility in the pursuit of a defined goal.}
\\
\\
\\
A. True
\\
\\
B. False
\\
\\
\\
\\
\textbf{Answer: A}
\\
\\
\\
\\
\textbf{5. Which of the following statement(s) is/are true regarding Policy in Reinforcement Learning:}
\\
\\
\\
$i$. A policy function $\pi$ takes in the current state $s$, and
outputs the move/action that the agent should take.
\\
\\
$ii$. Policy can be deterministic or stochastic.
\\
\\
$iii$. A policy which changes at each time step is called Stationary policy
\\
\\
\\
A. $i$ only
\\
\\
B. $ii$ only
\\
\\
C. $i$ and $ii$
\\
\\
D. $iii$ only
\\
\\
\\
\textbf{Answer: C}
\\
\\
\\
\\
\\
\\
\\
\\
\\
\end{widetext}
\end{document} 