{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "M2_AST_12_Image_Classification_(CIFAR_10)_SVM_C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACyro4_xpFwr"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 12: Image Classification (CIFAR-10) using Support Vector Machines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWEKPQW2pWCM"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op3MF_vLpXBE"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand CIFAR-10 dataset\n",
        "* understand the SVM Linear Classifier\n",
        "* perform Binary-Classification using SVM Linear Classifier on CIFAR-10 dataset\n",
        "* extract features using HOG (Histogram of Oriented Gradients) Method\n",
        "* perform Binary-Classification using SVM Linear Classifier on HOG features\n",
        "* perform Multi-class Classification using RBF kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NiLPMhPgYFa"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9CxCComgihL"
      },
      "source": [
        "#### Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0k4a9Qbe6p1"
      },
      "source": [
        "In this experiment, we will use the CIFAR-10 dataset. It consists of 60,000 colour images(32x32) in 10 classes, with 6000 images per class. There are 50,000 training images and 10,000 test images.\n",
        "\n",
        "\n",
        "The dataset is divided into five training batches and one test batch where each batch has 10000 images. The test batch contains 1000 randomly-selected images from each class. The training batches contain the remaining images in random order, but some training batches may contain more images from one class than another. Between them, the training batches contain exactly 5000 images from each class. \n",
        "\n",
        "Here are the classes in the dataset, as well as 10 random images from each:\n",
        "\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/CIFAR10.png\" alt=\"Drawing\" height=\"350\" width=\"440\"/>\n",
        "\n",
        "**The code returns the contents of each data file as a dictionary**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL8erXHHe6p2"
      },
      "source": [
        "There are 8 pickled files in the CIFAR-10 directory.\n",
        "\n",
        "    1. batches.meta\n",
        "\n",
        "    2. data_batch_1\n",
        "\n",
        "    3. data_batch_2\t\n",
        "\n",
        "    4. data_batch_3\n",
        "\n",
        "    5. data_batch_4\t\n",
        "\n",
        "    6. data_batch_5\n",
        "\n",
        "    7. readme.html\n",
        "\n",
        "    8. test_batch\n",
        "\n",
        "Getting into details of this dataset:\n",
        "\n",
        "\n",
        "**data**: A 50,000x3072 numpy array of unsigned integers. Each row of the array stores a 32x32 colour image. The first 1024 intensity values contain the red channel values, the next 1024 intensity values contain the green channel, and the final 1024 the blue channel. The image is stored in row-major order, so that the first 32 entries of the array are the red channel values of the first row of the image. \n",
        "\n",
        "\n",
        "**labels**: A list of 10,000 numbers from 0-9 (for the above mentioned classes airplane, automobile etc..). The number at index i indicates the label of the ith image in the array data.\n",
        "\n",
        "\n",
        "\n",
        "The dataset contains another file, called batches.meta. It too contains a Python dictionary object. It has the following entries:\n",
        "\n",
        "**label_names:**  A 10-element list which gives meaningful names to the numeric labels in the labels array described above. For example, label_names[0] == \"airplane\", label_names[1] == \"automobile\", etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecPwLon_e6p4"
      },
      "source": [
        "### DataSource\n",
        "\n",
        "https://www.cs.toronto.edu/~kriz/cifar.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exG368oL8jv2",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M2_AST_12_Image_Classification_(CIFAR-10)_SVM_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/DS_CIFAR-10_STD.zip\")\n",
        "    ipython.magic(\"sx unzip DS_CIFAR-10_STD.zip\")\n",
        "    # ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Iris.csv\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWNlZz8Ff1F0"
      },
      "source": [
        "### Importing required packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cmlHxf-e6p7"
      },
      "source": [
        "# Importing required packages\n",
        "import numpy as np                    # basic library to work with arrays\n",
        "import seaborn as sns                 # library for statistical data visualization\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt       # basic library for plotting graphs and visualization\n",
        "from sklearn.svm import SVC           # importing Support vector classifier\n",
        "\n",
        "# importing confusion matrix, accuracy score, classification_report\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_RfJjpFG0kp"
      },
      "source": [
        "#### Function to unpickle the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4qKDf-ee6qA"
      },
      "source": [
        "# Function to unpickle the data files which is in the dictionary format\n",
        "def unpickle(file):\n",
        "    with open(file, 'rb') as fo:\n",
        "        # Setting the encoding to latin1 allows to import the data directly\n",
        "        dict_1 = pickle.load(fo, encoding='Latin1')\n",
        "    return dict_1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6119xCxOAbF"
      },
      "source": [
        "When we pass a pickled file to the get_data function it returns features, labels, file names, list of classes of the corresponding file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1_yJnrxJEcr"
      },
      "source": [
        "def get_data(file):\n",
        "    \n",
        "    # Unpickle the data file\n",
        "    dict_1 = (unpickle(file))\n",
        "\n",
        "    # Storing the features\n",
        "    X = dict_1['data']\n",
        "\n",
        "    # Storing the labels\n",
        "    Y = np.asarray(dict_1['labels'])\n",
        "\n",
        "    # Storing the .png files of images\n",
        "    file_names = dict_1['filenames']\n",
        "\n",
        "    # Get the class names \n",
        "    list_class = (unpickle(\"DS_CIFAR-10_STD/batches.meta\")['label_names'])\n",
        "\n",
        "    return X, Y, file_names, list_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "706Q4tnBe6qE"
      },
      "source": [
        "### Visualizing the images in CIFAR-10 Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTnaWfKsWacC"
      },
      "source": [
        "# Read 10000 images -- from data_batch 3 by passing the file to 'get_data' function\n",
        "X, Y, names, classes = get_data(\"DS_CIFAR-10_STD/data_batch_3\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFJp6weiU-AR"
      },
      "source": [
        "fig = plt.figure(figsize=(20, 15))\n",
        "plt_id = 1\n",
        "# Plotting the images by selecting the first 10 images from the 10 classes in the dataset\n",
        "for label in range(10):\n",
        "  for idx, image_id in enumerate(np.where(Y==label)[0][:10], start=1):\n",
        "    plt.subplot(10, 10, plt_id)\n",
        "    # Reshape the images with height x width x channels\n",
        "    img = X[image_id].reshape(3, 32, 32).transpose([1, 2, 0])\n",
        "    plt.title(classes[Y[image_id]])\n",
        "    plt.imshow(img)\n",
        "    plt.axis(\"off\")\n",
        "    plt_id += 1\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7phgU6w1b29y"
      },
      "source": [
        "### Support Vector Machine - Classification (SVC)\n",
        "\n",
        "#### What is SVM?\n",
        "\n",
        "Support vector machines are supervised learning models used for classification and regression analysis. A simple linear SVM classifier works by making a straight line between two classes. That means all of the data points on one side of the line will represent a category and the data points on the other side of the line will be put into a different category. This means there can be an infinite number of lines to choose from.\n",
        "\n",
        "**Hyperplanes** are decision boundaries that help classify the data points. Data points falling on either side of the hyperplane can be attributed to different classes.\n",
        " * The hyperplane with maximum margin is called the optimal hyperplane.\n",
        "\n",
        "#### What are support vectors?\n",
        "\n",
        "* Linear SVM assumes that the data is linearly separable.\n",
        "\n",
        "* It chooses the line which is more distant from both the classes.\n",
        "\n",
        "In the SVM algorithm, we find the points closest to the line from both the classes. These points are called support vectors. \n",
        "\n",
        "**Support vectors** are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Then compute the distance between the line and the support vectors which is called the margin.\n",
        "\n",
        "**Margin** is the width that the boundary could be increased by before hitting a data point.\n",
        "\n",
        "\n",
        "\n",
        "![alt text](https://cdn.talentsprint.com/aiml/aiml_2020_b14_hyd/experiment_details_backup/linear_data.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNT1Qp-DhyR_"
      },
      "source": [
        "### Let us define a function for training the Linear Support Vector Classifier\n",
        "\n",
        "For more details on SVM refer to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBvmhIla-M5O"
      },
      "source": [
        "def train(train_features, train_labels):\n",
        "    \n",
        "    # Create an instance for the LinearSVC classifier\n",
        "    clf = SVC(kernel='linear', random_state=0, max_iter=1000)\n",
        "\n",
        "    # Fitting the data into the model\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUihkHaXiDdD"
      },
      "source": [
        "### Let us define a function to get the prediction on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47eStWTr-nBC"
      },
      "source": [
        "def predict(clf, test_features):\n",
        "    \n",
        "    # Get the prediction on the test data\n",
        "    predictions = clf.predict(test_features)\n",
        "\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S03AOIJW6Elf"
      },
      "source": [
        "### Let us define a function to calculate accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cz-ZheSN-rqe"
      },
      "source": [
        "def eval(predictions, test_labels):   \n",
        "    return accuracy_score(predictions, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w65f_8O3ZkGZ"
      },
      "source": [
        "###  Let us unpickle the data and labels from CIFAR-10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFpkHmA76z46"
      },
      "source": [
        "Now let us unpickle the data and labels from CIFAR-10 dataset and divide them into training and testing sets.."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsRkIdoCe6qq"
      },
      "source": [
        "train_features = []\n",
        "train_labels = []\n",
        "\n",
        "# Read all training features and labels from all the data_batch files\n",
        "for j in \"12345\": \n",
        "    batch_file = 'DS_CIFAR-10_STD/data_batch_'+ j\n",
        "    x_train, y_train, names_train, classes_train = get_data(batch_file)\n",
        "    train_features.extend(x_train)\n",
        "    train_labels.extend(y_train)\n",
        "\n",
        "# Converting the train features and labels in to an array\n",
        "train_features = np.asarray(train_features)\n",
        "train_labels = np.asarray(train_labels)\n",
        "\n",
        "# Read all test features and labels\n",
        "test_features, test_labels, names_test, classes_test = get_data(\"DS_CIFAR-10_STD/test_batch\")\n",
        "\n",
        "# Converting the test features and labels in to an array\n",
        "test_features = np.asarray(test_features)\n",
        "test_labels = np.asarray(test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTS853um7cOl"
      },
      "source": [
        "test_labels.shape, train_labels.shape, test_features.shape, train_features.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HiOaXFw5YFKH"
      },
      "source": [
        "### Let us define a function to extract two classes to perform binary classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMSkyevFwviz"
      },
      "source": [
        "# Function to extract 2 classes to perfrom SVM Linear classification \n",
        "def pick_2classes(class0, class1, X, Y):\n",
        "\n",
        "    # Select class #0\n",
        "    X_0 = X[Y == class0]\n",
        "    Y_0 = Y[Y == class0]\n",
        "\n",
        "    # Select class #1\n",
        "    X_1 = X[Y == class1]\n",
        "    Y_1 = Y[Y == class1]\n",
        "\n",
        "    # Join the two classes (vertically row wise) to make the set\n",
        "    X_classes = np.vstack((X_0, X_1))\n",
        "    Y_classes = np.append(Y_0, Y_1)\n",
        "\n",
        "    return X_classes, Y_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGAVzdP8wx58"
      },
      "source": [
        "# Select the classes #7 and #8 for training and testing the data to get the features\n",
        "# The class 7 belongs to 'horse' and class 8 belongs to 'ship'\n",
        "X_train_data, Y_train_data = pick_2classes(7, 8, train_features, train_labels)\n",
        "X_test_data, Y_test_data = pick_2classes(7, 8, test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ao5XLuw9UJFE"
      },
      "source": [
        "# Check the shape of train and test data sets\n",
        "X_train_data.shape, Y_train_data.shape, X_test_data.shape, Y_test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1u0MdpK7CjwH"
      },
      "source": [
        "# Call the 'train' function with the train data\n",
        "trained_clf = train(X_train_data, Y_train_data)\n",
        "\n",
        "# Call the 'predict' function by paasing the trained classifier and the test data\n",
        "predictions = predict(trained_clf, X_test_data)\n",
        "\n",
        "# Calculate the accuracy by passing the predictions and the test labels\n",
        "accuracy = eval(predictions, Y_test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luM-8iBnWY31"
      },
      "source": [
        "# Print the accuracy score\n",
        "print('Accuracy score is', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf32t_7TiSdo"
      },
      "source": [
        "**Exercise:** You can also select different class labels \n",
        "(0-9) and try passing it to the **pick_2classes** to perform the binary classification using LinearSVM and see the change in accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2P_1SKeDwopi"
      },
      "source": [
        "### Preparing the data to extract the HOG features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Slo3D78deYum"
      },
      "source": [
        "# Define a function to reshape the train and test features\n",
        "def batch_to_rgb(images: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Given loaded images from CIFAR-10 dataset (i.e. 32x32 values\n",
        "    of red, then green and blue), returns same set of images with\n",
        "    color channel being the last, i.e. batch_to_rgb[n][y][x]\n",
        "    returns 3-valued array with r, g, b of pixel (x, y) of n-th \n",
        "    image. It returns same images with transformed colors to rgb\n",
        "    \n",
        "    \"\"\"\n",
        "    return images.reshape((-1, 3, 32, 32)).transpose(0, 2, 3, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF-iSDqbemt5"
      },
      "source": [
        "# Call the function 'batch_to_rgb' on the train and test features\n",
        "train_features = batch_to_rgb(train_features)\n",
        "test_features = batch_to_rgb(test_features)\n",
        "# Check the shape of train features \n",
        "print(train_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7m41A29xtLR"
      },
      "source": [
        "\n",
        "### Feature Representation using HOG (Histogram of Oriented Gradients) Method\n",
        "\n",
        "The histogram of oriented gradients (HOG) is a feature descriptor used in computer vision and image processing for the purpose of object detection. The idea behind HOG is to extract features into a vector, and feed it into a classification algorithm that will assess whether a face (or any object you train it to recognize actually) is present in a region or not.\n",
        "\n",
        "Feature extraction is the process by which certain features of interest within an image are detected and represented for further processing. The resulting representation can be subsequently used as an input to the classification techniques, which will then classify, or recognize the semantic contents of the image or its objects. \n",
        "\n",
        "\n",
        "To identify the objects using HOG method requires the following steps to be followed :-\n",
        "\n",
        "\n",
        "**What is Gradient Image?**\n",
        "\n",
        "<img src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Images/Gradient.png\" alt=\"Drawing\" height=\"200\" width=\"460\"/>\n",
        "\n",
        "We start with computing the horizontal and vertical gradients of the image. In the image when we move from left to right pixel by pixel, there might be a change in the pixel value i.e, from a black lower pixel number to a white higher pixel number. Going from left to right gives us the horizontal gradient and going from top to down gives a vertical gradient. This sudden change in the color is called a gradient. \n",
        "\n",
        "The gradient image removes a lot of non-essential information (e.g. constant colored background), but highlighted outlines and still we can identify the image. At every pixel, the gradient has a magnitude and a direction.\n",
        "\n",
        "**Orientation of Gradients**\n",
        "\n",
        "HOG works with a block which is similar to a sliding window. A block is considered as a pixel grid in which gradients are constituted from the magnitude and direction of change in the intensities of the pixel within the block.\n",
        "\n",
        "We first calculate the gradients by taking a block from the images. For the selected block, determine the gradient (or change) in the x-direction and aslo calculate the gradient in the y-direction. Once we get the gradients, we will also calculate the gradient magnitude and gradient angle for each pixel (in the image).\n",
        "\n",
        "**Histogram of Gradients**\n",
        "    \n",
        "The next step is to create a histogram of gradients for the block of pixels from the image. We will take each pixel value, find the angle/orientation of the pixel and update the frequency table. The same process is repeated for all the pixel values and we end up with a frequency table that denotes angles and the occurrence of these angles in the image. This frequency table can be used to generate a histogram with angle values on the x-axis and the frequency on the y-axis.\n",
        "\n",
        "From the HOG features we will find that the structure of the object is well maintained, ignoring all the insignificant features.\n",
        "\n",
        "For more details refer to the following [link](https://medium.com/analytics-vidhya/a-gentle-introduction-into-the-histogram-of-oriented-gradients-fdee9ed8f2aa)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98hebNfVcLFA"
      },
      "source": [
        "# we will use the methods from scikit-image library, hog() and rgb2gray().\n",
        "from skimage.color import rgb2gray\n",
        "\n",
        "# First convert the train and test features to grayscale\n",
        "# The HOG method works for the gray scale image\n",
        "hog_train = rgb2gray(train_features)\n",
        "hog_test  = rgb2gray(test_features)\n",
        "\n",
        "# Picking up the horse index from train_features\n",
        "horse = train_features[2000]\n",
        "\n",
        "# Plotting the original image\n",
        "fig, ax = plt.subplots(1, 2, figsize=(5, 5))\n",
        "ax[0].imshow(horse)\n",
        "ax[0].set_title(\"Original\")\n",
        "\n",
        "# Plotting the coverted image for the selected index of horse\n",
        "horse_gray = hog_train[2000]\n",
        "\n",
        "# Plotting the gray scale image\n",
        "ax[1].imshow(horse_gray, cmap='gray')\n",
        "ax[1].set_title(\"Grayscaled\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcsHR6kugRqH"
      },
      "source": [
        "# We use the hog function from skimage.features directly. \n",
        "# So we don’t have to calculate the gradients, magnitude (total gradient) and orientation individually. \n",
        "# The hog function would internally calculate it and return the feature matrix.\n",
        "from skimage.feature import hog\n",
        "\n",
        "# Creating HOG feature Descriptor\n",
        "# First we pass the the gray scale image of horse\n",
        "# Set the parameter ‘visualize = True’, which return an image of the HOG\n",
        "fd, hog_horse = hog(horse_gray, visualize=True)\n",
        "\n",
        "# Plotting the original image\n",
        "fig, ax = plt.subplots(1, 2, figsize=(5, 5))\n",
        "ax[0].imshow(horse)\n",
        "ax[0].set_title(\"Original\")\n",
        "\n",
        "# Plotting the HOG image\n",
        "ax[1].imshow(hog_horse, cmap='gray')\n",
        "ax[1].set_title(\"HOG\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKi-rgXdgj9Q"
      },
      "source": [
        "From the above we can see the edges of the horse. Even it might be harder to identify a horse on the HOG image, compared to the original image, we can see that only the most important parts are preserved. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPOlPQArsqh7"
      },
      "source": [
        "# Joblib is an pacakge that can simply turn our Python code into parallel \n",
        "# computing mode and increases the computing speed\n",
        "# with the Parallel and delayed functions from Joblib, we can simply configure a parallel run of the below function\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "# Function which converts the train and test images to hog images\n",
        "def to_hog(images):\n",
        "    return hog(images)\n",
        "\n",
        "# Get to hog features from the train and test sets\n",
        "# n_jobs is the number of parallel jobs.\n",
        "# 'delayed(to_hog)(x) for x in hog_train' creates tuple of the function, x, and the parameters, one for each iteration. \n",
        "# Delayed creates these tuples, then Parallel will pass these to the interpreter.\n",
        "# Parallel(n_jobs=num_cores) does the heavy lifting of multiprocessing. \n",
        "# Parallel forks the Python interpreter into a number of processes equal to the number of jobs. \n",
        "# Each process will run one iteration, and return the result.\n",
        "\n",
        "hog_train = np.asarray(Parallel(n_jobs=-1)(delayed(to_hog)(x) for x in hog_train))\n",
        "hog_test = np.asarray(Parallel(n_jobs=-1)(delayed(to_hog)(x) for x in hog_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2oEDIFYkoTwG"
      },
      "source": [
        "# Check for the shape of hog train and test features\n",
        "hog_train.shape, hog_test.shape, train_labels.shape, test_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6hEN6aUoV0j"
      },
      "source": [
        "# Select classes #7 and #8 to extract the HOG features\n",
        "# Performing binary classification by selecting 2 classes from the data\n",
        "# The class 7 belongs to 'horse' and class 8 belongs to 'ship'\n",
        "X_train_hog, Y_train_hog = pick_2classes(7, 8, hog_train, train_labels)\n",
        "X_test_hog, Y_test_hog = pick_2classes(7, 8, hog_test, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7KbZxr5SmZf"
      },
      "source": [
        "# Check for the shape of HOG train and test features\n",
        "X_train_hog.shape, X_test_hog.shape, Y_train_hog.shape, Y_test_hog.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAWy89F2yLf5"
      },
      "source": [
        "### Train the classifier and get the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2FP7cIgl08I"
      },
      "source": [
        "# Call the functions to train the classifier and get the predictions\n",
        "trained_clf_hog = train(X_train_hog, Y_train_hog)\n",
        "predictions_hog = predict(trained_clf_hog, X_test_hog)\n",
        "# Calculate the accuracy\n",
        "result = eval(predictions_hog, Y_test_hog)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX12j8eVweCm"
      },
      "source": [
        "# Print the accuracy on HOG features\n",
        "print('Accuracy score of HOG features is:', result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYojlg24wmpe"
      },
      "source": [
        "**observation:** From the above experiment, we observe that by using the HOG method we get a better representation of the features considering the important parts of the images. The accuracy for the HOG features is 92.6% using Linear Support Vector Classifier when compared to the original features directly passing to the classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvSYDW45txVn"
      },
      "source": [
        "### Model Evaluation on HOG features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPbsk_17u0vY"
      },
      "source": [
        "#### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnSKar6Xtu9K"
      },
      "source": [
        "# Print the classification report\n",
        "print(classification_report(Y_test_hog, predictions_hog))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7lGgbKUvMyK"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4GBnOVuu8IC"
      },
      "source": [
        "mat = confusion_matrix(Y_test_hog, predictions_hog)\n",
        "\n",
        "plt.figure(figsize = (12,5))\n",
        "\n",
        "# Visualizing the confusion matrix as a heatmap\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFicwdnmnfCZ"
      },
      "source": [
        "### Multi-Class Classification using Kernelized SVMs on CIFAR-10 dataset\n",
        "\n",
        "**Note:** For more details on RBF kernel refer to the following [link](https://colab.research.google.com/drive/1C59StkNs68wKZWwHxJeFbHjLxGCPynao?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIGuPAx4byKm"
      },
      "source": [
        "### Let's define a function to train the SVM using 'RBF' kernel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIcw6wdsTug-"
      },
      "source": [
        "def train_kernel(train_features, train_labels):\n",
        "    \n",
        "    # Create an instance for the 'rbf' classifier\n",
        "    clf = SVC(kernel='rbf', max_iter=1000)\n",
        "\n",
        "    # Fitting the data to the model\n",
        "    clf.fit(train_features, train_labels)\n",
        "\n",
        "    return clf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahWEXdgPqygq"
      },
      "source": [
        "### Let's define a function to extract 3 classes to perfrom Multi-class classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9PnN0Wop8Gx"
      },
      "source": [
        "# Function to extract 3 classes to perform SVM multi-class classification \n",
        "def pick_3classes(class0, class1, class2, X, Y):\n",
        "    \n",
        "    # Select class #0\n",
        "    X_0 = X[Y == class0]\n",
        "    Y_0 = Y[Y == class0]\n",
        "\n",
        "    # Select class #1\n",
        "    X_1 = X[Y == class1]\n",
        "    Y_1 = Y[Y == class1]\n",
        "\n",
        "    # Select class #2\n",
        "    X_2 = X[Y == class2]\n",
        "    Y_2 = Y[Y == class2]\n",
        "\n",
        "    # Join the two classes (vertically row wise) to make the set\n",
        "    X_classes = np.vstack((X_0, X_1, X_2))\n",
        "    Y_classes = np.hstack((Y_0, Y_1, Y_2))\n",
        "\n",
        "    return X_classes, Y_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7k7nIGMp8G8"
      },
      "source": [
        "# Select the classes #2, #9, #4 for training and testing the data to get the features\n",
        "# The class 2 belongs to 'bird', class 9 belongs to 'truck' and class 4 belongs to 'deer'\n",
        "X_train, Y_train = pick_3classes(2, 9, 4, train_features, train_labels)\n",
        "X_test, Y_test = pick_3classes(2, 9, 4, test_features, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOTvYGFkLHfF"
      },
      "source": [
        "# Check for the shape of train and test sets\n",
        "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Azqr1utQIWo2"
      },
      "source": [
        "# Reshape the data of train and test data sets before passing it to the model\n",
        "X_train = X_train.flatten().reshape(15000, 3072)\n",
        "X_test = X_test.flatten().reshape(3000, 3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-23GHpNj_VV"
      },
      "source": [
        "The below code cell (for training the data with rbf kernel) takes around 5-7mins to complete it's execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVtMLy53IS_Y"
      },
      "source": [
        "# Call the 'train' function with the train data\n",
        "rbf_clf = train_kernel(X_train, Y_train)\n",
        "\n",
        "# Call the 'predict' function by paasing the trained classifier and the test data\n",
        "predictions_rbf = predict(rbf_clf, X_test)\n",
        "\n",
        "# Calculate the accuracy by passing the predictions and the test labels\n",
        "accuracy = eval(predictions_rbf, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFYqP-ESSQuD"
      },
      "source": [
        "# Print the accuracy score\n",
        "print('Accuracy score using rbf kernel is', accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdqeAROym5w6"
      },
      "source": [
        "### Let's define a function to extract 4 classes to perfrom Multi-class classification\n",
        "\n",
        "You can uncomment the below code cell to perform the Multi-class classification on 4 classes (optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hR99p52Amnvo"
      },
      "source": [
        "# Function to extract 4 classes to perform SVM multi-class classification \n",
        "# def pick_3classes(class0, class1, class2, class3, X, Y):\n",
        "    \n",
        "#     # Select class #0\n",
        "#     X_0 = X[Y == class0]\n",
        "#     Y_0 = Y[Y == class0]\n",
        "\n",
        "#     # Select class #1\n",
        "#     X_1 = X[Y == class1]\n",
        "#     Y_1 = Y[Y == class1]\n",
        "\n",
        "#     # Select class #2\n",
        "#     X_2 = X[Y == class2]\n",
        "#     Y_2 = Y[Y == class2]\n",
        "\n",
        "#     # Select class #3\n",
        "#     X_3 = X[Y == class3]\n",
        "#     Y_3 = Y[Y == class3]\n",
        "\n",
        "#     # Join the two classes (vertically row wise) to make the set\n",
        "#     X_classes = np.vstack((X_0, X_1, X_2, X_3))\n",
        "#     Y_classes = np.hstack((Y_0, Y_1, Y_2, Y_3))\n",
        "\n",
        "#     return X_classes, Y_classes\n",
        "\n",
        "# Select the classes #3, #6, #2, #7 for training and testing the data to get the features\n",
        "# The class 3 belongs to 'cat', class 6 belongs to 'frog', class 2 belongs to 'bird' and class 7 belongs to 'horse'\n",
        "# X_train, Y_train = pick_3classes(3, 6, 2, 7, train_features, train_labels)\n",
        "# X_test, Y_test = pick_3classes(3, 6, 2, 7, test_features, test_labels)\n",
        "\n",
        "# Reshape the data of train and test data sets before passing it to the model\n",
        "# X_train = X_train.flatten().reshape(20000, 3072)\n",
        "# X_test = X_test.flatten().reshape(4000, 3072)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90rxqrbnr7S6"
      },
      "source": [
        "### Extracting HOG features for 3 classes to perfrom Multi-class classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNujYuEERCvr"
      },
      "source": [
        "# Select the classes #2, #9, #4 for training and testing the data to get the features\n",
        "# The class 2 belongs to 'bird', class 9 belongs to 'truck' and class 4 belongs to 'deer'\n",
        "X_train_hog_multi, Y_train_hog_multi = pick_3classes(2, 9, 4, hog_train, train_labels)\n",
        "X_test_hog_multi, Y_test_hog_multi = pick_3classes(2, 9, 4, hog_test, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSLQ1SfIShVl"
      },
      "source": [
        "# Check for the shape of train and test sets\n",
        "X_train_hog_multi.shape, X_test_hog_multi.shape, Y_train_hog_multi.shape, Y_test_hog_multi.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLPFPsjzsa2l"
      },
      "source": [
        "# Call the functions to train the classifier and using 'rbf' kernel and get the predictions\n",
        "\n",
        "clf_hog = train_kernel(X_train_hog_multi, Y_train_hog_multi)\n",
        "\n",
        "hog_predictions = predict(clf_hog, X_test_hog_multi)\n",
        "\n",
        "result_score = eval(hog_predictions, Y_test_hog_multi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUifEjm2sqZz"
      },
      "source": [
        "# Print the accuracy score\n",
        "print('Accuracy score using rbf kernel on HOG features is:', result_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpv4Ww3uuylR"
      },
      "source": [
        "### Model Evaluation for Multi-Class using HOG features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSf9UCRsuylW"
      },
      "source": [
        "#### Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rCXxYt5-uylW"
      },
      "source": [
        "# Print the classification report\n",
        "print(classification_report(Y_test_hog_multi, hog_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUlJMWP_uylc"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyph7he-uyll"
      },
      "source": [
        "mat = confusion_matrix(Y_test_hog_multi, hog_predictions)\n",
        "\n",
        "plt.figure(figsize = (12,5))\n",
        "\n",
        "# Visualizing the confusion matrix as a heatmap\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True)\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWjZeb4uvlty"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRwvwQ8_VbWR"
      },
      "source": [
        "#### Consider the following statement about SVM and answer Q1.\n",
        "\n",
        "A. SVM is a supervised machine learning algorithm, which cannot be used for classification but can be used for regression.\n",
        "\n",
        "B. SVM is an unsupervised machine learning algorithm, which can be used for classification but not for regression.\n",
        "\n",
        "C. SVM is a supervised machine learning algorithm, which can be used for classification as well as for regression.\n",
        "\n",
        "D. SVM is an unsupervised machine learning algorithm, which can be used for classification as well as for regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNsVIkpnDpLc",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. Which of the following is true for a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer1 = \"\" #@param [\"\",\"Only A\",\"Only C\",\"Both A and C\",\"Both B and D\",\"All of the above\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDFfVBAyWPCL"
      },
      "source": [
        "#### Consider the following statement about SVM Kernels and answer Q.2.\n",
        "\n",
        "A. An SVM Kernel basically transforms lower dimensional data to higher dimensional space for easier segregation of the data.\n",
        "\n",
        "B. An SVM Kernel transforms the linearly inseparable data (or non-linear data) in a lower dimension space into a linearly separable one by projecting it into a higher dimension space using the Kernel trick.\n",
        "\n",
        "C. An SVM Kernel transforms the linearly inseparable data (or non-linear data) in a higher dimension space into a linearly separable one by projecting it into a lower dimension space using the Kernel trick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. Which of the following is true about the Kernel in Support Vector Machines?\n",
        "Answer2 = \"\" #@param [\"\",\"Only A\",\"Only B\",\"Only C\",\"Both A and C\",\"Both A and B\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}