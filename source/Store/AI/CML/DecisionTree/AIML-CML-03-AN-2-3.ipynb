{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-CML-03-AN-2-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"wHgBhEimyTQK"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","## Not for grading"]},{"cell_type":"markdown","metadata":{"id":"tqgerKxHsB4o"},"source":["## Learning Objective"]},{"cell_type":"markdown","metadata":{"id":"B4EetlousExQ"},"source":["The objective of this experiment is to: \n","\n","* understand Decision tree classifier \n","* visualize the decision boundaries\n","* understand overfitting"]},{"cell_type":"markdown","metadata":{"id":"FWRoF8wDuFhR"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"Zo5rkNr1xMKm"},"source":["Two handmade datasets (A,B) are chosen for this experiment"]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/A.csv\n","! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/B.csv.zip\n","! unzip B.csv.zip"],"metadata":{"id":"89CEgWKSHuyy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y22XdpvMdbE0"},"source":["### Importing the required packages"]},{"cell_type":"code","metadata":{"id":"ySc0yr3UyPRC"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","from mlxtend.plotting import plot_decision_regions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3WYGizycmleg"},"source":["###  Considering A as train data and B as test data"]},{"cell_type":"code","metadata":{"id":"T3BtlwCcyPRj"},"source":["A_train = pd.read_csv(\"A.csv\")\n","A_train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XriY9-HbyPRo"},"source":["B_test = pd.read_csv(\"B.csv\", names=['X','Y','Label'])\n","B_test.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cpoopr1PnCh2"},"source":["#### Visualize the data to get more sense of data\n","\n","\n","Matplotlib has a number of built-in colormaps using cmap, For more details refer to the following [link](https://matplotlib.org/tutorials/colors/colormaps.html)"]},{"cell_type":"code","metadata":{"id":"Vd2r9Hrdmbtt"},"source":["plt.scatter(A_train.X, A_train.Y, c= A_train.Label, cmap='RdYlBu', s=50, label=\"Train\") # s is marker size\n","plt.scatter(B_test.X, B_test.Y, c=B_test.Label, cmap='RdYlBu', marker=\"*\", s=150, label=\"Test\")\n","plt.xlabel(\"X\")\n","plt.ylabel(\"Y\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0sVevuNCj_Gk"},"source":["# Get the train and test sets from the data \n","X_train, y_train = A_train[['X','Y']].values, A_train['Label'].values\n","X_test, y_test = B_test[['X','Y']].values, B_test['Label'].values "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mOBtg6f0nTOB"},"source":["#### Train and Visualize a Decision Tree Classifier at each depth"]},{"cell_type":"code","metadata":{"id":"tx9Ny_-tjD-t"},"source":["# Training a decision tree classifier at each depth\n","\n","for depth in range(1,5):\n","        \n","        # max_depth parameter regularize the tree, or limit the way it grows to prevent over-fitting\n","        clf = DecisionTreeClassifier(criterion='gini', max_depth = depth)\n","        clf = clf.fit(X_train, y_train)\n","        training_acc = accuracy_score(clf.predict(X_train),y_train)\n","        testing_acc = accuracy_score(clf.predict(X_test),y_test)\n","        print(\"Training Accuracy: \"+str(round(training_acc,2)) + \" \" +  \"Testing Accuracy: \" + str(round(testing_acc,2)) + \" \" + \n","              \"At Depth: \" + str(depth)) \n","        plot_decision_regions(X_train, y_train, clf=clf, legend=2) \n","        plt.show()"],"execution_count":null,"outputs":[]}]}