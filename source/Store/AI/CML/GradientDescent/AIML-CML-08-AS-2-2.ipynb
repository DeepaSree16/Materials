{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-CML-08-AS-2-2.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UERx5ALuTJW_"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"aE5xNmtpBcT4"},"source":["## Learning Objective:"]},{"cell_type":"markdown","metadata":{"id":"K_UItlExBcPR"},"source":["   \n","  At the end of the experiment, you will be able to :\n","    \n","  * Understand various types of gradient descent approaches (Stochastic, Mini-Batch Gradient Descent) and their differences.\n"]},{"cell_type":"code","metadata":{"id":"lo6h67YIkEKp","cellView":"form"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","HTML(\"\"\"<video width='854' height='480' controls>\n","<source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Week_1/stochastic_minibatch_gradient_descent_2.mp4\" type='video/mp4'>\n","</video>\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2U7msw6GTJXB"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"PBplK_Hbw_EN"},"source":["### Description"]},{"cell_type":"markdown","metadata":{"id":"mDRTMSzJTJXD"},"source":["\n","The dataset consists of two columns and 90 rows. Each column represents a characteristic of a simple pendulum i.e l (length) and t (time period). The dataset describes the relationship between the l and t which is  $l∝t^2$ .\n"]},{"cell_type":"markdown","metadata":{"id":"Z8WpDaz2CukI"},"source":["##AI/ML Technique"]},{"cell_type":"markdown","metadata":{"id":"1jAMa55_xLmA"},"source":["#### Gradient Descent\n","\n","Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on  first order gradients, that tweaks it’s parameters iteratively to minimize a given function to its local minimum and global minima if the function is convex\n"]},{"cell_type":"code","source":["!wget -qq https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/Exp1/AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt"],"metadata":{"id":"pC3n8iB_PDA4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hmvlAERcEXCn"},"source":["## Import the required Packages"]},{"cell_type":"code","metadata":{"id":"sRjAjk1hTJXM"},"source":["import pandas as pd\n","import numpy as np\n","from  matplotlib import pyplot as plt\n","import random\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLdAeF_tTJXR"},"source":["## Load the data"]},{"cell_type":"code","metadata":{"id":"YDNrlfakTJXS"},"source":["# Load the data by using pandas read_csv()\n","data = pd.read_csv(\"AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\n","# Print the first 5 rows of dataframe 'data'\n","print(data.head())\n","# Print the last 5 rows of dataframe 'data'\n","print(data.tail())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QTtgOro7TJXT"},"source":["# Get the length and time period values from the dataset\n","l = data['l'].values\n","t = data['t'].values\n","# Get the square of the time variable to form a linear equation\n","tsq = t * t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"srgyaBxoTJXW"},"source":["## Stochastic gradient descent (Single sample)\n","\n","Instead of computing the sum of all gradients, stochastic gradient descent selects an observation uniformly at random."]},{"cell_type":"markdown","metadata":{"id":"zV7hm0GDdg0x"},"source":[" $y_i = mx_i + c$\n","\n","$E$ = $(y - y_i)^2$\n","\n","\n","$\\frac{\\partial E_i }{\\partial m}$ = $ -2(y_i - (mx_i + c)) * x_i$\n","\n","$\\frac{\\partial E_i }{\\partial c}$ = $ -2(y_i - (mx_i + c))$\n","\n","And then we update the slope and bias with change in slope $\\Delta m$ and change in bias $\\Delta c$ with learning rate $eta$\n","\n","$m$  = $m - \\Delta m * eta$\n","\n","$c$  = $c - \\Delta c * eta$\n"]},{"cell_type":"code","metadata":{"id":"hUq_mCNLTJXY"},"source":["\"\"\"\n","The function 'next_step' updates the values of m and c and calculates error. \n","The loss is minimized due to the changed values of m and c.\n","The new values m, c and the minimized loss is returned.\n","\"\"\"\n","def next_step(x, y, m, c, eta):\n","    ycalc = m * x + c\n","    error = (y - ycalc) ** 2\n","    delta_m = -(y - ycalc) * x * 2\n","    delta_c = -(y - ycalc) * 2\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    return m, c, error\n","\n","\"\"\"\n","The function below takes a random index and at that index idx, we calculate the values of m,c and error.\n","We use one data point at a time x[idx],y[idx]\n","Here we call the funtion 'next_step' to which we pass a data point x[idx],y[idx]\n","\"\"\"\n","\n","def one_loop_random(x, y, m, c, eta):\n","    # Making random idx\n","    random_idx = np.arange(len(y))\n","    # Training with random idx\n","    for idx in random_idx:\n","        m, c, e = next_step(x[idx], y[idx], m, c, eta)\n","    return m,c,e\n","  \n","\"\"\"\n","The function below trains the data for 1000 iterations. \n","In each iteration it calls the 'one_loop_random' function.\n","\"\"\"\n","def train_stochastic(x, y, m, c, eta, iterations=1000):\n","    for iteration in range(iterations):\n","        m, c, err = one_loop_random(x, y, m, c, eta)\n","    return m, c, err"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HM2O8HtFTJXa"},"source":["### TRAIN"]},{"cell_type":"code","metadata":{"id":"JlR-YQJoTJXd"},"source":["# Initialize m, c\n","m, c = 0, 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kijRetDpTJXg"},"source":["# Learning rate\n","lr = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8FDMh5YtTJXk"},"source":["# Training for 1000 iterations, plotting after every 100 iterations:\n","fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111)\n","\n","# Call the train_stochastic() method to update m and c and get error value with lr = 0.001.\n","for num in range(10):\n","    # We will plot the error values for every 100 iterations and error value will decrease for each iteration\n","    m, c, error = train_stochastic(l, tsq, m, c, lr, iterations=100) \n","    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n","    y = m * l + c\n","    ax.clear()\n","    ax.plot(l, tsq, '.k')\n","    ax.plot(l, y)\n","    time.sleep(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UwIZ7tuZTJXx"},"source":["### PROBLEM\n","\n","Problem with Sequential/Stochastic Gradient Descent is it does not scale well - it makes the same calculation of gradient descent on each sample. So the time taken will increase linearly with the number of samples. Many datasets have samples in the range of millions. Hence, even though it gives good results, it is not ideal.\n","\n","We need a gradient descent formulation that gives the speed of vanilla gradient descent and the accuracy of sequential/stochastic gradient descent.\n","\n","Next we will see **Minibatch Gradient Descent!**"]},{"cell_type":"markdown","metadata":{"id":"m3S-IvdHTJXx"},"source":["## Minibatch Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"ZT6lGuO8TJXy"},"source":["In Mini-Batch Gradient Descent algorithm, rather than using  the complete data set, in every iteration you use a subset of training examples (called \"batch\") to compute the gradient of the cost function. \n","\n","Common mini-batch sizes range between 50 and 256, but can vary for different applications."]},{"cell_type":"markdown","metadata":{"id":"upmgb7uzfdox"},"source":["train_one_batch() : we will be calculating the essenial parts of the Gradient Descent method: \n","\n","We assume there are $n$ samples in a batch $B$, for all  $i \\in B$, \n","\n","$y_i = mx_i + c$\n","        \n","$E$ =$\\frac{1}{n}$   $\\sum_{i=1}^n (y - y_i)^2$\n","\n","$\\frac{\\partial E }{\\partial m}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -x_i(y - (mx_i + c))$\n"," \n","$\\frac{\\partial E}{\\partial c}$ = $\\frac{2}{n}$   $\\sum_{i=1}^n  -(y - (mx_i + c))$\n","\n","And then we update the slope and bias with with change in slope $\\Delta m$ and change in bias $\\Delta c$ with learning rate $eta$\n","\n","$m$  = $m - \\Delta m * eta$\n","\n","$c$  = $c - \\Delta c * eta$"]},{"cell_type":"markdown","metadata":{"id":"mJgPCef6TJXz"},"source":["train_batches() : We will be splitting our data into batches."]},{"cell_type":"code","metadata":{"id":"JkRMHxA2TJX0"},"source":["\"\"\"\n","The function 'train_one_batch' updates the values of m and c and calculates error. \n","The loss is minimized due to the changed values of m and c.\n","The new values m, c and the minimized loss is returned.\n","\"\"\"\n","def train_one_batch(x, y, m, c, eta):\n","    const = - 2.0/len(y)\n","    ycalc = m * x + c\n","    delta_m = const * sum(x * (y - ycalc))\n","    delta_c = const * sum(y - ycalc)\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    error = sum((y - ycalc)**2)/len(y)\n","    return m, c, error\n","\n","\"\"\"\n","The function below takes a batch_size and loss is calculated w.r.t batches.\n","The batches are created using random index.\n","The m, c and error values are calculated for each batch of data.\n","So, it calls the function 'train_one_batch' by passing batch_x, batch_y for each batch.\n","\"\"\"\n","def train_batches(x, y, m, c, eta, batch_size):\n","    # Making the batches\n","    random_idx = np.arange(len(y)) \n","\n","    # Train each batch\n","    for batch in range(len(y)//batch_size):\n","        batch_idx = random_idx[batch*batch_size:(batch+1)*batch_size]\n","        batch_x = x[batch_idx]\n","        batch_y = y[batch_idx]\n","        m, c, err = train_one_batch(batch_x, batch_y, m, c, eta)  \n","    return m, c, err\n","\n","\"\"\"\n","The function below trains the data for 1000 iterations. \n","The data is traversed in batches, the batch size here is considered to be 10.\n","In each iteration it calls the 'train_batches' function. \n","The 'batch_size' is passed as a parameter to 'train_batches'.\n","\"\"\"\n","def train_minibatch(x, y, m, c, eta, batch_size, iterations=1000):\n","    for iteration in range(iterations):\n","        m, c, err = train_batches(x, y, m, c, eta, batch_size)\n","    return m, c, err"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-BLipYesTJX4"},"source":["#### TRAIN"]},{"cell_type":"code","metadata":{"id":"oAsrGHdLTJX4"},"source":["# Initializing m, c\n","m, c = 0, 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BXm2ZSEwTJX8"},"source":["# Learning rate\n","lr = 0.01"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Y-XhUFqTJYB"},"source":["# Training for 1000 iterations, plotting after every 100 iterations:\n","fig = plt.figure(figsize=(5, 5))\n","ax = fig.add_subplot(111)\n","\n","\n","# Call the train_minibatch() method to update m and c and get error value with lr = 0.01 and batch_size=10.\n","for num in range(10):\n","    # We will plot the error values for every 100 iterations\n","    m, c, error = train_minibatch(l, tsq, m, c, lr, batch_size=10, iterations=100) \n","    print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m, c, error))\n","    y = m * l + c\n","    ax.clear()\n","    ax.plot(l, tsq, '.k')\n","    ax.plot(l, y)\n","    time.sleep(1)"],"execution_count":null,"outputs":[]}]}