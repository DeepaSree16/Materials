{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-CML-08-AS-1-2.ipynb","provenance":[{"file_id":"1ga88fRoXmt9BRzgWvgj2kWoMoLNrBUMm","timestamp":1548224580383}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"tao2w0W4IwEO"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2Pkm2A7Ss0g1"},"source":["### Learning Objective"]},{"cell_type":"markdown","metadata":{"id":"YEjLydYUgjDV"},"source":["At the end of the experiment, you will be able to :\n","\n","- Understand the concept of Gradient descent method\n","- Observe the effect of learning rate"]},{"cell_type":"code","metadata":{"id":"XDbRvIzDp_K8","cellView":"form"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","HTML(\"\"\"<video width='854' height='480' controls>\n","<source src=\"https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/gradient_descent.mp4\" type='video/mp4'>\n","</video>\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"og1EHpbjgrHP"},"source":["##Dataset"]},{"cell_type":"markdown","metadata":{"id":"rSwgCEA_uJ40"},"source":["###Description"]},{"cell_type":"markdown","metadata":{"id":"mDRTMSzJTJXD"},"source":["\n","The dataset consists of two columns and 90 rows. Each column represents a characteristic of a simple pendulum i.e l (length) and t (time period). The dataset describes the relationship between the l and t which is  $l∝t^2$ .\n"]},{"cell_type":"markdown","metadata":{"id":"nn9drxhC3f6e"},"source":["##AI/ML Technique"]},{"cell_type":"markdown","metadata":{"id":"1jAMa55_xLmA"},"source":["#### Gradient Descent\n","\n","Gradient Descent is used while training a machine learning model. It is an optimization algorithm, based on  first order gradients, that tweaks it’s parameters iteratively to minimize a given function to its local minimum and global minima if the function is convex\n"]},{"cell_type":"code","source":["!wget -qq https://cdn.talentsprint.com/aiml/Experiment_related_data/week1/Exp1/AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt"],"metadata":{"id":"gEYrYrybOosl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHOajhjcuOGA"},"source":["### Import required Packages\n"]},{"cell_type":"code","metadata":{"id":"HxcZUqq5IwEY"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ecrcyzhvuRu2"},"source":["### Load the data"]},{"cell_type":"code","metadata":{"id":"5Ttnpu44IwEb"},"source":["# Load the data by using pandas read_csv()\n","data = pd.read_csv(\"AIML_DS_REGR01_SIMPLEPENDULUMOSCILLATIONDATA.txt\", sep=\" \", header=None, names=['l', 't'])\n","# Print the first 5 rows of dataframe 'data'\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7_l6XKV10o89"},"source":["# Print the last 5 rows of dataframe 'data'\n","data.tail()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZGdRFp1LIwEd"},"source":["# Get the length and time period values from the dataset\n","l = data['l'].values\n","t = data['t'].values\n","# Get the square of the time variable to form a linear equation\n","tsq = t * t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"79f75Z8KRRIs"},"source":["### Gradient Descent\n","\n","Gradient descent is an optimization algorithm used to find the values of parameters (coefficients) of a function that minimizes a cost function.\n","\n","A cost function is a measure of how wrong the model is in terms of its ability to estimate the relationship between X and y. This is typically expressed as a difference or distance between the predicted value and the actual value."]},{"cell_type":"markdown","metadata":{"id":"pUL8e-6JIwEg"},"source":["### Batch Gradient Descent\n","\n","Batch gradient descent is a variation of the gradient descent algorithm that calculates the error for each example in the training dataset, but only updates the model after all training examples have been evaluated. \n","\n","Firstly, we calculate the cost function which is $E$ = $(y - y_i)^2$ and we tweak the slope m and bias c to reduce the cost function.\n","\n","$\\frac{\\partial E_i }{\\partial m}$ = $ \\frac{-2}{n} \\sum_{i=1}^{n} (y_i - (mx_i + c)) * x_i$\n","\n","$\\frac{\\partial E_i }{\\partial c}$ = $ \\frac{-2}{n} \\sum_{i=1}^{n}(y_i - (mx_i + c))$\n","\n","And then we update the slope and bias with with change in slope $\\Delta m$ and change in bias $\\Delta c$ with learning rate $eta$\n","\n","$m$  = $m - \\Delta m * eta$\n","\n","$c$  = $c - \\Delta c * eta$\n","\n"]},{"cell_type":"code","metadata":{"id":"8m6BE6v-IwEg"},"source":["\"\"\"\n","The function 'train' updates the values of m and c and calculates error. \n","The loss is minimized due to the changed values of m and c.\n","The new values m, c and the minimized error is returned.\n","\"\"\"\n","def train(x, y, m, c, eta):\n","    const = - 2.0/len(y)\n","    ycalc = m * x + c\n","    delta_m = const * sum(x * (y - ycalc))\n","    delta_c = const * sum(y - ycalc)\n","    m = m - delta_m * eta\n","    c = c - delta_c * eta\n","    error = sum((y - ycalc)**2)/len(y)  \n","    return m, c, error"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MHjYX45BIwEj"},"source":["### Effect of varying LR (learning rate, $\\eta$) on error and final line\n","\n","Let us vary LR and find how the error decreases in each case, and how the final line looks, by training each case for the same number of iterations - 2000."]},{"cell_type":"markdown","metadata":{"id":"XydFY0FzIwEk"},"source":["#### $\\eta$ (Learning rate) = 0.1"]},{"cell_type":"code","metadata":{"id":"qjQJXM-4IwEl"},"source":["# Save errors\n","errs_1 = []\n","# Initialize m,c value\n","m, c = 0, 0 \n","eta = 0.1 # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.1.\n","for iteration in range(2000):\n","    m, c, error = train(l, tsq, m, c, eta)\n","    errs_1.append(error)\n","\n","# Save final line\n","m_1, c_1 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tB40sJhTIwEp"},"source":["#### $\\eta$ = 0.01"]},{"cell_type":"code","metadata":{"id":"cyLOvjaLIwEq"},"source":["# Save errors\n","errs_01 = []\n","m, c = 0, 0\n","eta = 0.01 # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.01.\n","for iteration in range(2000):\n","    m, c, error = train(l, tsq, m, c, eta)\n","    errs_01.append(error)\n","\n","# Save final line\n","m_01, c_01 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E22O2IakIwEt"},"source":["#### $\\eta$ = 0.001"]},{"cell_type":"code","metadata":{"id":"Cvl81aceIwEt"},"source":["# Save errors\n","errs_001 = []\n","m, c = 0, 0\n","eta = 0.001 # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.001.\n","for iteration in range(2000):\n","    m, c, error = train(l, tsq, m, c, eta) \n","    errs_001.append(error)\n","\n","# Save final line\n","m_001, c_001 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SIdlxdYIIwEv"},"source":["#### $\\eta$ = 0.0001"]},{"cell_type":"code","metadata":{"id":"Y_e1NK5qIwEw"},"source":["# Save errors\n","errs_0001 = []\n","m, c = 0, 0\n","eta = 0.0001 # Learning rate\n","\n","# Call the train() method for 2000 iterations to update m and c and get error value with eta = 0.0001. \n","for iteration in range(2000):\n","    m, c, error = train(l, tsq, m, c, eta)\n","    errs_0001.append(error)\n","\n","# Save final line\n","m_0001, c_0001 = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QNZgQMNZIwE0"},"source":["### Plot of lines vs $\\eta$ (Learning rate)"]},{"cell_type":"code","metadata":{"id":"XWFue0e6IwE1"},"source":["# Find the lines\n","y_1 = m_1 * l + c_1\n","y_01 = m_01 * l + c_01\n","y_001 = m_001 * l + c_001\n","y_0001 = m_0001 * l + c_0001"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LyCs_U80IwE4"},"source":["plt.figure(figsize=(15, 8))\n","plt.plot(l, tsq, '.k')\n","plt.plot(l, y_1, \"g\")\n","plt.plot(l, y_01, \"r\")\n","plt.plot(l, y_001, \"b\")\n","plt.plot(l, y_0001, \"y\")\n","plt.legend([\"l vs tsq\",\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oWp-qhiZIwE6"},"source":["Thus, we see that higher learning rates reach the best fit faster than lower learning rates (obviously)."]},{"cell_type":"markdown","metadata":{"id":"GNyPltb7IwE7"},"source":["### Plot of errors vs epochs for each $\\eta$ (Learning rate)"]},{"cell_type":"code","metadata":{"id":"ds8LI8XBIwE7"},"source":["epochs = range(0,2000)\n","plt.figure(figsize=(16,10))\n","plt.plot(epochs, errs_1, \"g\")\n","plt.plot(epochs, errs_01,\"r\")\n","plt.plot(epochs, errs_001,\"b\")\n","plt.plot(epochs, errs_0001,\"y\")\n","plt.legend([\"eta = 0.1\",\"eta = 0.01\",\"eta = 0.001\",\"eta = 0.0001\"])\n","plt.xlabel('Epochs') \n","plt.ylabel('Errors') \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MqL3Qd4dIwFA"},"source":["### With LR (learning rate, $\\eta$) Decay\n","\n","In some cases, the learning rate might be too high to give good fitting lines. For example, let us train with constant LR (learning rate, $\\eta$) of 0.8 and get the final line after 1000 iterations:"]},{"cell_type":"markdown","metadata":{"id":"eoW3VvR8IwFB"},"source":["#### $\\eta$ = 0.8"]},{"cell_type":"code","metadata":{"id":"A_Z-use6IwFC"},"source":["errs = []\n","m, c = 0, 0\n","eta = 0.8\n","\n","# Call the train() method for 1000 iterations to update m and c and get error value with constant eta = 0.8.\n","for times in range(1000):\n","    m, c, error = train(l, tsq, m, c, eta)\n","    errs.append(error)\n","    \n","m_normal, c_normal = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TjmDAryWIwFG"},"source":["Let us see the plot of error vs iterations:"]},{"cell_type":"code","metadata":{"id":"s0N0o3NKIwFH"},"source":["plt.plot(range(len(errs)), errs)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Error\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yN5_kJn0IwFJ"},"source":["We see that the error quickly goes to almost 0, but after some iterations it blows up.\n","\n","Let us check the \"best fit\" line that is found:"]},{"cell_type":"code","metadata":{"id":"DXumozZ-IwFL"},"source":["print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_normal, c_normal, errs[-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5zB7RTTOIwFM"},"source":["y = m_normal * l + c_normal \n","plt.plot(l, tsq, '.k', label = 'Actual')\n","plt.plot(l, y, \"r\", label = 'Prediction')\n","plt.xlabel(\"Length (L)\")\n","plt.ylabel(\"T^2\")\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fqmvtu-_IwFQ"},"source":["Clearly this is not ideal.\n","\n","This was a simple case where we can see the learning rate is too high. There might be cases where it is not so simple to identify this. Also, having a low learning rate is not good because training time would be too high!\n","\n","**Solution: Decay the learning rate.**\n","\n","Now let us train another model with decaying lr. But let us not decay lr below 0.0001."]},{"cell_type":"code","metadata":{"id":"edZDbSyBIwFR"},"source":["errs_decay = []\n","m, c = 0, 0\n","eta = 0.5\n","decay_factor = 0.99\n","\n","# Call the train() method for 1000 iterations to update m and c and get error value with decaying eta.\n","for iteration in range(1000):\n","    eta = max(0.0001, eta * decay_factor)\n","    m, c, error = train(l, tsq, m, c, eta)\n","    errs_decay.append(error)\n","\n","m_decay, c_decay = m, c"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xbEAsf4cIwFS"},"source":["print(\"m = {0:.6} c = {1:.6} Error = {2:.6}\".format(m_decay, c_decay, errs_decay[-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"clqa9StFVKqP"},"source":["Let us see the plot of error vs iterations:"]},{"cell_type":"code","metadata":{"id":"WNUxY6ZyIwFV"},"source":["plt.plot(range(len(errs_decay)), errs_decay)\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Error\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRoeB71EIwFW"},"source":["y = m_decay * l + c_decay \n","plt.plot(l, tsq, '.k')\n","plt.plot(l, y, \"r\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djZn9yLqIwFa"},"source":["Thus, this is correct."]}]}