{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "colab": {
      "name": "M3_SNB_MiniProject_4_Emotion_Classification_from_Speech.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "associate-sunset"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Speech Emotion Classification"
      ],
      "id": "associate-sunset"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "handled-tooth"
      },
      "source": [
        "## Problem Statement"
      ],
      "id": "handled-tooth"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accessory-watts"
      },
      "source": [
        "Build a model to recognize emotion from speech using Ensemble learning "
      ],
      "id": "accessory-watts"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twenty-indonesia"
      },
      "source": [
        "## Learning Objectives"
      ],
      "id": "twenty-indonesia"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZGAsPzQAU-X"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* extract the features from audio data\n",
        "* implement ML classification algorithms individually and as Ensembles, to classify emotions\n",
        "* record the voice sample and test it with trained model"
      ],
      "id": "1ZGAsPzQAU-X"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lesbian-bottom"
      },
      "source": [
        "## Dataset"
      ],
      "id": "lesbian-bottom"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fixed-trainer"
      },
      "source": [
        "**TESS Dataset**\n",
        "\n",
        "The first dataset chosen for this mini-project is the [TESS](https://dataverse.scholarsportal.info/dataset.xhtml?persistentId=doi:10.5683/SP2/E8H2MF) (Toronto emotional speech set) dataset. It contains 2880 files.  A set of 200 target words were spoken in the carrier phrase \"Say the word _____' by two actresses and the sets were recorded in seven different emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). Both actresses spoke English as their first language, were university educated, and had musical training. Audiometric testing indicated that both actresses had thresholds within the normal range."
      ],
      "id": "fixed-trainer"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Roo5A2aLVI07"
      },
      "source": [
        "**Ravdess Dataset**\n",
        "\n",
        "The second dataset chosen for this mini-project is [Ravdess](https://zenodo.org/record/1188976#.YLczy4XivIU) (The Ryerson Audio-Visual Database of Emotional Speech and Song). This dataset contains 1440 files: 60 trials per actor x 24 actors = 1440. The RAVDESS contains 24 professional actors (12 female, 12 male), vocalizing two lexically-matched statements in a neutral North American accent. Speech emotions includes calm, happy, sad, angry, fearful, surprise, and disgust expressions. Each expression is produced at two levels of emotional intensity (normal, strong), with an additional neutral expression.\n",
        "\n",
        "**File naming convention**\n",
        "\n",
        "Each of the 1440 files has a unique filename. The filename consists of a 7-part numerical identifier (e.g., 03-01-06-01-02-01-12.wav). These identifiers define the stimulus characteristics:\n",
        "\n",
        "**Filename identifiers**\n",
        "\n",
        "* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n",
        "* Vocal channel (01 = speech, 02 = song).\n",
        "* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n",
        "* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n",
        "* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n",
        "* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n",
        "* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n",
        "\n",
        "Filename example: `03-01-06-01-02-01-12.wav`\n",
        "\n",
        "    - Audio-only - 03\n",
        "    - Speech - 01\n",
        "    - Fearful - 06\n",
        "    - Normal intensity - 01\n",
        "    - Statement \"dogs\" - 02\n",
        "    - 1st Repetition - 01\n",
        "    - 12th Actor - 12 Female, as the actor ID number is even."
      ],
      "id": "Roo5A2aLVI07"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIR6a6TvVnRY"
      },
      "source": [
        "## Information"
      ],
      "id": "GIR6a6TvVnRY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mediterranean-february"
      },
      "source": [
        "**Speech Emotion Recognition (SER)** is the task of recognizing the emotion from  speech, irrespective of the semantics. Humans can efficiently perform this task as a natural part of speech communication, however, the ability to conduct it automatically using programmable devices is a field of active research.\n",
        "\n",
        "Studies of automatic emotion recognition systems aim to create efficient, real-time methods of detecting the emotions of mobile phone users, call center operators and customers, car drivers, pilots, and many other human-machine communication users. Adding emotions to machines forms an important aspect of making machines appear and act in a human-like manner\n",
        "\n",
        "Lets gain familiarity with some of the audio based features that are commonly used for SER. \n",
        "\n",
        "**Mel scale** — The mel scale (derived from the word *melody*) is a perceptual scale of pitches judged by listeners to be equal in distance from one another. The reference point between this scale and normal frequency measurement is defined by assigning a perceptual pitch of 1000 mels to a 1000 Hz tone, 40 dB above the listener's threshold. Above about 500 Hz, increasingly large intervals are judged by listeners to produce equal pitch increments. Refer [here](https://towardsdatascience.com/learning-from-audio-the-mel-scale-mel-spectrograms-and-mel-frequency-cepstral-coefficients-f5752b6324a8) for more detailed information.\n",
        "\n",
        "**Pitch** — how high or low a sound is. It depends on frequency, higher pitch is high frequency\n",
        "\n",
        "**Frequency** — speed of vibration of sound, measures wave cycles per second\n",
        "\n",
        "**Chroma** — Representation for audio where spectrum is projected onto 12 bins representing the 12 distinct semitones (or chroma). Computed by summing the log frequency magnitude spectrum across octaves.\n",
        "\n",
        "**Fourier Transforms** — used to convert from time domain to frequency domain. Time domain shows how signal changes over time. Frequency domain shows how much of the signal lies within each given frequency band over a range of frequencies"
      ],
      "id": "mediterranean-february"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5a6Dz9wCxOc"
      },
      "source": [
        "**Librosa**\n",
        "\n",
        "[Librosa](https://librosa.org/doc/latest/index.html) is a Python package, built for speech and audio analytics. It provides modular functions that simplify working with audio data and help in achieving a wide range of applications such as identification of the personal characteristics of different individuals' voice samples, detecting emotions from audio samples etc. \n",
        "\n",
        "For further details on the Librosa package, refer [here](https://conference.scipy.org/proceedings/scipy2015/pdfs/brian_mcfee.pdf).\n"
      ],
      "id": "Q5a6Dz9wCxOc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "operating-latter"
      },
      "source": [
        "## Grading = 10 Points"
      ],
      "id": "operating-latter"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caring-syndrome"
      },
      "source": [
        "### Download the dataset"
      ],
      "id": "caring-syndrome"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "talented-upset"
      },
      "source": [
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/M4_MP4_Ravdess_Tess.zip\n",
        "!unzip -qq M4_MP4_Ravdess_Tess.zip\n",
        "# Install packages\n",
        "!pip -qq install librosa soundfile\n",
        "!pip -qq install wavio"
      ],
      "id": "talented-upset",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appreciated-pattern"
      },
      "source": [
        "### Import Neccesary Packages"
      ],
      "id": "appreciated-pattern"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loose-marsh"
      },
      "source": [
        "import librosa\n",
        "import librosa.display\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import IPython.display as ipd\n",
        "from matplotlib import pyplot as plt\n",
        "from datetime import datetime\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "# sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn import tree\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "id": "loose-marsh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vxa85gvorY5"
      },
      "source": [
        "### Work-Flow\n",
        "\n",
        "* Load the TESS audio data and extract features and labels\n",
        "\n",
        "* Load the Ravdess audio data and extract features\n",
        "\n",
        "* Combine both the audio dataset features\n",
        "\n",
        "* Train and test the model with TESS + Ravdess Data\n",
        "\n",
        "* Record the team audio samples and add them to TESS + Ravdess data\n",
        "\n",
        "* Train and test the model with TESS + Ravdess + Team Recorded (combined) data\n",
        "\n",
        "* Test each of the models with live audio sample recording."
      ],
      "id": "2vxa85gvorY5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UtviWIXAtb4"
      },
      "source": [
        "### Load the Tess data and Ravdess data audio files (1 point)\n",
        "\n",
        "Hint: `glob.glob`"
      ],
      "id": "_UtviWIXAtb4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "518e945a"
      },
      "source": [
        "wav_files = glob.glob(\"Tess/*/*.wav\")\n",
        "len(wav_files)"
      ],
      "id": "518e945a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sound-chest"
      },
      "source": [
        "#### Play the sample audio"
      ],
      "id": "sound-chest"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "personalized-wildlife"
      },
      "source": [
        "ipd.Audio('Tess/YAF_fear/YAF_cool_fear.wav')"
      ],
      "id": "personalized-wildlife",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exposed-county"
      },
      "source": [
        "### Data Exploration and Visualization (1 points)"
      ],
      "id": "exposed-county"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hungry-cleaner"
      },
      "source": [
        "#### Visualize the distribution of all the labels"
      ],
      "id": "hungry-cleaner"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "political-jerusalem"
      },
      "source": [
        "emotions_ = []\n",
        "for file in wav_files:\n",
        "    emotions_.append(file.split(\"_\")[-1][:-4])\n",
        "set(emotions_)"
      ],
      "id": "political-jerusalem",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orange-taiwan"
      },
      "source": [
        "from collections import Counter\n",
        "freq_emotions = Counter(emotions_)\n",
        "plt.bar(freq_emotions.keys(),freq_emotions.values())"
      ],
      "id": "orange-taiwan",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "established-airfare"
      },
      "source": [
        "#### Visualize sample audio signal using librosa"
      ],
      "id": "established-airfare"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outstanding-caribbean"
      },
      "source": [
        "sample_audio_path = 'Tess/YAF_fear/YAF_cool_fear.wav'\n",
        " \n",
        "# librosa is used for analyzing and extracting features of an audio signal\n",
        "data, sampling_rate = librosa.load(sample_audio_path)\n",
        "plt.figure(figsize=(15, 5))\n",
        " \n",
        "# librosa.display.waveplot is used to plot waveform of amplitude vs time\n",
        "librosa.display.waveplot(data, sr=sampling_rate)\n",
        "plt.show()"
      ],
      "id": "outstanding-caribbean",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NJsKV_7cgXUv"
      },
      "source": [
        "sampling_rate"
      ],
      "id": "NJsKV_7cgXUv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "medical-confidence"
      },
      "source": [
        "### Feature extraction (2 points)\n",
        "\n",
        "Read one WAV file at a time using `Librosa`. An audio time series in the form of a 1-dimensional array for mono or 2-dimensional array for stereo, along with time sampling rate (which defines the length of the array), where the elements within each of the arrays represent the amplitude of the sound waves is returned by `librosa.load()` function. Refer to the supplementary notebook ('Audio feature extraction')\n",
        "\n",
        "To know more about Librosa, explore the [link](https://librosa.org/doc/latest/feature.html)"
      ],
      "id": "medical-confidence"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "superb-scotland"
      },
      "source": [
        "def extract_feature(file_name):\n",
        "    \"\"\"Function Extracts Features from WAV file\"\"\"\n",
        "    X, sample_rate = librosa.load(file_name)\n",
        "    stft=np.abs(librosa.stft(X))\n",
        "    result=np.array([])\n",
        "    mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n",
        "    result=np.hstack((result, mfccs))\n",
        "    chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "    result=np.hstack((result, chroma))\n",
        "    mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "    result=np.hstack((result, mel))\n",
        "    return result"
      ],
      "id": "superb-scotland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piano-accent"
      },
      "source": [
        "sample_feature = extract_feature(wav_files[0])#,mfcc=True, chroma=True, mel=True)\n",
        "sample_feature.shape"
      ],
      "id": "piano-accent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d3bd640"
      },
      "source": [
        "#### Create a dictionary or a function to encode the emotions"
      ],
      "id": "7d3bd640"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0c8d44d"
      },
      "source": [
        "emotions = {'angry':5, 'disgust':7, 'fear':6, 'happy':3, 'neutral':1, 'surprised':8, 'sad':4}"
      ],
      "id": "c0c8d44d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coupled-villa"
      },
      "source": [
        "#### TESS data feature extraction\n",
        "\n",
        "**Note:** Features are already saved in DataFrame and given in csv file, Ignore the below commented code "
      ],
      "id": "coupled-villa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quarterly-adrian"
      },
      "source": [
        "# Load the data and extract features for each sound file\n",
        "# tess_x, tess_y = [], []\n",
        "# for file_name in wav_files:\n",
        "#     emotion = emotions[file_name.split(\"_\")[-1][:-4]]\n",
        "#     feature = extract_feature(file_name)\n",
        "#     tess_x.append(feature)\n",
        "#     tess_y.append(emotion)\n",
        " \n",
        "# len(tess_x), len(tess_y)"
      ],
      "id": "quarterly-adrian",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XJMGdYXEQmE"
      },
      "source": [
        "# set(tess_y)"
      ],
      "id": "3XJMGdYXEQmE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKzclsG-FnW6"
      },
      "source": [
        "#### Ravdess data feature extraction"
      ],
      "id": "qKzclsG-FnW6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WS8Hk_e9GN69"
      },
      "source": [
        "rav_wav_files = glob.glob(\"ravdess/*/*.wav\")\n",
        "len(rav_wav_files), rav_wav_files[0]"
      ],
      "id": "WS8Hk_e9GN69",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C20Km6geFq77"
      },
      "source": [
        "# rav_x, rav_y = [], []\n",
        "# for file_name in rav_wav_files:\n",
        "#     emotion = emotions[file_name.split(\"_\")[-1][:-4]]\n",
        "#     feature = extract_feature(file_name)\n",
        "#     rav_x.append(feature)\n",
        "#     rav_y.append(emotion)\n",
        "# len(rav_x), len(rav_y)"
      ],
      "id": "C20Km6geFq77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itI8bT3jHcrk"
      },
      "source": [
        "# set(rav_y)"
      ],
      "id": "itI8bT3jHcrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDwNOgKEIH3w"
      },
      "source": [
        "# features = tess_x + rav_x\n",
        "# labels = tess_y + rav_y"
      ],
      "id": "iDwNOgKEIH3w",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLkbEo2zY2bO"
      },
      "source": [
        "# Download extracted features csv\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/MiniProjects/tess_ravdess_features.csv"
      ],
      "id": "kLkbEo2zY2bO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eoULlEdZYGiP"
      },
      "source": [
        "extracted = pd.read_csv(\"tess_ravdess_features.csv\")\n",
        "features = extracted.iloc[:,:-1]\n",
        "labels = extracted.iloc[:,-1]\n",
        "features.shape, labels.shape"
      ],
      "id": "eoULlEdZYGiP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bb62f16"
      },
      "source": [
        "#### Save the features\n",
        "\n",
        "It is best advised to save the features in dataframe and maintain so that feature extraction step is not required to be performed every time.\n",
        "\n",
        "* Make a DataFrame with features and labels\n",
        "\n",
        "* Write dataframe into `.CSV` file and save it offline."
      ],
      "id": "2bb62f16"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ec91c16"
      },
      "source": [
        "# saved_data = pd.DataFrame(features)\n",
        "# saved_data['label'] = labels\n",
        "# saved_data.to_csv(\"tess_ravdess_features.csv\",index=False)"
      ],
      "id": "9ec91c16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aggressive-cause"
      },
      "source": [
        "#### Split the data into train and test"
      ],
      "id": "aggressive-cause"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nearby-angle"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(np.array(features), np.array(labels), test_size=0.2, random_state=42)\n",
        "X_train.shape, X_test.shape, len(y_train)"
      ],
      "id": "nearby-angle",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ordered-weapon"
      },
      "source": [
        "### Train the model with TESS + Ravdess data (2 points)\n",
        "\n",
        "* Apply different ML algorithms (eg. DecisionTree, RandomForest, etc.) and find the model with best performance"
      ],
      "id": "ordered-weapon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y7efGFBIfqj"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression(random_state=42)\n",
        "lr.fit(X_train, y_train)\n",
        "lr.score(X_test, y_test), lr.score(X_train, y_train)"
      ],
      "id": "8y7efGFBIfqj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "extraordinary-fault"
      },
      "source": [
        "# Decision Tree\n",
        "from sklearn import tree\n",
        " \n",
        "dt_model = tree.DecisionTreeClassifier(random_state=42)\n",
        "dt_model = dt_model.fit(X_train, y_train)\n",
        "dt_model.score(X_test, y_test), dt_model.score(X_train, y_train)"
      ],
      "id": "extraordinary-fault",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "civic-palestine"
      },
      "source": [
        "# Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        " \n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_model.score(X_test, y_test), rf_model.score(X_train, y_train)"
      ],
      "id": "civic-palestine",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzCe1APDUpg0"
      },
      "source": [
        "#Linear SVC\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "Lsvm = LinearSVC(random_state=0, tol=1e-5)\n",
        "Lsvm.fit(X_train, y_train)\n",
        "Lsvm.score(X_test, y_test)"
      ],
      "id": "gzCe1APDUpg0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtlfZmtWSD_X"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        " \n",
        "sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "\n",
        "sgd.fit(X_train, y_train)\n",
        "sgd.score(X_test, y_test)"
      ],
      "id": "EtlfZmtWSD_X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "returning-bones"
      },
      "source": [
        "#### Apply the voting classifier"
      ],
      "id": "returning-bones"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuzzy-respondent"
      },
      "source": [
        "# Voting classifer\n",
        "dt = tree.DecisionTreeClassifier(random_state=42)\n",
        "lr = LogisticRegression(random_state=42)\n",
        "sgd = SGDClassifier(max_iter=1000, tol=1e-3)\n",
        "Lsvm = LinearSVC(random_state=0, tol=1e-5)\n",
        "voting = VotingClassifier(estimators=[('lr',lr), ('dt', dt), ('sgd', sgd),('Lsvm',Lsvm)], voting='hard')\n",
        "voting = voting.fit(X_train, y_train)\n",
        "voting.score(X_test, y_test)"
      ],
      "id": "fuzzy-respondent",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDfmEIYoV068"
      },
      "source": [
        "!pip -qq install xgboost"
      ],
      "id": "pDfmEIYoV068",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPYySueQV3pS"
      },
      "source": [
        "# xgb classifier\n",
        "from xgboost import XGBClassifier\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(X_train, y_train)\n",
        "xgb.score(X_test, y_test)"
      ],
      "id": "kPYySueQV3pS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "forced-spider"
      },
      "source": [
        "#### Testing with a sample"
      ],
      "id": "forced-spider"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "first-negotiation"
      },
      "source": [
        "# audio sample \n",
        "test_sample = 'Tess/YAF_fear/YAF_cool_fear.wav'\n",
        "ipd.Audio(test_sample)"
      ],
      "id": "first-negotiation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "surprising-liability"
      },
      "source": [
        "# prediction\n",
        "pred = rf_model.predict(extract_feature(test_sample).reshape(1,-1))\n",
        "list(emotions.keys())[list(emotions.values()).index(int(pred[0]))]"
      ],
      "id": "surprising-liability",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p1MHeY9oYqB"
      },
      "source": [
        "### Train the model with TESS + Ravdess + Team recorded data (2 points)\n",
        "\n",
        "* Record the audio samples, extract features and combine with TESS data features\n",
        "  - Record and gather all the team data with proper naming convention in separate folder\n",
        "\n",
        "    **Hint:** Follow the supplementary notebook\n",
        "\n",
        "  - Each team member must record 2 samples for each emotion (Use similar sentences as given in TESS data)\n",
        "\n",
        "* Train the different ML algorithms and find the model with best performance"
      ],
      "id": "6p1MHeY9oYqB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4In88rvtVmb"
      },
      "source": [
        "#### Load the team data\n",
        "\n",
        "**Note:** Upload your team data by running the below cell and proceed with next cells\n",
        "\n",
        "<font color=\"red\">If the team data is not uploaded in the following cell, subsequent cells may result in errors.</font>"
      ],
      "id": "M4In88rvtVmb"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cRkLBLYFoao"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "id": "3cRkLBLYFoao",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnOTXagNtJcu"
      },
      "source": [
        "!unzip -qq # Path here to unzip"
      ],
      "id": "JnOTXagNtJcu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxdN3240GuOq"
      },
      "source": [
        "**Change the path below**"
      ],
      "id": "MxdN3240GuOq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asTb3gyXYlRS"
      },
      "source": [
        "TeamData = glob.glob(\"/content/path_to_folder/*.wav\")\n",
        "len(TeamData)"
      ],
      "id": "asTb3gyXYlRS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9XfHW21BJOs"
      },
      "source": [
        "#### Extracting features of team data and combine with TESS + Ravdess"
      ],
      "id": "U9XfHW21BJOs"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nna7EZMRaKCI"
      },
      "source": [
        "# extracting team recorded data features\n",
        "team_features = []\n",
        "team_label = []\n",
        "for i in TeamData:\n",
        "  team_features.append(extract_feature(i))\n",
        "  emt = (i.split(\"_\")[-1][:-4]).strip()\n",
        "  team_label.append(emotions[emt])\n",
        "\n",
        "len(team_features),len(team_label)"
      ],
      "id": "nna7EZMRaKCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15br8qPwa2a6"
      },
      "source": [
        "# combining team data with Tess + ravdess data\n",
        "all_x = list(features.values) + team_features\n",
        "all_y = list(labels) + team_label\n",
        "len(all_x), len(all_y)"
      ],
      "id": "15br8qPwa2a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et7CLgszb01U"
      },
      "source": [
        "set(all_y)"
      ],
      "id": "et7CLgszb01U",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-ulIdIdtkDB"
      },
      "source": [
        "#### Train the different ML algorithms"
      ],
      "id": "w-ulIdIdtkDB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Fn0GlBL7D4"
      },
      "source": [
        "# Split the data into train and test\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(np.array(all_x), np.array(all_y), test_size=0.2, random_state=42)\n",
        "xtrain.shape, xtest.shape, len(ytrain)"
      ],
      "id": "x_Fn0GlBL7D4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7iBT_mrL_Ab"
      },
      "source": [
        "# Decision Tree\n",
        "dt_model_team = tree.DecisionTreeClassifier(random_state=42)\n",
        "dt_model_team = dt_model_team.fit(xtrain, ytrain)\n",
        "dt_model_team.score(xtest, ytest), dt_model_team.score(xtrain, ytrain)"
      ],
      "id": "G7iBT_mrL_Ab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cHo-pAWlMA_-"
      },
      "source": [
        "# Random Forest\n",
        "rf_model_team = RandomForestClassifier(random_state=42)\n",
        "rf_model_team.fit(xtrain, ytrain)\n",
        "rf_model_team.score(xtest, ytest), rf_model_team.score(xtrain, ytrain)"
      ],
      "id": "cHo-pAWlMA_-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yL44OfmMuEEq"
      },
      "source": [
        "#Linear SVC\n",
        "Lsvm = LinearSVC(random_state=0, tol=1e-5)\n",
        "Lsvm.fit(xtrain, ytrain)\n",
        "Lsvm.score(xtest, ytest)"
      ],
      "id": "yL44OfmMuEEq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mukMDkAzpskU"
      },
      "source": [
        "# Voting classifer\n",
        "dt = tree.DecisionTreeClassifier(random_state=42)\n",
        "lr = LogisticRegression(random_state=42)\n",
        "Lsvm = LinearSVC(random_state=0, tol=1e-5)\n",
        "voting_team = VotingClassifier(estimators=[('lr',lr),('dt', dt),('Lsvm',Lsvm)], voting='hard')\n",
        "voting_team = voting_team.fit(xtrain, ytrain)\n",
        "voting_team.score(xtest, ytest)"
      ],
      "id": "mukMDkAzpskU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCKpAnuDZBhY"
      },
      "source": [
        "# xgb classifier\n",
        "xgb = XGBClassifier()\n",
        "xgb.fit(xtrain, ytrain)\n",
        "xgb.score(xtest, ytest)"
      ],
      "id": "lCKpAnuDZBhY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMl0sRhXtXzL"
      },
      "source": [
        "### Test the best working model with live audio recording"
      ],
      "id": "iMl0sRhXtXzL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4znf9-pcTbUB"
      },
      "source": [
        "MODEL = rf_model"
      ],
      "id": "4znf9-pcTbUB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d598770",
        "cellView": "form"
      },
      "source": [
        "#@title Speak the utterance and test\n",
        "from IPython.display import Javascript\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        " \n",
        "RECORD = \"\"\"\n",
        "const sleep  = time => new Promise(resolve => setTimeout(resolve, time))\n",
        "const b2text = blob => new Promise(resolve => {\n",
        "  const reader = new FileReader()\n",
        "  reader.onloadend = e => resolve(e.srcElement.result)\n",
        "  reader.readAsDataURL(blob)\n",
        "})\n",
        "var record = time => new Promise(async resolve => {\n",
        "  stream = await navigator.mediaDevices.getUserMedia({ audio: true })\n",
        "  recorder = new MediaRecorder(stream)\n",
        "  chunks = []\n",
        "  recorder.ondataavailable = e => chunks.push(e.data)\n",
        "  recorder.start()\n",
        "  await sleep(time)\n",
        "  recorder.onstop = async ()=>{\n",
        "    blob = new Blob(chunks)\n",
        "    text = await b2text(blob)\n",
        "    resolve(text)\n",
        "  }\n",
        "  recorder.stop()\n",
        "})\n",
        "\"\"\"\n",
        "\n",
        "if not os.path.exists('ModelTesting/'):\n",
        "    os.mkdir(\"ModelTesting/\")\n",
        "def record(sec=3):\n",
        "    print(\"Start speaking!\")\n",
        "    now = datetime.now()\n",
        "    current_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    display(Javascript(RECORD))\n",
        "    s = output.eval_js('record(%d)' % (sec*1000))\n",
        "    b = b64decode(s.split(',')[1])\n",
        "    with open('ModelTesting/audio_'+current_time+'.wav','wb') as f:\n",
        "        f.write(b)\n",
        "    return 'ModelTesting/audio_'+current_time+'.wav'\n",
        "test_i = record()\n",
        "pred = MODEL.predict(extract_feature(test_i).reshape(1,-1))\n",
        "idx_emotion = list(emotions.values()).index(pred[0])\n",
        "print(list(emotions.keys())[idx_emotion])\n",
        "ipd.Audio(test_i)"
      ],
      "id": "2d598770",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7n58VaeWKGu"
      },
      "source": [
        "### Report Analysis\n",
        "\n",
        "- Report the accuracy for 10 live samples using the model trained on TESS+Ravdess+Team data\n",
        "- Discuss with the team mentor regarding deep learnt audio features (which will be introduced in Module 5 for another audio classification task). Read a related article [here](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8805181).\n"
      ],
      "id": "K7n58VaeWKGu"
    }
  ]
}