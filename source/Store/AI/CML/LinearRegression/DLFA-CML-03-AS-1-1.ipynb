{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2_AST_11_Naive_Bayes_Classifier_&_Linear_Regression_C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACyro4_xpFwr"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 11: Naive Bayes Classifier & Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWEKPQW2pWCM"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "op3MF_vLpXBE"
      },
      "source": [
        "At the end of the experiment, you will be able to:\n",
        "\n",
        "* understand the 20newsgroup dataset\n",
        "* preprocess the text data\n",
        "* understand the representation of text document using Bag of \n",
        "Words\n",
        "* classify the Bag of Words represented text data with the Naive Bayes Classifier\n",
        "* understand how to approach a Machine Learning problem\n",
        "* understand how to decide which algorithm to use\n",
        "* understand why we use Linear Regression \n",
        "* implement Linear Regresion using Normal Equation as well as scikit learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLNoSC3SrfAP"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fzskman3ZuWC"
      },
      "source": [
        "\n",
        "In this experiment we use the 20 newsgroup dataset\n",
        "\n",
        "### Description\n",
        "\n",
        "\n",
        "This dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups, that is there are approximately one thousand documents taken from each of the following newsgroups. Below are the 20 newsgroups categories:\n",
        "\n",
        "    alt.athesim\n",
        "    comp.graphics   \n",
        "    comp.os.ms-windows.misc\n",
        "    comp.sys.ibm.pc.hardware\n",
        "    comp.sys.mac.hardware\n",
        "    comp.windows.x\n",
        "    misc.forsale\n",
        "    rec.autos\n",
        "    rec.motorcycles\n",
        "    rec.sport.baseball\n",
        "    rec.sport.hockey\n",
        "    sci.crypt\n",
        "    sci.electronics\n",
        "    sci.med\n",
        "    sci.space\n",
        "    soc.religion.christian\n",
        "    talk.politics.guns\n",
        "    talk.politics.mideast\n",
        "    talk.politics.misc\n",
        "    talk.religion.misc\n",
        "\n",
        "The dataset consists **Usenet** posts--essentially an email sent by subscribers to that newsgroup. They typically contain quotes from previous posts as well as cross posts i.e. a few posts may be sent to more than once in a newsgroup.\n",
        "\n",
        "Each newsgroup is stored in a subdirectory, with each post stored as a separate file.\n",
        "\n",
        "Data source to this experiment : http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vW1Vu2adZ0oO"
      },
      "source": [
        "## Domain Information\n",
        "A newsgroup, despite the name, has nothing to do with news. It is what we would call today a mailing list or a discussion forum. Usenet is a distributed discussion system designed and developed in 1979 and deployed in 1980.  \n",
        "\n",
        "Members joined newsgroups of their interest and made *posts*. Posts are very similar to email -- in later years, newsgroups became mailing lists and people posted via email."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bo3iVdSkZ5gb"
      },
      "source": [
        "Here the objecive is solving the \"Text classification\" problem. This is a broadly defined task which is common to many services and products. For example, gmail classifies an incoming mail into different sections such as Updates, Forums etc..\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0muqcxoZ6fL"
      },
      "source": [
        "### Bag of Words (BoW)\n",
        "\n",
        "* The Bag of Words is a collection of words to represent a document with word count and mostly disregarding the order in which they appear. Here, we look at the histogram of the words within the text, i.e. considering each word count as a feature.\n",
        "\n",
        "* The bag-of-words is a simple way to understand the representation of documents and words. It makes use of the one-hot vector representation (which returns a sparse matrix), where it is used to distinguish each word in a vocabulary from every other word in the vocabulary. The vector consists of 0s in all cells with the exception of a single 1 in a cell used uniquely to identify the word and the document is represented as a sum of one-hot vectors of all the words in the document.\n",
        "\n",
        "* It is a way of representing a text document. The frequency of occurrence of each word in the defined vocabulary is represented in a vector form (1 Dimensional array). These vectors (features) can be used for training machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OiFi8nj77AW"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exG368oL8jv2",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M2_AST_11_Naive_Bayes_Classifier_&_Linear_Regression_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/insurance.csv\")\n",
        "    ipython.magic(\"sx wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/DL_DS_NEWSGROUPS_PICKELFILE.pkl\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUi2DsSsqwFO"
      },
      "source": [
        "### Importing required Packages\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9sC9--9wbUq"
      },
      "source": [
        "import pickle\n",
        "import re\n",
        "import operator\n",
        "import collections\n",
        "from collections import defaultdict\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import pandas  as pd \n",
        "import seaborn as sns\n",
        "\n",
        "# Sklearn Libraries\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBTL2XgRq87a"
      },
      "source": [
        "### Loading the dataset\n",
        "\n",
        "To know more details about the pickle refer to the following [link](https://colab.research.google.com/drive/1O1KGielUbjcRROG9YFHXgxb6iYNUT7R_?usp=sharing)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JEhKu_rjwbUt"
      },
      "source": [
        "# Loading the dataset\n",
        "dataset = pickle.load(open('DL_DS_NEWSGROUPS_PICKELFILE.pkl','rb'))\n",
        "print(dataset.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2k1kcF_HCy0"
      },
      "source": [
        "To get a sense of our data, let us first start by counting the frequencies of the target (categories) classes in our news articles in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZF9S3GYSwbUx"
      },
      "source": [
        "# Print frequencies of dataset\n",
        "print(\"Class : count\")\n",
        "print(\"--------------\")\n",
        "count = []\n",
        "number_of_documents = 0\n",
        "for key in dataset:\n",
        "    print(key, ':', len(dataset[key]))\n",
        "    count.append(len(dataset[key]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eShLUnKXzYny"
      },
      "source": [
        "### Visualization of the Newsgroup dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OI7Yw072Z7UW"
      },
      "source": [
        "fig = plt.figure(figsize = (40, 8))\n",
        " \n",
        "# Creating the bar plot of class vs frequencies\n",
        "plt.bar(list(dataset.keys()), count);\n",
        "plt.xlabel('Newsgroups')\n",
        "plt.ylabel('No of articles in the Newsgroups');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XO0UwQDw7wOT"
      },
      "source": [
        "# From the dataset select the class and the article number to be displayed\n",
        "dataset['sci.med'][4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJDnkxkmwbU1"
      },
      "source": [
        "Next, let us split our dataset which consists of approx.1000 samples per class, into training and test sets. We use 950 samples from each class in the training set, and the remaining 50 in the test set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6GK60NEwbU2"
      },
      "source": [
        "train_set = {}\n",
        "test_set = {}\n",
        "    \n",
        "# Break dataset into 95-5 split for training and testing\n",
        "n_train = 0\n",
        "n_test = 0\n",
        "\n",
        "split_ratio = 0.95\n",
        "\n",
        "for k in dataset:\n",
        "    split = int(split_ratio*len(dataset[k]))\n",
        "    train_set[k] = dataset[k][0:split]\n",
        "    test_set[k] = dataset[k][split:]\n",
        "    n_train += len(train_set[k])\n",
        "    n_test += len(test_set[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afd4qfAEwbU6"
      },
      "source": [
        "## 1. Bag-of-Words\n",
        "\n",
        "Let us begin our journey into text classification with one of the simplest but most commonly used feature representations for text documents - Bag-of-Words.\n",
        "\n",
        "As you might have realized, machine learning algorithms need good feature representations of different inputs.  Concretely, we would like to represent each news article $D$ in terms of a feature vector $V$, which can be used for classification. Feature vector $V$ is made up of the number of occurences of each word in the vocabulary.\n",
        "\n",
        "Let us begin by counting the number of occurences of every word in the news documents in the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OACynazXwbU7"
      },
      "source": [
        "### 1.1 Word frequency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWgzZVqYwbU8"
      },
      "source": [
        "Now let us try to count the frequencies of words and store it in a dictionary. By doing this we have the vocabulary built for the corpus (in this case 20 news groups)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kyqVjvBWLdK"
      },
      "source": [
        "def frequency_words(train_set): \n",
        "  # Initialize a dictionary to store frequencies of words.\n",
        "  # Key:Value === Word:Count \n",
        "  frequency = defaultdict(int)\n",
        " \n",
        "  # Loop through each newsgroup in the dataset\n",
        "  for key in train_set:\n",
        "      # Loop through each article in the dataset\n",
        "      for f in train_set[key]:\n",
        "\n",
        "          # For each article find all words which consist only of capital and lowercase characters and are between length of 2-9.\n",
        "          # We ignore all special characters such as !.$ and words containing numbers\n",
        "          words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n",
        "\n",
        "          for word in words:\n",
        "              frequency[word] += 1\n",
        "  return frequency"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4LhQstIEC8K"
      },
      "source": [
        "Let us try to understand the kind of words that appear frequently, and those that occur rarely. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdILkwjHwbU9"
      },
      "source": [
        "# Call the frequency_words() function by passing the training set\n",
        "frequency_of_words = frequency_words(train_set)\n",
        "\n",
        "# 'frequency_of_words.items()' returns the list with all dictionary keys with values (words and it's frequencies)\n",
        "# operator.itemgetter(1) method can be used to sort a dictionary by value (frequencies)\n",
        "# 'reverse=True' is to sort the frequencies in descending order\n",
        "sorted_words = sorted(frequency_of_words.items(), key=operator.itemgetter(1), reverse=True)\n",
        "\n",
        "print(\"Top-10 most frequent words:\")\n",
        "for word in sorted_words[:10]:\n",
        "    print(word)\n",
        "\n",
        "print('----------------------------')\n",
        "print(\"10 least frequent words:\")\n",
        "for word in sorted_words[-10:]:\n",
        "    print(word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxZqnO8GwbVB"
      },
      "source": [
        "Next, we attempt to plot a histogram of the frequencies of various words in descending order. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyWfgN4qwbVC"
      },
      "source": [
        "fig = plt.figure()\n",
        "fig.set_size_inches(20,10)\n",
        "\n",
        "# From the sorted words we try to plot the histogram for the first 100 words\n",
        "plt.bar(range(len(sorted_words[:100])), [v for k, v in sorted_words[:100]] , align='center')\n",
        "plt.xticks(range(len(sorted_words[:100])), [k for k, v in sorted_words[:100]])\n",
        "locs, labels = plt.xticks()\n",
        "plt.setp(labels, rotation=90)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DH0x2aJQCW4Q"
      },
      "source": [
        "Please observe the above bar chart of word vs its frequency. Generally the stop words. are the words that appear on most of the documents and the words that are too rare come in this list. These words don't add much value in the classification task. So, we remove them to decrease the vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvcU4VDkwbVE"
      },
      "source": [
        "### 1.2 Pre-processing to remove most frequent words\n",
        "\n",
        "We can see that different words appear with different frequencies.\n",
        "\n",
        "The most common words appear in almost all documents. Hence, for a classification task, having information about those word's frequencies does not matter much since they appear frequently in every type of document. To get a good feature representation, we eliminate them since they do not add too much value.\n",
        "\n",
        "Additionally, notice how the least frequent words appear so rarely that they might not be useful either.\n",
        "\n",
        "Let us pre-process our news articles now to remove the most frequent and least frequent words by thresholding their counts: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TrBX6IOSaofH"
      },
      "source": [
        "def cleaning_vocabulary_words(list_of_grams):\n",
        "  valid_words = defaultdict(int)\n",
        "\n",
        "  print('Number of words before preprocessing:', len(list_of_grams))\n",
        "\n",
        "  # Ignore the 25 most frequent words, and the words which appear less than 100 times\n",
        "  ignore_most_frequent = 25\n",
        "  freq_thresh = 100\n",
        "  feature_number = 0\n",
        "  for word, word_frequency in sorted_words[ignore_most_frequent:]:\n",
        "    if word_frequency > freq_thresh:\n",
        "        valid_words[word] = feature_number\n",
        "        feature_number += 1\n",
        "\n",
        "  print('Number of words after preprocessing:', len(valid_words))\n",
        "\n",
        "  vector_size = len(valid_words)\n",
        "\n",
        "  return valid_words, vector_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztxnNsH8bVQV"
      },
      "source": [
        "valid_words, number_of_words = cleaning_vocabulary_words(sorted_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL7KDD5nwbVG"
      },
      "source": [
        "### 1.3 Bag-of-Words representation\n",
        "\n",
        "The simplest way to represent a document $D$ as a vector $V$ would be to now count the relevant words in the document. \n",
        "\n",
        "For each document, make a vector of the count of each of the words in the vocabulary (excluding the words removed in the previous step - the \"stopwords\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmPFrRWfwbVH"
      },
      "source": [
        "def convert_to_BoW(dataset, number_of_documents):\n",
        "    bow_representation = np.zeros((number_of_documents, number_of_words))\n",
        "    labels = np.zeros((number_of_documents, 1))\n",
        "    \n",
        "    i = 0\n",
        "    for label, class_name in enumerate(dataset):\n",
        "        \n",
        "        # For each file\n",
        "        for f in dataset[class_name]:\n",
        "            \n",
        "            # Read all text in file\n",
        "            text = ' '.join(f).split(' ')\n",
        "            \n",
        "            # For each word in the article\n",
        "            for word in text:\n",
        "                # To check whether the words is present in the valid words\n",
        "                if word in valid_words:\n",
        "                    # For the given article if the word is present in the valid words we create a BOW representation\n",
        "                    bow_representation[i, valid_words[word]] += 1\n",
        "            \n",
        "            # Label of the document\n",
        "            labels[i] = label\n",
        "            \n",
        "            # Increment document counter\n",
        "            i += 1\n",
        "    \n",
        "    return bow_representation, labels\n",
        "\n",
        "# Convert the dataset into their bag of words representation by calling the 'convert_to_BoW()' function treating train and test separately\n",
        "train_bow_set, train_bow_labels = convert_to_BoW(train_set, n_train)\n",
        "test_bow_set, test_bow_labels = convert_to_BoW(test_set, n_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2L9gwfXwbVI"
      },
      "source": [
        "### 1.4 Document classification using Naive Bayes\n",
        "\n",
        "\n",
        "#### What is scikit learn?\n",
        "\n",
        "Scikit learn is a library used to perform machine learning in Python. Scikit learn is an open source machine learning library which is free software licensed and is reusable in various contexts, encouraging academic and commercial use. \n",
        "\n",
        "Scikit-learn  provides a range of supervised and unsupervised learning algorithms in Python. \n",
        "\n",
        "It provides efficient tools for data analysis, data pre-processing, model building, model evaluation, and much more. \n",
        "\n",
        "\n",
        "#### What are the standard methods present in classifier implementation?\n",
        "\n",
        "\n",
        "* First we start with importing the necessary classifier and creating an instance for it.\n",
        "\n",
        "* By using **fit()** we fit the classifier to the training data, which  is essentially the training part of the modeling process\n",
        "\n",
        "* By using **predict()** for a classifier, we can classify the data (from a test set) using the predict method.\n",
        "\n",
        "* By using **score()** returns the mean accuracy on the given test data and labels.\n",
        "\n",
        "* By using **predict_proba** returns probability estimates for the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnMaJ63b__0c"
      },
      "source": [
        "####  What is Naive bayes classifier? \n",
        "\n",
        "A Naive Bayes classifier is a probabilistic machine learning model that’s used for classification task. It's a classifier in machine learning model, where every feature (words) is independent of each other, so we look at individual words in a sentence, instead of the entire sentence in order to predict the category of a given text data. As Naive Bayes is a probabilistic classifier, therefore will calculate the probability of each category using Bayes theorem, and the category with the highest probability will be the output. \n",
        "\n",
        "Since 'Naive Bayes' is based on the Bayes Theorem, which helps us compute the conditional probabilities of occurrence of two events based on the probabilities of occurrence of each individual event, encoding those probabilities is extremely useful.\n",
        "\n",
        "\n",
        "\n",
        "#### Why MultinomialNB?\n",
        "\n",
        "MultinomialNB classifier is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features used by the classifier are the frequency i.e. the number of times a given word appears in a document. The Probability of each word per class is calculated.\n",
        "\n",
        "\n",
        "\n",
        "Refer to the following [link](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html) for the MultinomailNB classifier from scikit learn\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Kc0z4UXAkIk"
      },
      "source": [
        "#### What is alpha in scikit learn implementation of MultinomailNB? \n",
        "\n",
        "\n",
        "**alpha** represents the Laplacian Smoothing parameter.\n",
        "\n",
        "\n",
        "If we choose a value of alpha $\\neq 0$ (not equal to 0), the probability will no longer be zero even if a word is not present in the training dataset.\n",
        "\n",
        "#### Advantages of Laplacian Smoothing in Naive Bayes\n",
        "\n",
        "The frequency-based probability might introduce zeros when multiplying the probabilities, leading to a failure in preserving the information contributed by the non-zero probabilities. Therefore, a smoothing approach, for example, the Laplace smoothing, must be adopted to counter this problem.\n",
        "\n",
        "Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability in the Naïve Bayes machine learning algorithm.\n",
        "\n",
        "For Eg: By setting the smoothing parameter alpha (α) = 1, we add 1 to every probability, therefore the probability for each category, will never be zero.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SjrOxyI5DTp"
      },
      "source": [
        "# Create an instance for the MultinomialNB classifier\n",
        "clf = MultinomialNB(alpha=0.1)\n",
        "\n",
        "# Fit the model with the train data\n",
        "clf.fit(train_bow_set, train_bow_labels)\n",
        "\n",
        "# Get the prediction on the test set\n",
        "predicted = clf.predict(test_bow_set)\n",
        "\n",
        "# Calculate the accuracy score\n",
        "accuracy_score(test_bow_labels, predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nE7bNSYirv59"
      },
      "source": [
        "### 1.5 Model Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTR3t-7227kG"
      },
      "source": [
        "\n",
        "#### Classification Report : \n",
        "\n",
        "A Classification report is used to measure the quality of predictions from a classification algorithm. More specifically, True Positives, False Positives, True negatives and False Negatives are used to predict the metrics of a classification report as shown below.\n",
        "\n",
        " * **true positive** The correct label of the given instance is positive, and the classifier also predicts it as a positive\n",
        " * **false positive** The correct label is negative, but the classifier incorrectly predicts it as positive\n",
        " * **true negative** The correct label is negative, and the classifier also predicts a negative\n",
        " * **false negative** The correct label is positive, but the classifier incorrectly predicts it as negative\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC0Q5QYl_yl-"
      },
      "source": [
        "* **Precision:** The precision is calculated as the ratio between the number of Positive samples correctly classified to the total number of samples classified as Positive (either correctly or incorrectly)\n",
        "\n",
        "    Precision = $\\mathbf{\\frac{True \\ Positive}{True \\ Positive + False \\ Positive}}$\n",
        "\n",
        "* **Recall:** Recall tells us how many true positives (points labelled as positive) were recalled or found by our model.\n",
        "\n",
        "   Recall = $\\mathbf{\\frac{True \\ Positive}{True \\ Positive + False \\ Negative}}$\n",
        "\n",
        "* **F1-score:** precision and recall can be combined into a single score that seeks to balance both concerns, called the F-score or the F-measure.\n",
        "  \n",
        "   F1-score = $ 2* \\mathbf{\\frac{Precision * Recall}{Precision + Recall}}$\n",
        "\n",
        "* **Accuracy:** it is the ratio of the number of correct predictions to the total number of input samples.\n",
        "\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/precision_and_recall.jpg\" alt=\"Drawing\" height=\"400\" width=\"360\"/>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZhJlO0Tyr55"
      },
      "source": [
        "For more details on precision and Recall refer to the following [link](https://medium.com/@klintcho/explaining-precision-and-recall-c770eb9c69e9)\n",
        "\n",
        "For example of precision and Recall refer to the following [link](https://towardsdatascience.com/confusion-matrix-clearly-explained-fee63614dc7)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MsJWrXwoRk1b"
      },
      "source": [
        "# Print the classificatin report\n",
        "print(classification_report(test_bow_labels, predicted, target_names=dataset.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tFjlQvKrzSQ"
      },
      "source": [
        "#### Confusion Matrix:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MV233WTv2EXp"
      },
      "source": [
        "\n",
        "* **Confusion matrix:**  is a table that is used to describe the performance of a classification model on a set of test data for which the true values are known. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4wG_W0TqNEt"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "mat = confusion_matrix(test_bow_labels, predicted)\n",
        "\n",
        "plt.figure(figsize = (16,8))\n",
        "\n",
        "# Visualizing the confusion matrix as a heatmap\n",
        "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=True,\n",
        "            xticklabels=dataset.keys(),\n",
        "            yticklabels=dataset.keys())\n",
        "plt.xlabel('true label')\n",
        "plt.ylabel('predicted label');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhxQALxuV5T4"
      },
      "source": [
        "A confusion matrix gives us an idea of how good is our model. It describes the performance of a classification model on a set of test data for which the\n",
        "true values are known. Each row in a confusion matrix represents an actual class, while each column represents a predicted class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQplpZmIAZh2"
      },
      "source": [
        "From the above confusion matrix we can interpret that the diagonal elements represent the total correct values predicted per class. The lighter the color, the greater the number as we can see from the color-bar on the side. So we would assume that — lighter colours on the diagonal elements and darker on all others mean our model is performing well and vice-versa.\n",
        "\n",
        "For Eg. 44 documents have been correctly predicted as belonging to class 2 (rec.autos) out of 58 documents and the remaining are incorrectly classified\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNLoNRyoGhJT"
      },
      "source": [
        "### Plotting alpha values vs F1-score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uHQHzdCrlqo"
      },
      "source": [
        "# Define the parameter values that should be searched for Multinomial Naive Bayes classifier\n",
        "alphas = [0.001, 0.01, 0.1, 1, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "\n",
        "f1_observed = []\n",
        "for i in alphas:\n",
        "    clf = MultinomialNB(alpha=i, fit_prior=False)\n",
        "    clf.fit(train_bow_set, train_bow_labels)\n",
        "    predicted = clf.predict(test_bow_set)\n",
        "    f1_observed.append(f1_score(test_bow_labels, predicted, average='macro'))\n",
        "\n",
        "plt.plot(alphas, f1_observed)\n",
        "plt.xlabel('MultinomialNB (alpha values)')\n",
        "plt.ylabel('F1-score')\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLuuPgm3MrHE"
      },
      "source": [
        "## Problem Statement: Medical Insurance Expense Prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO-UuAKlHwLO"
      },
      "source": [
        "### Introduction\n",
        "What is the importance of medical insurance cost prediction?\n",
        "\n",
        "If we built such a system that can help to give an estimate of insurance cost based on some conditions of the patient such as age, gender, bmi, etc, it would help insurance companies as well as individuals to know how much they have to spend on their health insurance.\n",
        "\n",
        "Here, the dataset provided to you contains attributes of a patient such as age, sex, bmi, number of children, region, smoking habit along with the corresponding medical insurance charges incured to them. \n",
        "\n",
        "You are expected to use this dataset and build a prediction model to help estimate the cost incured to the future patient given all the relevant details.\n",
        "\n",
        "Following are the details of each feature/attribute of the given dataset.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwZojrqsttQl"
      },
      "source": [
        "### Loading the Insurance dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iDKH5qF0WQd"
      },
      "source": [
        "insurance_data = pd.read_csv('insurance.csv')\n",
        "print('\\nNumber of insurance beneficiaries: {} ; features per beneficiary: {}  '.format(insurance_data.shape[0],insurance_data.shape[1]))\n",
        "print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaSSvTnRIKpm"
      },
      "source": [
        "### Summary of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaJ8QBwGINd7"
      },
      "source": [
        "insurance_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwUywNRFIouk"
      },
      "source": [
        "\n",
        "1. Each attribute has a full of `1338 non-null entries`, hence the dataset has no missing values. In case, if any null values(missing values) exist, we generally replace them with mean, median, or mode.\n",
        "\n",
        "2. The dataset has `4 real` and `3 categorical` attributes/features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vZeG3qxKi4P"
      },
      "source": [
        "real_cols = insurance_data.select_dtypes(exclude=['object'])\n",
        "cat_cols = insurance_data.select_dtypes(include=['object'])\n",
        "print(\"The numeric columns are: \\n{} \\n\\nThe non-numeric columns are: \\n{}\"\n",
        "      .format(list(real_cols.columns),list(cat_cols.columns)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkXXaTKhxdZQ"
      },
      "source": [
        "When we look at the shape of dataset it returned $(1338,7)$. So there are  $m=1338$  training data and  $n=7$  features. Here, the target variable is charges and remaining six variables are age, sex, bmi, children, smoker, region are independent variable as shown earlier. There are multiple independent variable, so we need to fit Multiple linear regression. For this, the hypothesis function looks like\n",
        "\n",
        "$h_{θ}(x_{i})=θ_{0}+θ_{1}age+θ_{2}sex+θ_{3}bmi+θ_{4}children+θ_{5}smoker+θ_{6}region$\n",
        " \n",
        "This multiple linear regression equation for given dataset.\n",
        "\n",
        "If $i=1$ then\n",
        "\n",
        "$h_{θ}(x_{1})=θ_{0}+θ_{1}19+θ_{2}female+θ_{3}27.900+θ_{4}0+θ_{5}yes+θ_{6}southwest$\n",
        "\n",
        "$y_{1}=16884.92400$\n",
        "\n",
        "If $i=3$ then\n",
        "\n",
        "$h_{θ}(x_{3})=θ_{0}+θ_{1}28+θ_{2}male+θ_{3}33.000+θ_{4}3+θ_{5}no+θ_{6}northwest$\n",
        "\n",
        "$y_{3}=4449.46200$\n",
        "\n",
        "$x_{1}=(x_{11}\\hspace{0.1cm}x_{12}\\hspace{0.1cm}x_{13}\\hspace{0.1cm}x_{14}\\hspace{0.1cm}x_{15}\\hspace{0.1cm}x_{16})=(19 \\hspace{0.5cm} female \\hspace{0.5cm} 27.9001 \\hspace{0.5cm} no \\hspace{0.5cm} northwest)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoJLIqOqyUnC"
      },
      "source": [
        "### Matrix Formulation\n",
        "\n",
        "In general we can write above vector as\n",
        "\n",
        "$x_{ij}=(x_{i1}x_{i2}...x_{in})$\n",
        " \n",
        "Now, we combine all available individual vector into single input matrix of size  $(m,n)$  and denoted by  $X$  input matrix, which consist of all training examples,\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "    x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
        "    x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
        "    \\vdots        & \\vdots   & \\vdots   & \\vdots  & \\vdots\\\\\n",
        "    x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
        "\\end{bmatrix}_{(m,n)}$\n",
        "\n",
        " $\\hat X = \\begin{bmatrix}\n",
        "   1 & x_{11}       & x_{12} & x_{13} & \\dots & x_{1n} \\\\\n",
        "   1 & x_{21}       & x_{22} & x_{23} & \\dots & x_{2n} \\\\\n",
        "    \\vdots        & \\vdots   & \\vdots   & \\vdots  & \\vdots\\\\\n",
        "   1 & x_{m1}       & x_{m2} & x_{m3} & \\dots & x_{mn}\n",
        "\\end{bmatrix}_{(m,n+1)}$\n",
        "\n",
        "We represent parameter of function and dependent variable in vector form as\n",
        "\n",
        "$\\theta = \\begin{bmatrix}\n",
        "    \\theta_{0}\\\\\n",
        "    \\theta_{1}\\\\\n",
        "    \\vdots    \\\\\n",
        "    \\theta_{j}  \\\\\n",
        "    \\vdots     \\\\\n",
        "     \\theta_{n}  \\\\\n",
        "\\end{bmatrix}_{(n+1,1)}$  $Y = \\begin{bmatrix}\n",
        "    y_{1}\\\\\n",
        "    y_{2}\\\\\n",
        "    \\vdots    \\\\\n",
        "    y_{i}  \\\\\n",
        "    \\vdots     \\\\\n",
        "    y_{m}  \\\\\n",
        "\\end{bmatrix}_{(m,1)}$  \n",
        "\n",
        " \n",
        "Here, we represent hypothesis function in vectorized form\n",
        "\n",
        "$h_{θ}(X)=X_{θ} =  X \\theta$\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8Xk1Ut3C-p_"
      },
      "source": [
        "### Visualization for Charges vs BMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzDgGLSmyd9q"
      },
      "source": [
        "\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \n",
        "and charges as dependent variable\"\"\"\n",
        "\n",
        "sns.lmplot(x='bmi',y='charges',data=insurance_data,aspect=2,height=6)\n",
        "plt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\n",
        "plt.ylabel('Insurance Charges: as Dependent variable')\n",
        "plt.title('Charges Vs BMI');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i68qMVlFypZ3"
      },
      "source": [
        "In above plot we fit regression line into the variables.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAZd4O3syrp8"
      },
      "source": [
        "### Cost Function\n",
        "\n",
        "A cost function measures how much error is there in the model by checking in terms of ability to estimate the relationship between  x  and  y . We can measure the accuracy of our hypothesis function by using a cost function. This takes an average difference of observed dependent variable in the given the dataset and those predicted by the hypothesis function.\n",
        "\n",
        "$J(θ) = \\frac{1}{m}\\sum_{i=1}^{m} (\\hat{y}_{i}−y_{i})^2$\n",
        "\n",
        "$J(θ) = \\frac{1}{m}\\sum_{i=1}^{m} (h_{\\theta}({x}_{i})−y_{i})^2$\n",
        "  \n",
        "To implement the linear regression, you should take training example add an extra column that is  $x_{0}$  feature, where  $x_{0}$=1 .  \n",
        "\n",
        "$xo=(x_{i_{0}}x_{i_{1}}x_{i_{2}}...x_{m_{i}})$ ,\n",
        "\n",
        "where  $x_{i_{0}}=0$  and input matrix will become as\n",
        "\n",
        "$X = \\begin{bmatrix}\n",
        "    x_{10}       & x_{11} & x_{12} & \\dots & x_{1n} \\\\\n",
        "    x_{20}       & x_{21} & x_{22} & \\dots & x_{2n} \\\\\n",
        "    \\vdots        & \\vdots   & \\vdots   & \\vdots  & \\vdots\\\\\n",
        "    x_{m0}       & x_{m1} & x_{m2} & \\dots & x_{mn}\n",
        "\\end{bmatrix}_{(m,n+1)}$ \n",
        "\n",
        "Each of the m input samples is similar to a column vector with n+1 rows,  $x_{0}$  being 1 for our convenience, that is  $x_{10},x_{20},x_{30}...x_{m0}=1$ . Now, we rewrite the ordinary least square cost function in matrix form as:\n",
        "\n",
        "$J(θ)=\\frac{1}{m}(X_{θ}−Y)^T(X_{θ}−Y)$\n",
        " \n",
        "Let's look at the matrix multiplication concept. The multiplication of two matrix happens only if number of column of first matrix is equal to number of rows of second matrix. Here, input matrix  X  of size  $(m,n+1)$ , parameter of function is of size  $(n+1,1)$  and dependent variable vector of size  $(m,1)$ . The product of matrix  $X(m,n+1)θ(n+1,1)$  will return a vector of size  (m,1) , then product of $(X_{θ}−Y)^T_{(1,m)}(X_{θ}−Y)_{(m,1)}$  will return size of unit vector.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma2GKT-kFSAX"
      },
      "source": [
        "### Normal Equation\n",
        "\n",
        "The normal equation is an analytical solution to the linear regression problem with a ordinary least square cost function. To minimize our cost function, take partial derivative of  $J(θ)$  with respect to  $θ$  and equate to  $0$. The derivative of function is nothing but checking that for a small change in input what would be the change in output of function.\n",
        "\n",
        "$min_{θ_{0},θ_{1}...θ_{n}}J(θ_{0},θ_{1}..θ_{n})$\n",
        " \n",
        "$\\frac{∂J(θ_{j})}{∂ θ_{j}}=0$\n",
        " \n",
        "where  j=0,1,2,....n \n",
        "\n",
        "Now we will apply partial derivative to our cost function,\n",
        "\n",
        "$\\frac{∂J(θ_{j})}{∂ θ_{j}}=\\frac{\\partial}{\\partial \\theta} (X_{\\theta}-Y)^T(X_{\\theta}-Y)$\n",
        "\n",
        "\n",
        " \n",
        "We will remove 1/m  since we are going to equate the derivative to 0 and solve $J(θ)$. \n",
        "\n",
        "$J(θ)=(X_{θ}−Y)^T(X_{θ}−Y)$\n",
        " \n",
        "=$(X_{θ}^T−Y^T)(X_{θ}−Y)$\n",
        " \n",
        "\n",
        "=$(Xθ)^T−Y^T)(Xθ−Y)$\n",
        " \n",
        "=$(Xθ)^T Xθ−Y^TXθ−(Xθ)^TY+Y^TY$\n",
        " \n",
        "=$θ^T X^TXθ−2(Xθ)^T Y+Y^TY$\n",
        "\n",
        "Here $\\theta$ is unknown. To find where the above function has a minimum, we will derive by $\\theta$ and equate it to 0. Also, we only use matrix notation to conveniently represent a system of linear formula. So, we derive by each component of the vector, and then combine the resulting derivatives into a vector again. \n",
        " \n",
        "$\\frac{∂J(θ)}{∂θ}=\\frac{∂}{∂θ}(θ^T X^TXθ−2θ^T X^TY+Y^TY)$\n",
        " \n",
        "$0= 2 X^TXθ−2  X^Ty$\n",
        " \n",
        "$ X^TXθ = X^TY$\n",
        " \n",
        "$θ=(X^TX)^{(−1)} X^TY$\n",
        " \n",
        "This is the normal equation for linear regression."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COv1N5HSzCAs"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLXE0fwKKw_i"
      },
      "source": [
        "Let us look into the description of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUKresQMKsEv"
      },
      "source": [
        "insurance_data.describe(include=\"all\").T "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7Lo0wOnUcYv"
      },
      "source": [
        "1. Compare means and standard devaiations for all the real columns to establish that the distributions are different. \n",
        "\n",
        "2. What is the problem if different continuous variables have different scales and magnitude?\n",
        "  \n",
        "  If there are different ranges for different continuous variables, then, the dataset will have some exceptional values very high or very low among the common values present in the dataset.   \n",
        "\n",
        "3. What is the possible solution for handling different ranges and why?\n",
        "\n",
        "  Normalizing the data is the solution. By Normalizing the data, the values can be stuffed in a specific range. Moreover, you will understand about Normalization in the later section. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9iNiVHBzUDD"
      },
      "source": [
        "#### Check for missing values\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4Ups59-zWNA"
      },
      "source": [
        "plt.figure(figsize=(12,4))\n",
        "sns.heatmap(insurance_data.isnull(),cbar=False,cmap='viridis',yticklabels=False)\n",
        "plt.title('Missing value in the dataset');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWmxNzpS0Prr"
      },
      "source": [
        "There is no missing values in the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m89QcxvL0Qgn"
      },
      "source": [
        "#### Visualization of Correlation Matrix\n",
        "\n",
        "From the below matrix, we observe that the highly correlated features has the correlation value close to 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yf1hFJII0ST8"
      },
      "source": [
        "# correlation plot\n",
        "corr = insurance_data.corr()\n",
        "sns.heatmap(corr, cmap = 'Wistia', annot= True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9lRyI9OyIsv"
      },
      "source": [
        "#### Visualization of Distribution of Insurance Charges"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5YMNISK0bXN"
      },
      "source": [
        "f = plt.figure(figsize=(12,4))\n",
        "\n",
        "ax = f.add_subplot(121)\n",
        "sns.distplot(insurance_data['charges'],bins=50,color='r',ax=ax)\n",
        "ax.set_title('Distribution of insurance charges')\n",
        "\n",
        "ax = f.add_subplot(122)\n",
        "sns.distplot(np.log10(insurance_data['charges']),bins=40,color='b',ax=ax)\n",
        "ax.set_title('Distribution of insurance charges in $log$ sacle')\n",
        "ax.set_xscale('log');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blZCjUkT0oe2"
      },
      "source": [
        "If we look at the left plot the charges varies from 1120 to 63500, the plot is right skewed. In right plot we will apply natural log, then plot approximately tends to normal. For further analysis we will apply log on target variable charges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaDXky2UDhbg"
      },
      "source": [
        "#### Visualization for Charges vs Sex and Charges vs Smoker"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8Mm316k0pOU"
      },
      "source": [
        "f = plt.figure(figsize=(14,6))\n",
        "ax = f.add_subplot(121)\n",
        "sns.violinplot(x='sex', y='charges',data=insurance_data,palette='Wistia',ax=ax)\n",
        "ax.set_title('Violin plot of Charges vs sex')\n",
        "\n",
        "ax = f.add_subplot(122)\n",
        "sns.violinplot(x='smoker', y='charges',data=insurance_data,palette='magma',ax=ax)\n",
        "ax.set_title('Violin plot of Charges vs smoker');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uv2AF_t0yzL"
      },
      "source": [
        "From left plot the insurance charge for male and female is approximatley in same range, it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-ehI6fFDvOI"
      },
      "source": [
        "#### Visualization for Charges vs Children"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLrdchWR01Gs"
      },
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "sns.boxplot(x='children', y='charges',hue='sex',data=insurance_data,palette='rainbow')\n",
        "plt.title('Box plot of charges vs children');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5rJb_8y06ys"
      },
      "source": [
        "insurance_data.groupby('children').agg(['mean','min','max'])['charges']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-R15zOc-xIcB"
      },
      "source": [
        "#### Visualization for Charges vs Region"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2IpncI01ABU"
      },
      "source": [
        "plt.figure(figsize=(14,6))\n",
        "sns.violinplot(x='region', y='charges',hue='sex',data=insurance_data,palette='rainbow',split=True)\n",
        "plt.title('Violin plot of region vs children');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVNalvCUH_4y"
      },
      "source": [
        "#### Visualization for Charges vs Age and Charges vs BMI"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBPl6e1E1E8P"
      },
      "source": [
        "f = plt.figure(figsize=(14,6))\n",
        "ax = f.add_subplot(121)\n",
        "sns.scatterplot(x='age',y='charges',data=insurance_data,palette='magma',hue='smoker',ax=ax)\n",
        "ax.set_title('Scatter plot of Charges vs age')\n",
        "\n",
        "ax = f.add_subplot(122)\n",
        "sns.scatterplot(x='bmi',y='charges',data=insurance_data,palette='viridis',hue='smoker')\n",
        "ax.set_title('Scatter plot of Charges vs bmi')\n",
        "plt.savefig('sc.png');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snSf0svK1Mnq"
      },
      "source": [
        "From left plot the minimum age of a person who is insured is 18 year. \n",
        "\n",
        "Body mass index (BMI) is a measure of body fat based on height and weight of a person. Here, the minimum bmi is 16 $kg/m^2$  and maximum upto 54 $kg/m^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga7VMaL-1Szz"
      },
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "#### Encoding the categorical data\n",
        "\n",
        "Machine learning algorithms cannot work with categorical data directly. So,categorical data must be converted to number. We do the conversion in three ways:\n",
        "\n",
        "1. Label Encoding\n",
        "\n",
        "2. One hot encoding\n",
        "\n",
        "3. Dummy variable trap\n",
        "\n",
        "`Label encoding` refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them.\n",
        "\n",
        "A `One hot encoding` is a representation of categorical variable as binary vectors. It allows the representation of categorical data to be more expressive. This first requires that the categorical values should be mapped to integer values, that is label encoding is performed. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with 1.\n",
        "\n",
        "The Dummy variable trap is a scenario in which the independent variable are multicollinear, a scenario in which two or more variables are highly correlated. In simple terms, one variable can be predicted from the others.\n",
        "\n",
        "By using pandas `get_dummies` function we can do all above three step in line of code. We will use this fuction to get dummy variable for sex, children,smoker, and region. By setting drop_first =True, function will remove dummy variable trap by droping original variable. To know more about pandas get_dummies, click [here](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5e-zmm5106s"
      },
      "source": [
        "# Dummy variable\n",
        "categorical_columns = ['sex','children', 'smoker', 'region']\n",
        "insurance_data_encode = pd.get_dummies(data = insurance_data, prefix = 'OHE', prefix_sep='_',\n",
        "               columns = categorical_columns,\n",
        "               drop_first =True,\n",
        "              dtype='int8')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYwnIw3E1-5Z"
      },
      "source": [
        "# Lets verify the dummy variable process\n",
        "print('Columns in original data frame:\\n',insurance_data.columns.values)\n",
        "print('\\nNumber of rows and columns in the dataset:',insurance_data.shape)\n",
        "print('\\nColumns in data frame after encoding dummy variable:\\n',insurance_data_encode.columns.values)\n",
        "print('\\nNumber of rows and columns in the dataset:',insurance_data_encode.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDux0Jdtxshq"
      },
      "source": [
        "### Normalizing the data\n",
        "Earlier we have observed that the ranges of different continuous variables are different. This is actually problematic. Therefore, we perform normalization for continuous variable.\n",
        "  \n",
        "The goal of normalization is to change the values of numeric columns in the dataset to a common scale without distorting differences in the ranges of values. We normalize the data to bring all the variables to the same range.\n",
        "For this we can use `MinMaxScaler`. It scales and translates each feature individually such that it is in the given range on the training set, e.g. between `[0,1]` or else in the range `[-1, 1]` if there are negative values in the dataset. To learn more about MinMaxScaler click [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#:~:text=Transform%20features%20by%20scaling%20each,e.g.%20between%20zero%20and%20one.).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h9AQhRwHw_g"
      },
      "source": [
        "# Normalization\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "insurance_data_encode[[\"charges\"]] = scaler.fit_transform(insurance_data_encode[[\"charges\"]])\n",
        "\n",
        "insurance_data_encode.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0Bco2t52Yet"
      },
      "source": [
        "# Checking the inverse transform to cross verify the values\n",
        "scaler.inverse_transform(insurance_data_encode[['charges']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IhEQdrt6gOq"
      },
      "source": [
        "### Splitting the dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E79d5JOx6hl4"
      },
      "source": [
        "X = insurance_data_encode.drop('charges',axis=1) # Independent feature\n",
        "y = insurance_data_encode['charges'] # Dependent features\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VwHiY0FhqTYF"
      },
      "source": [
        "# Print first five rows from the data\n",
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ_zBBvd6uJy"
      },
      "source": [
        "### Model building\n",
        "\n",
        "#### Implementing Linear Regression using Normal Equation\n",
        "\n",
        "In this step, we will build model using our linear regression equation  $ θ=(X^TX)^{−1}X^Ty$ . In first step we need to add a feature $x_{0}=1$  to our original data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBXAck7K60rN"
      },
      "source": [
        "# Step 1: add x0 = 1 to dataset\n",
        "X_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\n",
        "X_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n",
        "\n",
        "# Step2: Building the model\n",
        "theta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ravWNfsQ63UE"
      },
      "source": [
        "# The parameters for linear regression model\n",
        "parameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\n",
        "columns = ['intersect:x_0=1'] + list(X.columns.values)\n",
        "parameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YomYzpLtLng0"
      },
      "source": [
        "#### Implementing Linear Regression using Scikit learn\n",
        "\n",
        "1. Create an object of Linear Regression function\n",
        "  \n",
        "  *lr = LinearRegression()*\n",
        "\n",
        "2. Fit the training data(features, dependent variable) using the `fit` method from sklearn.linear_model\n",
        "\n",
        "  *fit(X_train, y_train)*\n",
        "\n",
        "3. `Predict` the results using the test data that serves as unknown features to the linear regression model.\n",
        "\n",
        "  *y_pred = lr.predict(X_test)*\n",
        "\n",
        "4. Find the `Root Mean Square Error` i.e. the difference between the predicted value and the test set value.\n",
        "\n",
        "  *rmse(y_test,y_pred)*\n",
        "\n",
        "5. Finally, find the `R2 Score` for y_test and y_pred that depicts the accuracy score of the model built.\n",
        "\n",
        "  *r2_score(y_test,y_pred)*\n",
        "\n",
        "If you want to learn more about sklearn Linear Regression, click \n",
        "[here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzQ25jpz67jV"
      },
      "source": [
        "# Scikit Learn module\n",
        "lin_reg = LinearRegression()\n",
        "lin_reg.fit(X_train,y_train) # Note: x_0 =1 is not required here as sklearn will take care of it.\n",
        "\n",
        "# Parameter\n",
        "sk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\n",
        "parameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\n",
        "parameter_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R49rds-x7Ceg"
      },
      "source": [
        "The parameter obtained from both the model are same. So, we succefully built our model using normal equation and verified using sklearn linear regression module. Let's move ahead with the next step of prediction and model evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHEu4HJF7ErO"
      },
      "source": [
        "### Model evaluation using Normal Equation\n",
        "\n",
        "\n",
        "We will predict value for target variable by using our model parameter for test dataset. Then, compare the predicted value with actual value in test set. We compute Mean Square Error using formula\n",
        "$J(θ)=\\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_{i}−y_{i})^2$\n",
        " \n",
        "$R^2$  is statistical measure of how close data are to the fitted regression line.  $R^2$  is always between 0 to 100%. 0% indicated that model explains none of the variability of the response data around it's mean. 100% indicated that model explains all the variablity of the response data around the mean.\n",
        "\n",
        "$R^2$ = 1−$\\frac{SSE}{SST}$\n",
        " \n",
        "SSE = Sum of Square Error\n",
        "\n",
        "SST = Sum of Square Total\n",
        "\n",
        "$SSE = \\sum_{i=1}^{m} (\\hat{y}_{i}−y_{i})^2$\n",
        "\n",
        "$SST = \\sum_{i=1}^{m} (y_{i}−\\bar{y}_{i})^2$\n",
        "\n",
        " \n",
        "Here  $\\hat{y}$  is predicted value and  $\\bar{y}$  is mean value of  y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnT_PgS-7NMn"
      },
      "source": [
        "# Normal equation\n",
        "y_pred_norm =  np.matmul(X_test_0,theta)\n",
        "\n",
        "# Evaluation: MSE\n",
        "J_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n",
        "\n",
        "# R_square \n",
        "sse = np.sum((y_pred_norm - y_test)**2)\n",
        "sst = np.sum((y_test - y_test.mean())**2)\n",
        "R_square = 1 - (sse/sst)\n",
        "print('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\n",
        "print('R square obtain for normal equation method is :',R_square)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKjJEinz02aM"
      },
      "source": [
        "### Model Evaluation using sklearn module\n",
        "\n",
        "If you want to know more about $R^2$ score, click \n",
        "[here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)\n",
        "\n",
        "If you want to know more about Mean Square Error, click \n",
        "[here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2Bf4DJV7RZP"
      },
      "source": [
        "# sklearn regression module\n",
        "y_pred_sk = lin_reg.predict(X_test)\n",
        "\n",
        "#Evaluation: MSE\n",
        "J_mse_sk = mean_squared_error(y_pred_sk, y_test)\n",
        "\n",
        "# R_square\n",
        "R_square_sk = lin_reg.score(X_test,y_test)\n",
        "print('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\n",
        "print('R square obtain for scikit learn library is :',R_square_sk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHEMCUi1MSk2"
      },
      "source": [
        "Hence, we have successfully built and evaluated our Linear Regression model with the equation as well as the sklearn library. In both cases, we get the Mean Square Error as 0.00895630468233185 and the R square as 0.7305284299807451.\n",
        "\n",
        "The low value for MSE denotes that our model has least error values. The R square value denotes the accuracy of the model. As the accuracy of the model increases, the R square value reaches close to 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlhryqWxbXMz",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. What is incorrect regarding the Naive Bayes?\n",
        "Answer1 = \"\" #@param [\"\",\"features are equally important\", \"features are statistically dependent of one another given the class value\",\"features are statistically independent of one another given the class value\",\"features can be nominal or numeric\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8R6F2KGyiwk",
        "cellView": "form"
      },
      "source": [
        "#@title Q.2. The values of x and their corresponding values of y, (x, y) are as follows: (0, 2), (1, 3), (2, 4), (3, 5), (4, 6). Find the least square regression line y = a x + b for the given values and estimate the value of y when x = 12.\n",
        "Answer2 = \"\" #@param [\"\",\"11.0\", \"13.0\",\"15.2\",\"14.0\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}