{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-CML-02-SNB-2-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"EkUKAYe7XU8r"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","### Not For Grading"]},{"cell_type":"markdown","metadata":{"id":"TSZDYb5ZXbCf"},"source":["## Learning Objective"]},{"cell_type":"markdown","metadata":{"id":"N_AZDSAFXc_v"},"source":["At the end of the experiment, you will be able to :\n","\n","* Apply Linear Regression on different datasets"]},{"cell_type":"markdown","metadata":{"id":"irJjJrpeXu93"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"e2zXO1srXwqS"},"source":["### Description"]},{"cell_type":"markdown","metadata":{"id":"11N105gaX0vI"},"source":["For this experiment we have chosen three datasets:\n","\n","1. Human height and weight dataset\n","\n","2. Human brain weight and head size dataset\n","\n","3. Boston Housing dataset\n","\n","**Human height and weight dataset**\n","\n","This is made up of 10000 records. It contains 2 columns / features. The first column represents height and the second column represents weight of the person.\n","\n","**Human brain weight and head size dataset**\n","\n","The dataset is made up of 237 records. It contains 2 columns / features. The first column represents head size (cms) and the second column represents brain weight(grams) of the person.\n","\n","**Boston Housing dataset**\n","\n","The dataset contains 506 rows and 14 columns. It consists of price of houses in various locations in Boston. Along with price, the dataset also provides information such as :\n","\n","* CRIM - per capita crime rate by town.\n","* ZN - proportion of residential land zoned for lots over 25,000 sq.ft.\n","* INDUS - proportion of non-retail business acres per town.\n","* CHAS- Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).\n","* NOX - nitrogen oxides concentration (parts per 10 million).\n","* RM - average number of rooms per dwelling.\n","* AGE - proportion of owner-occupied units built prior to 1940.\n","* DIS - weighted mean of distances to five Boston employment centres.\n","* RAD - index of accessibility to radial highways.\n","* TAX - full-value property-tax rate per $10,000$.\n","* PTRATIO - pupil-teacher ratio by town.\n","* B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town.\n","* LSTAT - Percentage of lower status of the population.\n","* MEDV - Median value of owner-occupied homes in $1000s\n","\n","\n","**File names :**\n","\n","* new_height_weight.csv\n","* HumanBrain_WeightandHead_size.csv"]},{"cell_type":"code","source":["!wget https://cdn.talentsprint.com/aiml/Experiment_related_data/HumanBrain_WeightandHead_size.csv\n","!wget https://cdn.talentsprint.com/aiml/Experiment_related_data/new_height_weight.csv"],"metadata":{"id":"qontDavXm6wg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQafr7q5eYKn"},"source":["### Importing Required Packages"]},{"cell_type":"code","metadata":{"id":"pGyxdqS0ecSd"},"source":["import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cVclUaWAKsWI"},"source":["### Loading the Datasets"]},{"cell_type":"markdown","metadata":{"id":"YUTEO-lnXmK3"},"source":["1. Loading the height weight dataset"]},{"cell_type":"code","metadata":{"id":"diuLjQGIhDLy"},"source":["height_weight_data = pd.read_csv(\"/content/new_height_weight.csv\")\n","height_weight_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dpT93IeNXszx"},"source":["2. Loading the Human Brain Weight Head Size dataset"]},{"cell_type":"code","metadata":{"id":"ro8sSyBziHat"},"source":["HumanBrain_WeightandHead_Size = pd.read_csv(\"/content/HumanBrain_WeightandHead_size.csv\")\n","HumanBrain_WeightandHead_Size.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZVXjjH7TX1B2"},"source":["3. Loading the boston data"]},{"cell_type":"code","metadata":{"id":"i_l5nQsQKBcC"},"source":["#Loading all the dataset present in sklearn\n","from sklearn import datasets\n","dir(datasets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Azb7vVgfhsV"},"source":["# Loading the dataset from sklearn datasets package\n","from sklearn.datasets import load_boston\n","boston = load_boston()\n","print(boston.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3tcVwmDVfqNy"},"source":["# Consider only the first feature \n","boston_data = boston.data[:, :1]\n","boston_target = boston.target\n","boston_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hperFBBGnTpB"},"source":["# Consider only one feature\n","# boston_data = boston.data[:, np.newaxis, 5] # Considering 5th column data as a feature\n","# boston_target = boston.target\n","# boston_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_tmxN3gDnoOI"},"source":["# Considering 5th column data as a feature\n","# boston_data = boston.data[:, 6].reshape(-1, 1)\n","# boston_target = boston.target\n","# boston_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_HAVRaQqf1QS"},"source":["### Apply Linear Regression on all the three datasets given (Ungraded Exercise)"]},{"cell_type":"markdown","metadata":{"id":"4cEzyN75bImM"},"source":["#### Create an object for Linear regression model"]},{"cell_type":"code","metadata":{"id":"bt8hcbkGbHMU"},"source":["# YOUR CODE HERE\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","reg = LinearRegression()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GL1maqZ4bTdJ"},"source":["#### Perform Linear regression on Height weight data\n","\n","1. Get the features and labels from the data\n","2. Split the data into train and test sets\n","3. Fit the model\n","2. Predict the model with test data\n"]},{"cell_type":"code","metadata":{"id":"GcF5KFjWgL1s"},"source":["# YOUR CODE HERE\n","print(\"shape of height weight data\",height_weight_data.shape)\n","height = height_weight_data.iloc[:,:-1] \n","weight = height_weight_data.iloc[:, 1] \n","\n","#height = height_weight_data.iloc[:,0].values.reshape(-1,1)\n","#weight = height_weight_data.iloc[:,1].values.reshape(-1,1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(height, weight, test_size=0.2)\n","\n","reg1 = LinearRegression()\n","reg1.fit(X_train, y_train)\n","predictions = reg1.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lxsomXktbhZR"},"source":["#### Perform Linear regression on Human Brain Weight and Head Size data\n","\n","1. Get the features and labels from the data\n","2. Split the data into train and test sets\n","3. Fit the model\n","4. Predict the model with test data\n"]},{"cell_type":"code","metadata":{"id":"welyf7CNbSkD"},"source":["# YOUR CODE HERE\n","print(\"shape of height weight data\",HumanBrain_WeightandHead_Size.shape)\n","head_Size = HumanBrain_WeightandHead_Size.iloc[:,:-1]   # exclude last column\n","brain_Weight = HumanBrain_WeightandHead_Size.iloc[:,1]  \n","\n","#head_Size = HumanBrain_WeightandHead_Size.iloc[:,0].values.reshape(-1,1)\n","#brain_Weight = HumanBrain_WeightandHead_Size.iloc[:,1].values.reshape(-1,1)\n","\n","X_train, X_test, y_train, y_test = train_test_split(head_Size, brain_Weight, test_size=0.2)\n","\n","reg2 = LinearRegression()\n","reg2.fit(X_train, y_train)\n","predictions = reg1.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GqRBpBUobqC5"},"source":["#### Perform Linear regression on Boston data\n","1. Fit the model\n","2. Predict the model with test data\n"]},{"cell_type":"code","metadata":{"id":"pZn3HwAmfXH5"},"source":["# YOUR CODE HERE\n","X_train, X_test, y_train, y_test = train_test_split(boston_data, boston_target, test_size=0.2)\n","\n","reg.fit(X_train, y_train)\n","predictions = reg.predict(X_test)\n","print(predictions.shape)"],"execution_count":null,"outputs":[]}]}