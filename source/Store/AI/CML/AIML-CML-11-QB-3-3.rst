UNIT-2 Week-11 Weekly Test Question & Answers 
==============================================

**Question 1**

Given α = 4, β = 5, γ = 2, what would be the output (O1) value

.. image:: https://cdn.extras.talentsprint.com/CentralRepo/AIML_Images/AIML_11_Neuralnetwork.png

Options

A.0

B.1

C.2

D.3

**Answer: B**


**Question 2**

Find the points where minima and maxima occur (respectively) for the following
Function:

f(x) = xˆ3- 3xˆ2 - 24x + 46

Options

A.-2,2

B.4,-2

C.3,-4

D.-2,-2

**Answer: B**



**Question 3**

Read the following `link <https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0>`_ and answer the question below:

Answer True or False, sequentially, for all the questions below:

1. For classification jobs, sigmoid is a good choice of the activation function
2. The tanh and sigmoid have different ranges but similar characteristics
3. Among standard activation functions, ReLU has a high computational complexity
4. By choosing an appropriate activation function we can reduce training time

Options

A.True, True, False, True

B.False, True,True, False

C.False, False,False, True

D.True,False,True,False

**Answer: A**


**Question 4**

Read the following `link <https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0>`_ and answer the question below:

If we are facing the vanishing gradient problem.

Options

A.We are probably using sigmoid but definitely not tanh

B.It is more likely that we are using the sigmoid or tanh and less likely that we are using leaky ReLu

C.We are probably using tanh but definitely not sigmoid

D.We are definitely using a linear activation function

**Answer: B**


