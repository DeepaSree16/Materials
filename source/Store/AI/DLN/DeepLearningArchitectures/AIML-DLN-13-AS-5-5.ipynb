{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-13-AS-1-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2gD6DFr189TS"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"JkxYpEOgC2-q"},"source":["\n","## Learning Objectives\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q8a1fbkpD3h3"},"source":["At the end of the experiment, you will be able to learn:\n","*  reconstructing images using convolutional autoencoder.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8fGMzMRFD58Q"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"-dXYsmesDTWo"},"source":["### Description\n","\n","The fingerprint dataset has 320 images, 80 images per sensor and each sensor have varying image sizes. It consists of 4 different sensors fingerprints namely :\n","\n","* Low-cost Optical Sensor\n","* Low-cost Capacitive Sensor\n","* Optical Sensor \n","* Synthetic Generator\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w2ak1KYfqpPz"},"source":["### Autoencoder\n","\n","An autoencoder is made up of two components- the encoder and decoder network. The task of the encoder is to generate a lower dimensional embedding Z, which is referred to latent vector, or latent representation. After that, we have the decoder stage in which Z is reconstructed to X' prime, which is the same as X (input).\n","\n","\n","![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/6.png)\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Fingerprints.zip\n","! unzip /content/Fingerprints.zip"],"metadata":{"id":"cLOawM4VbgpI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3ksxwTIbz8X"},"source":["### Importing Required Packages"]},{"cell_type":"code","metadata":{"id":"iFLQbbFiQSRX"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","from glob import glob\n","from tifffile import imread\n","from skimage.transform import resize\n","import torch\n","import torch.nn as nn      \n","import torch.nn.functional as F\n","import torch.optim as optim\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D0XK3l2fcLTx"},"source":["### Load the  data"]},{"cell_type":"markdown","metadata":{"id":"jBic0RTDr4XY"},"source":["#### About glob.iglob:\n","\n","The glob library  provides methods for traversing the file system and returning files that matched a defined set of glob patterns.\n","\n","**Note:** Refer to  [glob.iglob](https://docs.python.org/3/library/glob.html)"]},{"cell_type":"code","metadata":{"id":"uZtHPdpMSQAc"},"source":["data = glob('/content/fingerprints/DB*/*')\n","images = []\n","for i in range(len(data)):\n","\n","    # Reading the data using imread\n","    img = imread(data[i])\n","\n","    # Resize the images to 224 * 224 as the images are of different sizes\n","    img = resize(img,(224,224))\n","    \n","    # Appending all the images \n","    images.append(img)\n","\n","# Converting the images into float32 array\n","images_arr = np.asarray(images)\n","images_arr = images_arr.astype('float32')\n","print(\"Dataset:\", images_arr.shape)# The data has a shape of 320 x 224 x 224 since there are 320 samples each of the 224 x 224-dimensional matrix."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DC2mQzZmcPCt"},"source":["### Visualizing the Images "]},{"cell_type":"code","metadata":{"id":"IFUe2gi0TrdE"},"source":["# Display the first 6 images in the dataset\n","for i in range(6):\n","  plt.subplot(3,3, i+1)\n","  plt.axis('off')\n","  plt.imshow(images_arr[i], cmap=\"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NQIF2nATcUxH"},"source":["### Data Preparation\n","\n","The images are in grayscale with a dimension of 224 X 224 and the number of channels for grayscale image is '1'. Reshaping the input array into four dimensions to feed into the Neural Network, which is 320 X 1 X 224 X 224 (nsamples, nchannels, height, width)"]},{"cell_type":"code","metadata":{"id":"rdKipz6oQWPm"},"source":["images_arr = images_arr.reshape(-1,1, 224,224)\n","print(images_arr.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W11JtAruDmNd"},"source":["### Split the data into training and a validation set."]},{"cell_type":"code","metadata":{"id":"GbYBFNHPDjeq"},"source":["from sklearn.model_selection import train_test_split\n","# Training images both act as the input as well as the ground truth similar to the labels have in the classification task\n","train_X,valid_X,train_ground,valid_ground = train_test_split(images_arr,images_arr,test_size=0.2,random_state=13)\n","train_X.shape,valid_X.shape,train_ground.shape,valid_ground.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"us2lW783RAyn"},"source":["# To convert numpy to tensor, load the data using tensordataset and convert the values to FloatTensor\n","train_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(train_X),torch.FloatTensor(train_ground))\n","test_dataset = torch.utils.data.TensorDataset(torch.FloatTensor(valid_X),torch.FloatTensor(valid_ground))\n"," \n","# Loading the train dataset aand test dataset \n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64)\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbm-8HxIqvxR"},"source":["**Initializing CUDA**\n","\n","CUDA is used as an interface between our code and the GPU.\n","\n","Normally, we run the code in the CPU. To run it in the GPU, we need CUDA. Check if CUDA is available:"]},{"cell_type":"code","metadata":{"id":"YHj_ZREiqvxU"},"source":["# To test whether GPU instance is present in the system or not.\n","use_cuda = torch.cuda.is_available()\n","print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuGyWSz8q4NQ"},"source":["If it's False, the code is runining on CPU. If it's True, the code is runinng  on GPU.\n","\n","Let us initialize some GPU-related variables:"]},{"cell_type":"code","metadata":{"id":"lVUOypccj9yO"},"source":["device  =  torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qdltnANL3A03"},"source":["### Defining the Convolutional Autoencoder Architecture"]},{"cell_type":"markdown","metadata":{"id":"TEhqiDSmfE3C"},"source":["Define the Convolutional Autoencoder as a class where the encoding network component is made up of two convolutional layers to compress the data. Decoding network component is made up of two convolutional layers. Each layer output in encoding and decoding network are passed through Relu activation function in the forward function.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w6MgNN46B3sE"},"source":["The autoencoder is divided into two parts:\n","\n","**Encoder**\n","\n","The first layer will have 32 output channels with filter size 3 x 3\n","\n","The second layer will have 64 output channels with filter size 3 x 3, followed by a downsampling (max-pooling) layer,\n","\n","\n","\n","**Decoder**\n","\n","The first layer will have 32 output channels with filter size size 2 x 2\n","\n","The second layer will have 1 output channels with filter size size 2 x 2 \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"eJbe1Wtpg5AH"},"source":["# Define the Convolutional Autoencoder\n","class ConvAutoencoder(nn.Module):\n","    def __init__(self):\n","        super(ConvAutoencoder, self).__init__()\n","       \n","        # Encoder\n","        # Defining the convolution layer with input_channels = 1, output_channels = 32, kernel_size = 3, padding =1\n","        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)  \n","        # Defining the convolution layer with input_channels = 32, output_channels = 64, kernel_size = 3, padding =1\n","        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n","        # Max pooling layer with filter size 2x2\n","        self.pool = nn.MaxPool2d(2, 2)\n","      \n","        \n","        # Decoder \n","        # Defining the convolution layer with input_channels = 64, output_channels = 32, kernel_size = 2, stride =2\n","        self.t_conv1 = nn.ConvTranspose2d(64, 32, 2, stride=2)\n","        # Defining the convolution layer with input_channels = 32, output_channels = 1, kernel_size = 2, stride =2\n","        self.t_conv2 = nn.ConvTranspose2d(32, 1, 2, stride=2)\n","      \n","\n","    def forward(self, x):\n","        # Linear layers with RELU activation\n","        x = F.relu(self.conv1(x))\n","        x = F.relu(self.conv2(x))\n","        x = self.pool(x)\n","        x = F.relu(self.t_conv1(x))\n","        x = self.pool(x)\n","        x = F.relu(self.t_conv2(x))\n","               \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0hIHbU7ZEZ-O"},"source":["#### Calling the instances of the network"]},{"cell_type":"code","metadata":{"id":"TE8se2sLAwXI"},"source":["\n","#Instantiate the model\n","model = ConvAutoencoder().to(device)\n","print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bn9n2HwLEdZ-"},"source":["#### Defining the loss function and optimizer"]},{"cell_type":"code","metadata":{"id":"1Aa-oEGDRPIj"},"source":["# Initialization of Mean Square Error\n","loss_func = nn.MSELoss()\n","\n","# Initialization of Optimizer\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.005)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H1QKgmOIEvx0"},"source":["### Training the Model \n","\n","Applying Autoencoders on the train data and finding the loss on the train dataset"]},{"cell_type":"code","metadata":{"id":"wr5IrB8IRnwc"},"source":["EPOCH = 10\n","for epoch in range(EPOCH):\n","    for x, y in train_loader:\n","        t_x = x.to(device)\n","        t_y = y.to(device)\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # Passing the data to the model (Forward Pass)\n","        decoded1 = model(t_x)\n","\n","        # Calculating mean square error loss\n","        loss = loss_func(decoded1, t_y) \n","        train_loss = loss.item()\n","\n","        # Performing backward pass (Backpropagation)\n","        loss.backward() \n","      \n","        # optimizer.step() updates the weights accordingly                    \n","        optimizer.step()  \n","    print('Epoch: ', epoch, '| train loss: %.4f' % train_loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CGMFhAOeidUt"},"source":["### Evaluate the Model"]},{"cell_type":"markdown","metadata":{"id":"V7TDs5zyEpw8"},"source":["Applying Autoencoders on the test data and finding the loss on the test dataset"]},{"cell_type":"code","metadata":{"id":"qWYbOndyJWSw"},"source":["# Keeping the network in evaluation mode \n","model.eval()\n","for x, y in test_loader:\n","  # Convert the images and labels to gpu for faster execution\n","  eval_x = x.to(device)\n","  eval_y = y.to(device)\n","  # Passing the data to the model (Forward Pass)\n","  decoded2 = model(eval_x)\n","   # Calculating mean square error loss\n","  loss = loss_func(decoded2, eval_y)\n","print(loss)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbduvLqrFMfL"},"source":["### Visualizing the reconstruct images of the test data."]},{"cell_type":"code","metadata":{"id":"Yv3ZC8aOKJqL"},"source":["f, a = plt.subplots(2, 5, figsize=(8,6))\n","for i in range(5):\n","  a[0,i].imshow(eval_x[i].detach().cpu().numpy().reshape(224,224), cmap='gray')\n","  a[0,i].set_xticks(()); \n","  a[0,i].set_yticks(())\n","  a[0,0].title.set_text('Test Images')\n","  \n","\n","for i in range(5):\n","  \n","  a[1,i].imshow(decoded2[i].detach().cpu().numpy().reshape(224,224), cmap='gray')\n","  a[1,i].set_xticks(())\n","  a[1,i].set_yticks(())\n","  a[1,0].title.set_text('Reconstructed Test Images')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WeBPTt5RFnTv"},"source":["From the above figures, you can observe that your model did a good job of reconstructing the test images that you predicted using the model. At least visually, the test and the reconstructed images look almost similar."]}]}