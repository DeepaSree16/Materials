{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-14-AS-1-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vfbdl_SC0WsP"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"nqqRjnCqz1-e"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"bSefHHdRz5Z3"},"source":["At the end of the experiment, you will be able to:\n","\n","* Understand the challenges of information retrieval on Wikipedia articles\n","* Weighted kNN algorithm for relevance feedback \n","* Rocchio algorithm for relevance feedback "]},{"cell_type":"code","metadata":{"id":"0kDzYJ_17eVC","cellView":"form"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_batch_14/rocchios_algorithm.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"coZOItoX0Bhy"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"IFrW8XRP0HZl"},"source":["### History:\n","Wikipedia dataset is the most widely-used dataset for information retrieval. It is based on Wikipediaâ€™s \"featured articles\", a continually updated article collection. There are totally 29 categories in \"featured articles\". Each article is split into several sections according to its section headings. This dataset is finally generated as a set of 2,866 image/text pairs.\n","\n","As an important benchmark dataset for information retrieval, The wikipedia dataset has been widely used since being publicly available.\n","\n","**Wikipedia featured articles:**\n","\n","Featured articles are considered to be some of the best articles Wikipedia has to offer, as determined by Wikipedia's editors. They are used by editors as examples for writing other articles. Articles are reviewed as featured article candidates for accuracy, neutrality, completeness, and style, before being added to . There are 5,410 featured articles out of 5,754,701 articles on the English Wikipedia (~0.1% are featured).\n","\n","### Description:\n","\n","The dataset contains the following:\n","1. 2866 Wikipedia articles belonging to a total of 29 categories.\n","2. Each article is given a query ID based on it's category.\n","    \n","\n","\n","\n","### Challenges:\n","\n","The following challenges are associated with query based retrieval of documents:\n","\n","1.   Incomplete query or semantically wrong query\n","2.   Huge number of documents returns as a result to the query"]},{"cell_type":"markdown","metadata":{"id":"HkXdKH0A0JJy"},"source":["## Domain Information"]},{"cell_type":"markdown","metadata":{"id":"L18qKAxq0LA3"},"source":["\n","\n","Information Retrieval (IR) systems allow users to access large amounts of data. A user submitting a request to an IR system will receive a number of results relating to the request. The results may include images, pieces of text,\n","web pages, segments of video or speech samples. \n","\n","But, the users may not have a well-developed idea of what information they are searching for, they may not be able to express entirely what they want into a suitable query and they may not have a good idea of what information is available for retrieval. \n","\n","To solve the above problem, the system after presenting the user with an initial set of documents a feedback mechanism could identify those documents that had useful information. This is the Relevance Feedback (RF). The system then can use this information quantitatively - retrieving more documents like the relevant documents - and qualitatively - retrieving only those documents similar to the relevant documents. Therefore RF is a cycle.\n","\n","The image below helps us understand the Relevance Feedback cycle:\n","\n","<img src = \"\n","https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/1.1.png\" width = \"700\" height = \"500\">"]},{"cell_type":"markdown","metadata":{"id":"NQZr_6dN0TLV"},"source":["## AI / ML Technique"]},{"cell_type":"markdown","metadata":{"id":"Cb18GQOJ0WAQ"},"source":["### Relevance Feedback for Information Retrieval\n","\n","In this experiment, dataset is queried to get relevant documents. The significant part of any Information retrieval system is to make it responsive to user interaction. Hence our system needs to incorporate the feedback and refine the results using the feedback on the fly.\n","\n","Search engines like Google, Bing and other websites like Youtube are use - cases which personalize the content for  better user experience.\n","\n","In this experiment we implement two algorithms to achieve adaptive retrieval based on user input.\n","\n","Below steps are specific to this experiment. In the real-word, based on the usecase, few or all of these steps can be applied.\n","\n","1. #### Weighted kNN \n","\n","     The steps are:\n","\n","    - Random list of wikipedia documents (varied category) are displayed to the User. \n","    - The user based on his interest clicks on any document belonging to a particular category. \n","    - We implement a weighted kNN to get a ranking of the relevant documents based on the user input.\n","    - The user then again clicks a document of the relevant documents.\n","    - We update the weight vector to get a new ranking which ensures better recall than before\n","\n","\n","2. #### Rocchio Algorithm\n","\n","     The steps are:\n","      \n","    - Random list of documents are displayed.\n","    - The user clicks on a particular document and also decides the relevant and non relevant documents of the displayed ones.\n","    - The initial document clicked is our Query $q$ (initial centroid for relevant documents). <br> Relevant Documents are $D_{r}$, Non relevant ones are $D_{nr}$.\n","    - We use the following formula to converge to a new centroid for the relevant documents.<br> This ensures better precision with every iteration\n","\n","$$q_{t+1} = a . q_{t} + b. (\\frac{1}{|D_{r}|} \\sum_{d_{j} \\in D_{r}} d_{j}) - c. (\\frac{1}{|D_{nr}|} \\sum_{d_{k} \\in D_{nr}} d_{k})$$\n","\n","- Display the nearest neighbours of the new centroid $q_{t+1}$, which would be more relevant\n","\n"]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week12/Exp1/wikipedia_dataset.tar.gz\n","! tar xvf  wikipedia_dataset.tar.gz"],"metadata":{"id":"bg7PWBMzp5_F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GFPkgHv_RULY"},"source":["### Importing required packages"]},{"cell_type":"code","metadata":{"id":"_X-Y2YMr00-a"},"source":["from glob import glob\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import numpy as np\n","import random\n","from bs4 import BeautifulSoup as Soup\n","from sklearn.neighbors import NearestNeighbors\n","import pandas as pd\n","\n","from IPython.display import clear_output\n","from IPython.display import display, HTML\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BcxI0fIBoESV"},"source":["f = open('wikipedia_dataset/categories_originalids.list')\n","cat = f.readlines()\n","print('The following ' + str(len(cat)) + ' categories', cat)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aVlMqadF00-2"},"source":["### Parsing the XML data files "]},{"cell_type":"code","metadata":{"id":"Q_auParQ00-4"},"source":["def parseLog(file, docs, qid):\n","    # Read the file\n","    handler = open(file).read()\n","    # To the Soup pass the data to be parsed and the name of a parser 'lxml' as parameters\n","    soup = Soup(handler,'lxml')\n","    # Finding the text\n","    txt = soup.findAll('text')[0].text       \n","    for message in soup.findAll('document'):\n","        # message.attrs attribute, which returns a dictionary of key-value pairs\n","        msg_attrs = dict(message.attrs)\n","        qid.append(int(msg_attrs['cat']))\n","        docs.append(msg_attrs['name'] + ' ' + txt)\n","\n","lis = glob('wikipedia_dataset/texts/*')\n","docs = []\n","qid = []\n","for i in range(len(lis)):\n","    parseLog(lis[i], docs, qid)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uC5zJJMy00_A"},"source":["print('Sample document:', qid[100], docs[100][:100] + '...')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TW0-0wt00_I"},"source":["### Construct feature vector from documents"]},{"cell_type":"code","metadata":{"id":"MXWWTbcr00_K"},"source":["stoplist = open('wikipedia_dataset/stopwords.txt').read().splitlines()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vxKov96n00_Q"},"source":["# TF-IDF Vectorizer\n","vectorizer = TfidfVectorizer(stop_words=stoplist, min_df=0.001)\n","matrix = vectorizer.fit_transform(docs)\n","doc_vectors = matrix.todense()\n","doc_vectors = np.array(doc_vectors)\n","vocab = vectorizer.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yv0zIPBu00_a"},"source":["print('Length of the vocabulary:', len(vocab))\n","print('A sample ' +str(len(vocab)) +' dimensional document vector:', doc_vectors[2][:100])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OcZAAhVN00_o"},"source":["print(doc_vectors.shape, len(qid))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ks7ZCRnf00_4"},"source":["### 1. Weighted KNN \n","- Initially display random 10 documents and get user click\n","- Run the KNN Algorithm and choose 100 nearest neighbours\n","- Define a weight matrix to get the ranking of the 100 relevant vectors based on similarity\n","\n","Given $m$ - dimensional query $q$ and document $d_{i}$, the weighted similarity is calculated as follows <br> \n","$q = (q_{1}, q_{2}, \\cdots, q_{m})$ <br>\n","$d_{i} = (d_{1}, d_{2}, \\cdots, c_{m})$ <br>\n","$$Similarity = w^T d'_{i}$$ where ,\n","$$d'_{i} = \\frac{q * d_{i}}{\\parallel q \\parallel \\parallel d_{i} \\parallel}  $$"]},{"cell_type":"code","metadata":{"id":"XElrswMo00_6"},"source":["# Fit KNN Model\n","KNN = NearestNeighbors(100, metric = 'cosine')\n","KNN.fit(doc_vectors)\n","neighbours = KNN.kneighbors(doc_vectors,return_distance=False)\n","print(neighbours.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d74_Jk9-01AE"},"source":["# Number of articles to be displayed \n","num = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wsVDV7P01AO"},"source":["def to_display(doc_ids, init= False):\n","    if init == True:\n","        ix = random.sample(range(0, len(doc_vectors)), num)\n","    else:\n","        ix = doc_ids\n","    \n","    df = pd.DataFrame(columns=['index', 'Article','category'])\n","    df['index'] = ix\n","    df['Article'] = [docs[i][:100] for i in ix]\n","    df['category'] = [cat[qid[i]-1][:-1] for i in ix] \n","    clear_output()\n","    display(HTML(df.to_html()))\n","    return ix\n","\n","index = to_display([],init= True)\n","res = []\n","ctr = 0\n","\n","# Initialize the weight vector to ones\n","weights = np.ones((doc_vectors.shape[1],1))\n","while (True):  \n","    doc_id = input(\"Select the index of the article you would like to read or type 'stop' if you want to end the search\")\n","    if doc_id == 'stop':\n","        break    \n","    doc_id = int(doc_id) \n","\n","    query_doc = doc_vectors[doc_id].reshape(1, doc_vectors.shape[1])\n","    relevant_docs_ix = neighbours[doc_id]\n","    relevant_docs = doc_vectors[relevant_docs_ix]\n","\n","    # Calculate Similarity\n","    new_relevance = (query_doc[0] * relevant_docs) / (np.linalg.norm(relevant_docs) * np.linalg.norm(query_doc))\n","    similarity = np.matmul(new_relevance , weights)\n","\n","    # Return the number of most ranked doc ids\n","    ranked_docs_ix = relevant_docs_ix[np.argsort(similarity[:,0])]\n","    index = to_display(ranked_docs_ix[:num])\n","    \n","    weights = weights + doc_vectors[doc_id].reshape(doc_vectors.shape[1], 1)\n","    weights = weights/np.linalg.norm(weights)\n","    ctr += 1\n","    res_ = input(\"Give the precision count\\a number between 1-10 about how satisfied are you with the results\")\n","    res.append(res_)\n","\n","print('Enjoy your Article')\n","plt.plot(res)\n","plt.xlabel(\"number of searches\")\n","plt.ylabel(\"Score given by you\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vr-Q32V301AY"},"source":["### 2. Rocchio Algorithm\n","- Initially random articles are displayed\n","- Select an index value based on your choice\n","- Select all the indices of relevant documents\n","- Update the centroid of relevant vector as shown in the formula above\n","- Display the nearest neighbors of the relevant vector\n","\n","- $alpha$ - how close to relevant \n","- $beta$ - how far from non relevant\n","\n","Ideally we would like to have high $alpha$ and low $beta$"]},{"cell_type":"code","metadata":{"id":"aA-JfYdu01Aa"},"source":["alpha = 1.0\n","beta = 0.5\n","index = to_display([], init= True)\n","while(True):\n","    doc_id = input(\"Select the index of the article you would like to read or type 'stop' if you want to end the search\")\n","    if doc_id == 'stop':\n","        break\n","    print(\"Enter the indices, comma separated for relevant documents\")\n","    R = [int(x) for x in input().split(',')]\n","    NR = [i for i in index if i not in R]\n","    doc_id = int(doc_id)\n","    query_doc = doc_vectors[doc_id].reshape(1, doc_vectors.shape[1])\n","    R_docs = doc_vectors[R]\n","    NR_docs = doc_vectors[NR]\n","\n","    q_new = query_doc + (alpha * np.sum(R_docs, 0)/len(R)) - (beta * np.sum(NR_docs, 0)/len(NR))\n","    print(q_new.shape)\n","    retrieved = KNN.kneighbors(q_new, return_distance=False)\n","    index = to_display(retrieved[0][:num])\n","    \n","print('Enjoy your article')"],"execution_count":null,"outputs":[]}]}