{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-08-AS-1-3.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"HjrYIpy2yYbI"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"x77SzRGJcY3o"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"UKB4OmrMccgl"},"source":["At the end of the experiment,  you will be able to :\n","\n","* Understand Neural network with more than one neuron"]},{"cell_type":"code","metadata":{"id":"9OerY5NgyXqn","cellView":"form"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/MLP_Walkthrough.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nFEZUAvJdXcW"},"source":["\n","\n","## Dataset"]},{"cell_type":"markdown","metadata":{"id":"Kz2T6C2MrPp6"},"source":["#### History\n","\n","This is a multivariate dataset introduced by R.A.Fisher (Father of Modern Statistics) for showcasing linear discriminant analysis. This is arguably the best known dataset in Feature Selection literature.\n","\n","\n","The data set contains 3 classes of 50 instances each, where each class refers to a type of iris plant. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other. "]},{"cell_type":"markdown","metadata":{"id":"62AAQoRKrQIK"},"source":["#### Description\n","The Iris dataset consists of 150 data instances. There are 3 classes (Iris Versicolor, Iris Setosa, and Iris Virginica) each has 50 instances. \n","\n","\n","For each flower, we have the below data attributes \n","\n","- sepal length in cm\n","- sepal width in cm\n","- petal length in cm\n","- petal width in cm\n"]},{"cell_type":"markdown","metadata":{"id":"06lEbv7grYmm"},"source":["## Domain Information"]},{"cell_type":"markdown","metadata":{"id":"QIB3dtlNrWy6"},"source":["\n","\n","Iris Plants are flowering plants with showy flowers. They are very popular among movie directors as it gives an excellent background. \n","\n","They are predominantly found in dry, semi-desert, or colder rocky mountainous areas in Europe and Asia. They have long, erect flowering stems and can produce white, yellow, orange, pink, purple, lavender, blue, or brown colored flowers. There are 260 to 300 types of iris.\n","\n","![alt text](https://cdn-images-1.medium.com/max/1275/1*7bnLKsChXq94QjtAiRn40w.png)\n","\n","As you could see, flowers have 3 sepals and 3 petals.  The sepals are usually spreading or drop downwards and the petals stand upright, partly behind the sepal bases. However, the length and width of the sepals and petals vary for each type.\n"]},{"cell_type":"markdown","metadata":{"id":"RlGNyzR5fsxQ"},"source":["## AI / ML Technique"]},{"cell_type":"markdown","metadata":{"id":"Vy4vk5Fdfwg3"},"source":["A feedforward neural network is an artificial neural network where the connections between units do not form a cycle. Feedforward neural networks were the first type of artificial neural network invented and are simpler than their counterparts. Feedforward travels forward in the network (no loops), first through the input nodes, then through the hidden nodes, and finally through the output nodes."]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Iris.csv\n"],"metadata":{"id":"xuk_lMvxmwDZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_R_HrXRcyYbL"},"source":["## Importing Required Packages"]},{"cell_type":"code","metadata":{"id":"FGFqBo0HIV-J"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AKPhAgnZIdIp"},"source":["##Load Data"]},{"cell_type":"code","metadata":{"id":"HEXS-fVb3aQK"},"source":["# Load data using Pandas\n","iris = pd.read_csv(\"/content/Iris.csv\")\n","iris.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNJ_Zjc-uqxn"},"source":["# Shuffling the data using .sample()\n","df = iris.sample(frac = 1)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s4d4Bo84Npb1"},"source":["df.drop('Id',axis = 1,inplace = True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fFDwV8WPqBYm"},"source":["## Label Encoding\n","\n","Let us now encode our species column, to understand more about label encoding, refer [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)"]},{"cell_type":"code","metadata":{"id":"_MeiqaqXyUq7"},"source":["from sklearn.preprocessing import LabelEncoder\n","le = LabelEncoder()\n","df['species'] = le.fit_transform(df['species'])\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iShL-KB3MfH5"},"source":["## Neural Network with more than one Neuron\n","\n","Multilayer perceptron (MLP) is a cascade of single-layer perceptrons. There is a layer of input nodes, a layer of output nodes, and one or more hidden layers.\n"]},{"cell_type":"markdown","metadata":{"id":"OmgHwYLRpfAC"},"source":["![alt text](https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Nueral_Net_MLP.PNG)"]},{"cell_type":"markdown","metadata":{"id":"ZhNiaTvy7Q1k"},"source":["### Part-A: Understand Multi Layer Perceptron using mathematical approach"]},{"cell_type":"code","metadata":{"id":"Mohw8NCW1Ohv"},"source":["# Let's take 3 samples from dataset from the location 1, 51, 101\n","df.loc[[1, 51, 101]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wbWwHkzkMlTy"},"source":["# Inputs and Weights could be randomly selected. Let's consider a hidden layers.\n","\n","# Choose any one sample from the above features\n","inputs = [[4.9, 3.0, 1.4, 0.2]] \n","\n","# Defining random weights for hidden layer \n","hidden_layer_weights = [[ 0.9,  0.8, -1.0, -1.0], [-0.5, -0.5,  1.5,  1.0]]\n","\n","# Defining random weights for output layer \n","output_layer_weights = [[ 2.0, -1.0], [ 1.0,  1.0],  [-1.0,  2.0]]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1F6lVzR4A-GP"},"source":["# This function returns the sum of product of inputs and weights\n","def weighted_sum(inputs, weights):\n","    total = 0\n","    for inputs_value,weight in zip(inputs, weights):\n","        total += inputs_value * weight\n","    return total"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"61oThPzfe3IF"},"source":["### Sigmoid Function\n","\n","The sigmoid function is a mathematical function having a characteristic “S” — shaped curve, which transforms the values between the range 0 and 1. For a given number n, we can see that the sigmoid function would map that number between 0 and 1. As the value of n gets larger, the value of the sigmoid function gets closer to 1 and as n gets smaller, the value of the sigmoid function gets closer to 0. \n"]},{"cell_type":"code","metadata":{"id":"nucemkrpBCFV"},"source":["def sigmoid(x):\n","    s = 1 / (1 + np.exp(-x))\n","    if s >= 0.8:\n","        return 1\n","    else:\n","        return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nSRrCiRgMvOl"},"source":["# This function returns the sum of product of inputs and weights\n","\n","def determine_layer_outputs(list_of_inputs, list_of_weights, activation_function = True):\n","    list_of_outputs = []\n","    for inputs in list_of_inputs:\n","        node_outputs = []\n","        for weights in list_of_weights:\n","            node_input = weighted_sum(inputs, weights)  \n","            if activation_function:\n","                node_output = sigmoid(node_input)\n","            else:\n","                node_output = node_input\n","            node_outputs.append(node_output)\n","            \n","        list_of_outputs.append(node_outputs)\n","    \n","    return list_of_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BmDZpY5bWMiY"},"source":["### Calculating outputs of the hidden layer\n","\n"]},{"cell_type":"code","metadata":{"id":"MzHGWbjOSsoe"},"source":["# Get the outputs of the first hidden layer before passing through the activation\n","first_hidden_layer_inputs = determine_layer_outputs(inputs, hidden_layer_weights, activation_function = False)\n","\n","print(\"Before applying activation function\", first_hidden_layer_inputs) \n","\n","# Get the outputs of the first hidden layer after passing through the activation\n","first_hidden_layer_outputs = determine_layer_outputs(inputs, hidden_layer_weights)\n","print(\"After applying activation function\", first_hidden_layer_outputs) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vq7Q-4ynWaWI"},"source":["### Calculating Output Layer"]},{"cell_type":"code","metadata":{"id":"0oIkVFzjS0MK"},"source":["output_layer_inputs = determine_layer_outputs(first_hidden_layer_outputs, output_layer_weights, activation_function = False)\n","print(\"Before applying activation function\", output_layer_inputs)\n","\n","# After passing through the activation function\n","output_layer_outputs = determine_layer_outputs(first_hidden_layer_outputs, output_layer_weights)\n","print(\"After applying activation function\", output_layer_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RmUoLx2q2dUV"},"source":["### Performing for all samples\n","\n","Performing weighted sum  and sigmoid function for hidden layer and output layer for each of the features by taking random weights and finding accuracy for actuals and predictions. \n","\n","In the Output layer after applying activation fuction, the predictions for:\n","\n","Setosa =  [1,0,0]\n","\n","Versicolor = [0,1,0]\n","\n","Virginica = [0,0,1]\n","\n","For label setosa, the output layer should return [1,0,0] (which means first position neuron is activated), similarly for versicolor and virginica."]},{"cell_type":"code","metadata":{"id":"sgTicVgJL1CC"},"source":["samples = df.drop(columns=['species']).values\n","predicted = []\n","\n","# Defining weights for hidden and output layers\n","hidden_layer_weights1 = [[ 0.9,  0.8, -1.0, -1.0], [-0.5, -0.5,  1.5,  1.0]]\n","output_layer_weights1 = [[ 2.0, -1.0], [ 1.0,  1.0],  [-1.0,  2.0]] # Change the weights and observe the change in accuracy\n","\n","for sample in samples:\n","  # Hidden layer\n","  first_hidden_layer_outputs1 = determine_layer_outputs([sample], hidden_layer_weights1)\n","\n","  # Output layer\n","  output_layer_outputs = determine_layer_outputs(first_hidden_layer_outputs1, output_layer_weights1)\n","\n","  # Selecting the node which is activated; out of 3 output nodes only one node gives output as 1\n","  predicted.append(output_layer_outputs[0].index(max(output_layer_outputs[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CYhZnjkz6gx2"},"source":["Comparing the predictions and actual labels and calculating the accuracy"]},{"cell_type":"code","metadata":{"id":"43Q5aM6u6e0l"},"source":["actual = df.iloc[:, 4].values\n","print(\"Actual labels from the data\", actual)\n","print(\"Predicted labels\", predicted)\n","print(\"Length of actual and predicted are\", len(actual), len(predicted))\n","\n","acc = accuracy_score(actual, predicted)\n","\n","print(\"Accuracy of the MLP using mathematical approach\", acc)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dgWCAX_3wG1B"},"source":["### Part-B: MLP from sklearn\n","\n","1. From the given data, Select features and labels\n","2. Split the data into train and test sets \n","3. Train using MLP Classifier"]},{"cell_type":"code","metadata":{"id":"X6hKRCdtvE_x"},"source":["# Extracting features and labels from the data\n","labels = df.species.values\n","features = df.iloc[:, :4].values # Features\n","features.shape, labels.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e9AngdEsuQ9W"},"source":["from sklearn.model_selection import train_test_split\n","xtrain, xtest, ytrain, ytest = train_test_split(features, labels, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aYoAEWktvJSc"},"source":["from sklearn.neural_network import MLPClassifier\n","model = MLPClassifier(random_state=42)\n","# Fitting the data into the model\n","model.fit(xtrain, ytrain)\n","# Predicting the labels for test data\n","accuracy = model.score(xtest, ytest)\n","print(\"Accuracy of MLP Classifier\", accuracy)"],"execution_count":null,"outputs":[]}]}