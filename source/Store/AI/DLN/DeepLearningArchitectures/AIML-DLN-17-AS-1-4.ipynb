{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-17-AS-2-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-Va8_ShN-EKe"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n"]},{"cell_type":"markdown","metadata":{"id":"xUZ0_bK9MvLw"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"iijYVnsgMxj3"},"source":["At the end of the experiment, you will be able to :\n","\n","* understand how to deal with the text preprocessing\n","* create effective semantic representations of variable sized text\n"]},{"cell_type":"code","metadata":{"cellView":"form","id":"yUJpt-lB1J5n"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/Autoencoders_Word2vec_Walkthrough.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MnZvg3FgOVah"},"source":["## Dataset\n","\n","In this experiment we use the 20 newsgroup dataset"]},{"cell_type":"markdown","metadata":{"id":"rHLqaq2FOXDT"},"source":["\n","\n","### Description\n","\n","This dataset is a collection of approximately 20,000 newsgroup documents, partitioned nearly evenly across 20 different newsgroups. That is there are approximately one thousand documents taken from each of the following newsgroups:\n","\n","    alt.athesim\n","    comp.graphics   \n","    comp.os.ms-windows.misc\n","    comp.sys.ibm.pc.hardware\n","    comp.sys.mac.hardware\n","    comp.windows.x\n","    misc.forsale\n","    rec.autos\n","    rec.motorcycles\n","    rec.sport.baseball\n","    rec.sport.hockey\n","    sci.crypt\n","    sci.electronics\n","    sci.med\n","    sci.space\n","    soc.religion.christian\n","    talk.politics.guns\n","    talk.politics.mideast\n","    talk.politics.misc\n","    talk.religion.misc\n","\n","The dataset consists **Usenet** posts--essentially an email sent by someone to that newsgroup. They typically contain quotes from previous posts as well as cross posts; that is a few posts that may be sent to more than one newsgroup.\n","\n","Each newsgroup is stored in a subdirectory, with each post stored as a separate file.\n","\n","Data source to this experiment: http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"]},{"cell_type":"markdown","metadata":{"id":"nbdQqI60Tqz4"},"source":["## Domain Information"]},{"cell_type":"markdown","metadata":{"id":"7kawRr2DTt0T"},"source":["\n","A newsgroup, despite the name, has nothing to do with news. It is what we would call today a mailing list or a discussion forum. *Usenet* is a distributed discussion system designed and developed in 1979 and deployed in 1980.  \n","\n","Members joined newsgroups of interest to them and made *posts* to them. Posts are very similar to email -- in later years, newsgroups became mailing lists and people posted via email.  "]},{"cell_type":"markdown","metadata":{"id":"AEa9GIjoQQI6"},"source":["## AI/ML Technique"]},{"cell_type":"markdown","metadata":{"id":"_SWHkVYyQU-Q"},"source":["### Autoencoder\n","\n","An autoencoder is a feedforward, non-recurrent neural network which is similar to a multilayer perceptron (MLP) ‚Äì having an input layer, an output layer and one or more hidden layers connecting them. However, in an autoencoder, the output layer has the same number of nodes as the input layer and reconstructs its own inputs (instead of predicting the target value Y given inputs X). \n","\n","\n","An autoencoder is made up of two components- the encoder and the decoder network. The task of the encoder is to generate a lower-dimensional embedding Z, which is referred to as latent vector, or latent representation. After that, we have the decoder stage in which Z is reconstructed to X' prime, which is the same as X (input).\n","\n","\n","![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/6.png)\n"]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp2/AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip\n","! unzip AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip\n","! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp2/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin\n","    "],"metadata":{"id":"om_VJja1brig"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9zex8Sk91oiN"},"source":["**Note** : To Download \"AIML_DS_NEWSGROUPS_PICKELFILE.pkl and AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin\" file it will take 30 mins."]},{"cell_type":"markdown","metadata":{"id":"SZUjWvk_-EKk"},"source":["### Importing required packages"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"lkmPNGdN-EKn"},"source":["# Importing pytorch packages\n","import torch\n","from torch import nn\n","import torch.optim as optim\n","\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","import numpy as np \n","import gensim\n","\n","# Matplotlib is used for ploting graphs\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BQU75ihE-EKs"},"source":["### Loading the dataset"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"SNAgKctO-EKt"},"source":["import pickle\n","dataset = pickle.load(open(\"AIML_DS_NEWSGROUPS_PICKELFILE.pkl\",'rb'))\n","print(dataset.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJPdVgh1-EK1"},"source":["### Dividing the dataset into train and test\n","\n","We will use 950 samples from each class in the training set, and the remaining 50 (Are you sure?)  in the test set. "]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"b7Q4P0BB-EK3"},"source":["train_set = {}\n","test_set = {}\n","\n","# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n","for key in dataset:\n","    dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n","    \n","# Break dataset into 95-5 split for training and testing\n","n_train = 0\n","n_test = 0\n","for k in dataset:\n","    split = int(0.95*len(dataset[k]))\n","    train_set[k] = dataset[k][0:split]\n","    test_set[k] = dataset[k][split:-1]\n","    n_train += len(train_set[k])\n","    n_test += len(test_set[k])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gg4kcLVI-ELA"},"source":["### Calculating the frequency of the words\n","\n","As you might have realized, machine learning algorithms need good feature representations of different inputs. Concretely, we would like to represent each news article ùê∑ in terms of a feature vector ùëâ, which can be used for classification. Feature vector ùëâ is made up of the number of occurrences of each word in the vocabulary. Let us count the number of occurrences of every word in the news documents in the training set."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"3tmv8TOH-ELB"},"source":["import collections\n","import re\n","import operator\n","frequency = collections.defaultdict(int)\n","    \n","for key in train_set:\n","    for f in train_set[key]:\n","        \n","        # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n","        # We ignore all special characters such as !.$ and words containing numbers\n","        words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n","    \n","        for word in words:\n","            frequency[word] += 1\n","\n","sorted_words = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iIQA8ipd-ELF"},"source":["### Preprocessing the data\n","\n","Now we got some rough ideas about kind of words that appear frequently, and those that occur rarely, and we can observe that different words appear with different frequencies. The most common words appear in almost all documents. Hence, for a classification task, having information about those words' frequencies does not matter much since they appear frequently in every type of document. To get a good feature representation, we eliminate them since they do not add too much value. Additionally, notice how the least frequent words appear so rarely that they might not be useful either.\n","\n","Let us pre-process our news articles now to remove the most frequent and least frequent words by thresholding their counts:"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"zPFI5rVV-ELG"},"source":["valid_words = collections.defaultdict(int)\n","\n","print('Number of words before preprocessing:', len(sorted_words))\n","\n","# Ignore the 25 most frequent words, and the words which appear less than 100 times\n","ignore_most_frequent = 25\n","freq_thresh = 100\n","feature_number = 0\n","for word, word_frequency in sorted_words[ignore_most_frequent:]:\n","    if word_frequency > freq_thresh:\n","        valid_words[word] = feature_number\n","        feature_number += 1\n","        \n","print('Number of words after preprocessing:', len(valid_words))\n","\n","word_vector_size = len(valid_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z5C924mf-EK6"},"source":["### Loading the predefined word2vec file"]},{"cell_type":"markdown","metadata":{"id":"khZP5uKCP53X"},"source":["we aim to predict the next word given the context in which the word appears. (For example, given the last ùëõ words, predict the next word). A very smart way to do this is by using a feature representation called \"Word2Vec\" with transforms each word into 300-dimensional vectors.\n","Link to pretrained 300 dimensional word2vec: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download\n","\n","\n","Converting each document into average of the word2vec vectors of all valid words in document.\n"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"q-Brrxn8-EK7"},"source":["model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"werGndNl-ELM"},"source":["### Function for word2vec"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"Tu5vwIaP-ELN"},"source":["word2vec_vector_size = 300\n","\n","'''\n"," This method converts documents to word vectors. It first checks if the word is valid according to our initial frequency \n"," threshold. Next, if it is, we add the precomputed word vectors together. If the word is valid, but we do not have a valid \n"," vector to represent the word, we add a random gaussian noise instead. Since we do not want to induce new noise each time,\n"," we store the same noise vector for training and test time in substitute_word_vecs variable.\n","'''\n","def convert_to_w2v(dataset, number_of_documents, substitute_word_vecs={}):\n","    d = {}\n","    labels = np.zeros((number_of_documents, 1))\n","    w2v_rep = np.zeros((number_of_documents, word2vec_vector_size))\n","    \n","    # Iterate over the dataset and split into words\n","    i = 0\n","    for label, class_name in enumerate(dataset):\n","        for f in dataset[class_name]:\n","            text = ' '.join(f).split(' ')\n","            valid_count = 1\n","            for word in text:\n","                \n","                # Check if word is valid or not according to original dataset pruning\n","                if word in valid_words:\n","                    try:\n","                        w2v_rep[i] += model[word]\n","                        d[word] = model[word]\n","                    except:\n","                        '''The word isn't in our pretrained word-vectors, hence we add a random gaussian noise\n","                         to account for this. We store the random vector we assigned to the word, and reuse \n","                         the same vector during test time to ensure consistency.'''\n","                        \n","                        if word not in substitute_word_vecs.keys():\n","                            substitute_word_vecs[word] = np.random.normal(-0.25, 0.25, word2vec_vector_size)\n","                            \n","                        w2v_rep[i] += substitute_word_vecs[word]\n","                    \n","                    valid_count += 1\n","            \n","            # Average\n","            w2v_rep[i] = w2v_rep[i] / valid_count\n","            \n","            # Save label\n","            labels[i] = label\n","            \n","            i += 1\n","\n","    return d, w2v_rep, labels, substitute_word_vecs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sHA2QArx-ELR"},"source":["### Convert the train and test datasets into their word2vec representations"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"OfC2CyID-ELT"},"source":["d_train,train_w2v_set, train_w2v_labels, substitute_word_vecs = convert_to_w2v(train_set, n_train)\n","d_test, test_w2v_set, test_w2v_labels,_ = convert_to_w2v(test_set, n_test, substitute_word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"1dkaSOlwNb8v"},"source":["len(d_train),train_w2v_set.shape, train_w2v_labels.shape, len(substitute_word_vecs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VtZVJgg1-ELW"},"source":["### Defining the Model\n","\n","\n","Let us create an autoencoder architecture. Typically an autoencoder is made up of two components, the encoder network and the decoder network. Encoding network is made up of four layers where we are trying to compressing the data into 2 features which can be easily visualized in the plot.  Decoding network is made up of four layers.\n","\n","The sentences in the documents as word2vec representation are fed into the encoder network, transformed into a semantic vector of fixed size, and then unfolded back into an ordered sentences in document. \n"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"ocL1vHQ_-ELX"},"source":["class autoencoder(nn.Module):\n","    def __init__(self):\n","        super(autoencoder, self).__init__()\n","        self.encoder = nn.Sequential(\n","            nn.Linear(300,100),\n","            nn.ReLU(),\n","            nn.Linear(100,30),\n","            nn.ReLU(),\n","            nn.Linear(30,30),\n","            nn.ReLU(),\n","            nn.Linear(30,2))\n","        self.decoder = nn.Sequential(\n","            nn.Linear(2,30),\n","            nn.ReLU(),\n","            nn.Linear(30,30),\n","            nn.ReLU(),\n","            nn.Linear(30,100),\n","            nn.ReLU(),\n","            nn.Linear(100,300))\n","\n","    def forward(self, x):\n","        y = self.encoder(x)\n","        z = self.decoder(y)\n","        return y,z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kcq8SykTWhEf"},"source":["### Calling the instances of the network"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"k218aqN3WY6P"},"source":["model = autoencoder()\n","model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YqX7jet1WqhC"},"source":["### Defining the loss function and optimizer"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"y-heUVtSWsJk"},"source":["criterion = nn.MSELoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45QZz43PW6Xn"},"source":["### Training the Model\n","\n","1. Pass the data through the model.\n","2. Calculate the loss by comparing the reconstructed image with the original image\n","3. Train the model to minimize the reconstruction error which measures the diference between the original image and the consequent reconstructed image."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"9uaAhSCcjXoP"},"source":["EPOCH = 10\n","for epoch in range(EPOCH):\n","    for i, (word, w2v) in enumerate(d_train.items()):\n","        b_x = torch.FloatTensor(np.array(w2v))     # Converting an array to a tensor             \n","    \n","        # Passing the data to the model (Forward Pass)\n","        encoded, decoded = model(b_x)\n","\n","        # Calculating mean square error loss between the reconstructed image and the original image\n","        loss = criterion(decoded, b_x)  \n","\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()   \n","        \n","        # Performing backward pass (Backpropagation)             \n","        loss.backward()                     \n","\n","        # optimizer.step() updates the weights accordingly \n","        optimizer.step()   \n","\n","    print(\"Loss:\",float(loss))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FhsGPDnk-ELc"},"source":["We will convert training dataset first into a torch tensor, and form a differentiable Variable."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"7NSJxx_m-ELd"},"source":["# Converting an array to a tensor \n","value = torch.Tensor(np.array(train_w2v_set))  \n","\n","# Passing the data to the model (Forward Pass)\n","model.eval()\n","encoder_values, decoder_values = model(value)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"sU0c6r0X-ELj"},"source":["# Converting pytorch variable into numpy array\n","encode = encoder_values.data.numpy()\n","decode = decoder_values.data.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"u4XknZRDO4g_"},"source":["print(encode.shape, decode.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"h0_BD-Vx-ELm"},"source":["# Storing all the words in words list\n","words = []\n","for key in substitute_word_vecs:\n","    words.append(key)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DxcB_RsAJiFd"},"source":["### Visualization of encoded data\n","\n","To visualize the higher dimensional data in 2D space, we can use autoencoders."]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"KstrrMo6CxZp"},"source":["plt.figure(figsize=(10,6))\n","plt.scatter(encode[:50,0],encode[:50,1])\n","for i in range(len(words[:50]) - 1):\n","    plt.annotate(words[i], xy = (encode[i][0],encode[i][1]))\n","plt.show()"],"execution_count":null,"outputs":[]}]}