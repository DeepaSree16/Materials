{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-13-AS-1-5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fKYsgPAIs26K"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"9-ScFyDQs9Py"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"xjaxZyKLs_yr"},"source":["At the end of the experiment you will be able to :\n","\n","* understand Fashion-MNIST dataset\n","* classify Fashion-MNIST data using neural networks\n"]},{"cell_type":"code","metadata":{"id":"ez6veGEcbT8g","cellView":"form"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_batch_15/preview_videos/Fashion_MNIST.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3mE8sH-otYGH"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"-s7GpYrkCaSB"},"source":["### History"]},{"cell_type":"markdown","metadata":{"id":"9-sRjaldCcCI"},"source":["The original MNIST dataset contains handwritten digits. People from AI/ML or Data Science community love this dataset. They use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset they would try on. As per popular belief, If the algorithm doesn’t work on MNIST, it won’t work at all. Well, if algorithm works on MNIST, it may still fail on other datasets.\n","\n","\n","As per the original [paper](https://arxiv.org/abs/1708.07747) describing about Fashion-MNIST, It is a dataset recomposed from the product pictures of Zalando’s websites. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits.\n","\n","There are some good reasons for the challenges faced by MNIST dataset:\n","\n","* MNIST is too easy - Neural networks can achieve 99.7% on MNIST easily, and similarly, even classic ML algorithms can achieve 97%. \n","\n","* MNIST is overused - Almost everyone who has experience with deep learning has come across MNIST at least once.\n","\n","* MNIST cannot represent modern CV task\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ijqHaydutZ5K"},"source":["### Description"]},{"cell_type":"markdown","metadata":{"id":"6ANHcoEyg_gp"},"source":["The dataset choosen for this experiment is Fashion-MNIST. The dataset is made up of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. \n","\n","Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255.\n","\n","**Labels / Classes**\n","\n","0 - T-shirt/top\n","\n","1 - Trouser\n","\n","2 - Pullover\n","\n","3 - Dress\n","\n","4 - Coat\n","\n","5 - Sandal\n","\n","6 - Shirt\n","\n","7 - Sneaker\n","\n","8 - Bag\n","\n","9 - Ankle boot"]},{"cell_type":"markdown","metadata":{"id":"QB0bQEYVZoFV"},"source":["### Importing the Required Packages"]},{"cell_type":"markdown","metadata":{"id":"no90mbVqbGJb"},"source":["* First, we import pytorch, the deep learning library which we’ll be using, and torchvision, which provides our dataset and data transformations. \n","\n","* We also import torch.nn (pytorch’s neural network library), torch.nn.functional (includes non-linear functions like ReLu and sigmoid) and torch.optim for implementing various optimization algorithms."]},{"cell_type":"code","metadata":{"id":"xus61CSGmqxs"},"source":["# Importing torch packages\n","import torch.nn as nn\n","import torch.nn.functional as F \n","import torch.optim as optim\n","\n","import torch\n","import torchvision\n","from torchvision import datasets, transforms\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbm-8HxIqvxR"},"source":["### Initializing CUDA\n","\n","CUDA is used as an interface between our code and the GPU.\n","\n","Normally, we run the code in the CPU. To run it in the GPU, we need CUDA. Check if CUDA is available:"]},{"cell_type":"code","metadata":{"id":"YHj_ZREiqvxU"},"source":["# To test whether GPU instance is present in the system of not.\n","use_cuda = torch.cuda.is_available()\n","print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DuGyWSz8q4NQ"},"source":["If it's False, then we run the program on CPU. If it's True, then we run the program on GPU.\n","\n","Let us initialize some GPU-related variables:"]},{"cell_type":"code","metadata":{"id":"m_WeWksDqvxb"},"source":["device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","device"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YkBWGgw7cUFP"},"source":["### Load Fashion MNIST data\n","\n","Now, we'll load the Fashion MNIST data. For the first time, we may have to download the data, which can take a while.\n","\n","Now, \n","\n","* We will load both the training set and the testing sets \n","\n","* We will use  transform.compose() to convert the datasets into tensors using transforms.ToTensor(). We also normalize them by setting the mean and standard deviation using transforms.Normalize().\n","\n"]},{"cell_type":"code","metadata":{"id":"2JxUAwmdlswv"},"source":["# Normalize the data with mean and std (0.2860 and 0.3530 are the mean and std for Fashion MNIST data)\n","transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.2860,), (0.3530,))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Jl5wAeelu6M"},"source":["# Downloading the Training set\n","trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', train=True, transform=transform, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWsShsOO1sDU"},"source":["# Verifying mean and std of Fashion MNIST data\n","trainset.data.float().mean() / 255, trainset.data.float().std() / 255"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wO6aHjhGlyYG"},"source":["# Loading the downloaded training set\n","trainloader = torch.utils.data.DataLoader(trainset, shuffle=True, batch_size=64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3d4K58jMl0uN"},"source":["# Downloading the testing set\n","testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', train= False, transform=transform, download=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxI_WbvUl2ge"},"source":["# Loading the downloaded testing set\n","testloader = torch.utils.data.DataLoader(testset, shuffle=False, batch_size=64)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DUTWWsGtnC2h"},"source":["The train and test data are provided via data loaders that provide iterators over the datasets.\n","\n","The first element of training data (X_train) is a 4th-order tensor of size (batch_size, 1, 28, 28), i.e. it consists of a batch of images of size 1x28x28 pixels. y_train is a vector containing the correct classes (\"0\", \"1\", ..., \"9\") for each training image. Here batch size is 64.\n"]},{"cell_type":"code","metadata":{"id":"XajA9PKwmM3W"},"source":["for (X_train, y_train) in trainloader:\n","    print('X_train:', X_train.size(), 'type:', X_train.type())\n","    print('y_train:', y_train.size(), 'type:', y_train.type())\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXXtfQXgmEO7"},"source":["### Plotting the images of 9 classes"]},{"cell_type":"code","metadata":{"id":"lrEYzDz3l4QG"},"source":["labels =[]\n","features = []\n","for X,y in zip(X_train, y_train):\n","  # Getting unique labels  \n","  if y not in labels:\n","    labels.append(y)\n","    features.append(X)\n","\n","labels_map = {0 : 'T-Shirt', 1 : 'Trouser', 2 : 'Pullover', 3 : 'Dress', 4 : 'Coat', 5 : 'Sandal', 6 : 'Shirt',\n","              7 : 'Sneaker', 8 : 'Bag', 9 : 'Ankle Boot'};\n","pltsize=1  \n","plt.figure(figsize=(7,7))\n","for i in range(9):\n","    plt.subplot(3,3, i+1)\n","    plt.axis('off')\n","    # Convert the tensor to numpy for displaying the image\n","    plt.imshow(features[i].numpy().reshape(28,28), cmap=\"gray\")\n","    plt.title('Class: '+labels_map[int(str(labels[i].numpy()))])\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pEYZoawOppFN"},"source":["### Defining the CNN’s Architecture"]},{"cell_type":"markdown","metadata":{"id":"TEhqiDSmfE3C"},"source":["Neural Networks are inherited from the nn.Module class.\n","\n","Now let us define a neural network. Here we are using two functions \\__init__ and forward function.\n","\n","In the \\__init__  function, we define the layers using the provided modules from the nn package. The forward function is called on the Neural Network for a set of inputs, and it passes that input through the different layers that have been defined. \n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"w6MgNN46B3sE"},"source":["**Convolutional Layer:**  Convolutional layers are the layers where filters are applied to the original image. The most common type of convolution used is the 2D convolution layer and is abbreviated as **conv2d**. \n","\n","The **kernel** is a filter used to extract the features from the images. The kernel is a matrix that moves over the input data, performs the dot product with the sub-region of input data, and gets the output as the matrix of dot products. \n","\n","Calculate the output size of Convolution Layer, using below formula:\n","\n","  $O = \\frac{W- K + 2P}{S} +1$\n","\n","*   O : output height/length\n","*   W : input height/length\n","*   K : filter size (kernel size)\n","*   P : padding \n","*   S : stride\n","\n","The **Pooling layer** is used to reduce the spatial volume of input image after convolution. It is used between the convolution layers.\n","\n","Output formula for Pooling, $O = \\frac{W- K}{S} +1$   \n","\n","If using PyTorch default stride (default stride is same as kernel size), then output formula for pooling will result,  $O = \\frac{W}{K}$\n","\n","\n","The **Fully connected layers** involves weights, biases, and neurons. It connects neurons in one layer to neurons in another layer. It is used to classify images between different category by training."]},{"cell_type":"markdown","metadata":{"id":"rqycwcVSvhD0"},"source":["Output after Convolutional layer 1 = $\\frac{28-3+ 2(0)}{1}+1 = 26$ ----> Multiply it with the output channels, the output shape becomes (8, 26, 26)\n","\n","Output after appyling Maxpool layer on Convolutional layer 1 = $\\frac{26}{2} = 13$ -----> The output shape changes from (8, 26, 26) to (8, 13, 13)\n","\n","Output after Convolutional layer 2 = $\\frac{13-3+ 2(0)}{1}+1 = 11$ Multiply it with the output channels, the output shape becomes (16, 11, 11)\n","\n","Output after appyling Maxpool layer on Convolutional layer 2 = $\\frac{11}{2} = 5$ -----> The output shape changes from (16, 11, 11) to (16, 5, 5)\n","\n","The above output (16, 5, 5) is then passed as input to the first fully connected layer"]},{"cell_type":"code","metadata":{"id":"4wxqnZknpclo"},"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","        \n","        # Defining first convolution layer with input_channels = 1, output_channels = 8, kernel_size = 3\n","        self.conv1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3)\n","\n","        # Defining second convolution layer with input_channels = 8, output_channels = 16, kernel_size = 3               \n","        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3)\n","        \n","        # Define the Fully connected layers\n","        # The output of the second convolution layer will be input to the first fully connected layer\n","        self.fc1 = nn.Linear(16*5*5, 256)\n","        # 256 input features, 128 output features \n","        self.fc2 = nn.Linear(256, 128)\n","        # 128 input features, 64 output features \n","        self.fc3 = nn.Linear(128, 64)\n","        # 64 input features, 10 output features for our 10 defined classes\n","        self.fc4 = nn.Linear(64, 10)\n","\n","        # Max pooling\n","        self.pool = nn.MaxPool2d(kernel_size=2)  # Max pooling layer with filter size 2x2\n","   \n","    def forward(self, x):\n","\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","\n","        # Flatten the image\n","        x = x.view(-1, 16*5*5)  # Output shape of convolutional layer is 16*5*5 \n","        \n","        # Linear layers with RELU activation\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = F.relu(self.fc3(x))\n","        x = self.fc4(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LlTLMs5Q-6Uh"},"source":["#### Calling the instances of the network"]},{"cell_type":"code","metadata":{"id":"-5DecLBbmnYC"},"source":["model = Model()\n","model = model.to(device)\n","model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GgJQ6bNG-Mx0"},"source":["#### Defining the loss function and optimizer"]},{"cell_type":"code","metadata":{"id":"QjUUgDMQmw7Y"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jmNz24u_n5fw"},"source":["#### Training and Evaluating the model"]},{"cell_type":"markdown","metadata":{"id":"rWcqXz9PAM9B"},"source":["In Training Phase, we iterate over a batch of images in the train_loader. For each batch, we perform  the following steps:\n","\n","* First we zero out the gradients using zero_grad()\n","\n","* We pass the data to the model i.e. we perform forward pass by calling the forward()\n","\n","* We calculate the loss using the actual and predicted labels\n","\n","* Perform Backward pass using backward() to update the weights"]},{"cell_type":"code","metadata":{"id":"kG_e4hjdrgs7"},"source":["# No of Epochs\n","epoch = 10\n","\n","# keeping the network in train mode\n","model.train()\n","train_losses,  train_accuracy = [], []\n","\n","# Loop for no of epochs\n","for e in range(epoch):\n","    train_loss = 0\n","    correct = 0\n","    # Iterate through all the batches in each epoch\n","    for images, labels in trainloader:\n","\n","      # Convert the image and label to gpu for faster execution\n","      images = images.to(device)\n","      labels = labels.to(device)\n","\n","      # Zero the parameter gradients\n","      optimizer.zero_grad()\n","\n","      # Passing the data to the model (Forward Pass)\n","      outputs = model(images)\n","\n","      # Calculating the loss\n","      loss = criterion(outputs, labels)\n","      train_loss += loss.item()\n","\n","      # Performing backward pass (Backpropagation)\n","      loss.backward()\n","\n","      # optimizer.step() updates the weights accordingly\n","      optimizer.step()\n","\n","      _, predicted = torch.max(outputs, 1)\n","      correct += (predicted == labels).sum().item()\n","      \n","    # Accuracy calculation\n","    train_losses.append(train_loss/len(trainset))\n","    train_accuracy.append(100 * correct/len(trainset))\n","    print('epoch: {}, Train Loss:{:.6f} Train Accuracy: {:.2f} '.format(e+1,train_losses[-1], train_accuracy[-1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tdg2JUD1BxOC"},"source":["In Testing Phase, we iterate over a batch of images in the test_loader. For each batch we perform the following steps:\n","\n","* We pass the images through the model (network) to get the outputs\n","* Pick the class / label with the highest probability\n","* Calculate the accuracy"]},{"cell_type":"code","metadata":{"id":"YEZPsRndr1i9"},"source":["# Keeping the network in evaluation mode \n","model.eval()  \n","\n","Test_accuracy = 0\n","\n","# Iterate through all the batches in each epoch\n","for images,labels in testloader:\n","    \n","    # Convert the images and labels to gpu for faster execution\n","    images = images.to(device)\n","    labels = labels.to(device)\n","\n","    # Do the forward pass \n","    outputs = model(images)\n","\n","    # Accuracy calculation\n","    _, predicted = torch.max(outputs, 1)\n","    Test_accuracy += (predicted == labels).sum().item()\n","\n","Accuracy = 100 * Test_accuracy / len(testset)\n","print(\"Accuracy of Test Data is\", Accuracy)"],"execution_count":null,"outputs":[]}]}