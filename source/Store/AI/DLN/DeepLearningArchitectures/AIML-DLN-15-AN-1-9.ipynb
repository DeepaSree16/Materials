{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-15-AN-3-4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YUAYH8SHdlSI"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z1B-hn6Ids0P"},"source":["### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"hSCn4S8xen2g"},"source":["## NLP with CNNs"]},{"cell_type":"code","metadata":{"id":"01RclMCIerry","cellView":"form"},"source":["#@title Case Study Walkthrough\n","#@markdown  NLP with CNNs\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"320\" height=\"240\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/nlp_with_cnns.mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dCzV2botd8sG"},"source":["The objective of this experiment is to see the application of Convolutional Neural Networks in NLP.\n","\n","####Note that this case study based on this [paper.](http://www.aclweb.org/anthology/D14-1181)"]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week9/Exp2/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin&sa=D&source=hangouts&ust=1550651743825000&usg=AFQjCNHh2LSwNi9czsqAAuBLvx_vDeUE_Q\n"],"metadata":{"id":"JJVNzX2Ac7yJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EqGNvFFo_oKx"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pv4srXyUfZtZ"},"source":["##Importing required packages"]},{"cell_type":"code","metadata":{"id":"bRmt8q7o1_Ve"},"source":["import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import nltk\n","import random\n","import numpy as np\n","from collections import Counter, OrderedDict\n","import nltk\n","import re\n","from copy import deepcopy\n","flatten = lambda l: [item for sublist in l for item in sublist]\n","random.seed(1024)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJHLi22fy4ts"},"source":["## Code for accessing CUDA"]},{"cell_type":"code","metadata":{"id":"t-XZypg51_Vq"},"source":["USE_CUDA = torch.cuda.is_available()\n","gpus = [0]\n","torch.cuda.set_device(gpus[0])\n","FloatTensor = torch.cuda.FloatTensor if USE_CUDA else torch.FloatTensor\n","LongTensor = torch.cuda.LongTensor if USE_CUDA else torch.LongTensor\n","ByteTensor = torch.cuda.ByteTensor if USE_CUDA else torch.ByteTensor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PNompkhM28vF"},"source":["## Function to split the data in to batches"]},{"cell_type":"code","metadata":{"id":"uxznew651_V4"},"source":["def getBatch(batch_size, train_data):\n","    random.shuffle(train_data)\n","    sindex = 0\n","    eindex = batch_size\n","    while eindex < len(train_data):\n","        batch = train_data[sindex: eindex]\n","        temp = eindex\n","        eindex = eindex + batch_size\n","        sindex = temp\n","        yield batch\n","    \n","    if eindex >= len(train_data):\n","        batch = train_data[sindex:]\n","        yield batch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kAdtU2GD3CA4"},"source":["## Function to add the padding to batches if required"]},{"cell_type":"code","metadata":{"id":"bOENlo3U1_V-"},"source":["def pad_to_batch(batch):\n","    x,y = zip(*batch)\n","    max_x = max([s.size(1) for s in x])\n","    x_p = []\n","    for i in range(len(batch)):\n","        if x[i].size(1) < max_x:\n","            x_p.append(torch.cat([x[i], Variable(LongTensor([word2index['<PAD>']] * (max_x - x[i].size(1)))).view(1, -1)], 1))\n","        else:\n","            x_p.append(x[i])\n","    return torch.cat(x_p), torch.cat(y).view(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t3HBhPtB3Lvc"},"source":["## Function to prepare the sequence"]},{"cell_type":"code","metadata":{"id":"jeQZxFP21_WD"},"source":["def prepare_sequence(seq, to_index):\n","    idxs = list(map(lambda w: to_index[w] if to_index.get(w) is not None else to_index[\"<UNK>\"], seq))\n","    #print(idxs)\n","    return Variable(LongTensor(idxs))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7Na5eA1e1_WK"},"source":["## Data load & Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"8KPUw7sh1_WN"},"source":["### TREC question dataset(http://cogcomp.org/Data/QA/QC/)"]},{"cell_type":"markdown","metadata":{"id":"Siu3p2ujzFQu"},"source":["The following command gets the required TREC question dataset."]},{"cell_type":"code","metadata":{"id":"w-9ylZtd38xo"},"source":["!wget http://cogcomp.org/Data/QA/QC/train_5500.label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lYGRuNDx1_WP"},"source":["Task involves\n","classifying a question into 6 question\n","types (whether the question is about person,\n","location, numeric information, etc.)"]},{"cell_type":"markdown","metadata":{"id":"6eqzI8qfqPYY"},"source":["## Load the data"]},{"cell_type":"code","metadata":{"id":"VZaRXuDq1_WR"},"source":["data = open('train_5500.label', 'r', encoding='latin-1').readlines()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"p0T9ReECPRx6"},"source":["data[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rnzM407MqIS0"},"source":["## Split the data by seperating the labels"]},{"cell_type":"code","metadata":{"id":"r4iaes_K1_WV"},"source":["data = [[d.split(':')[1][:-1], d.split(':')[0]] for d in data]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHfgzSu6YCnd"},"source":["data[:5]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CyfLz0g51_Wa"},"source":["X, y = list(zip(*data))\n","X = list(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PtNVffazByxZ"},"source":["print(X[:5])\n","print(y[:5])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"i9s_QpyZqiQJ"},"source":["## Print the labels in the data"]},{"cell_type":"code","metadata":{"id":"3Lpu65lLHPgs"},"source":["set(y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2KG6M3P1_Wf"},"source":["## Number masking "]},{"cell_type":"code","metadata":{"id":"knwW1fJN1_Wi"},"source":["for i, x in enumerate(X):\n","    X[i] = re.sub('\\d', '#', x).split()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bTnKxx0s1_Wh"},"source":["Replacing the numbers with # (hash)\n","\n","It reduces the search space. \n","\n","For example, \n","\n","my birthday is 12.22 ==> my birthday is ##.##"]},{"cell_type":"code","metadata":{"id":"87lvnCRXwa40"},"source":["X[:2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9T0Hs5p1_Wk"},"source":["## Building the Vocabulary"]},{"cell_type":"code","metadata":{"id":"9MggQLTr1_Wl"},"source":["vocab = list(set(flatten(X)))\n","print(len(vocab))\n","print(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9k_kLbgJsFb5"},"source":["## Check for number of classes"]},{"cell_type":"code","metadata":{"id":"ojTOyUZI1_Wu"},"source":["len(set(y)) # num of class"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H9vB6LwPGBuR"},"source":["## Create the index to words in the vocabulary"]},{"cell_type":"code","metadata":{"id":"SpkCJnOws7Qd"},"source":["word2index={'<PAD>': 0, '<UNK>': 1}\n","print(len(word2index))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6cqECh-tEGP"},"source":["print(word2index.get('<PAD>'))\n","print(word2index.get('<UNK>'))\n","print(word2index.get(vocab[1]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tIYNkrPyti78"},"source":["for vo in vocab:\n","    if word2index.get(vo) is None:\n","        word2index[vo] = len(word2index)\n","#print(word2index)\n","index2word = {v:k for k, v in word2index.items()}\n","#print(index2word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcAWpe8IuhX0"},"source":["## Create the index to target"]},{"cell_type":"code","metadata":{"id":"xQYIks631_Wz"},"source":["target2index = {}\n","\n","for cl in set(y):\n","    if target2index.get(cl) is None:\n","        target2index[cl] = len(target2index)\n","\n","index2target = {v:k for k, v in target2index.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXAeK_83u_nl"},"source":["## Preparing the data in tensor format"]},{"cell_type":"code","metadata":{"id":"GTtK3Dkp1_W3"},"source":["X_p, y_p = [], []\n","for pair in zip(X,y):\n","    ## Create the indexes for the list of split words of questions present in X and changing to tensor format\n","    X_p.append(prepare_sequence(pair[0], word2index).view(1, -1)) \n","    ## Changes the format of labels to tensor format\n","    y_p.append(Variable(LongTensor([target2index[pair[1]]])).view(1, -1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KphcA1HoxjC4"},"source":["## Zipping both the data and labels and shuffle randomly"]},{"cell_type":"code","metadata":{"id":"iZ5nV535vMwj"},"source":["data_p = list(zip(X_p, y_p))\n","random.shuffle(data_p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhuzT6-gxp7H"},"source":["## Split the data into train and test"]},{"cell_type":"code","metadata":{"id":"ArtPreOfvO6g"},"source":["train_data = data_p[: int(len(data_p) * 0.9)]\n","test_data = data_p[int(len(data_p) * 0.9):]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skNvclR81_W7"},"source":["## Load Pretrained word vector"]},{"cell_type":"code","metadata":{"id":"J13H4xuG1_W-"},"source":["import gensim\n","model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True)\n","len(model.index2word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tUIhdz3jbqm2"},"source":["model.index2word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2nU__b36zJET"},"source":["word2index.keys()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EO1MaKXxzcio"},"source":["print(model['pail'].shape)\n","print(np.random.randn(300).shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rbSaefTF0Ym6"},"source":["## Get the vector corresponding to the word using the pretrained model"]},{"cell_type":"code","metadata":{"id":"rhGjID2V1_XK"},"source":["pretrained = []\n","\n","for index, key in enumerate(word2index.keys()):\n","    try:\n","        pretrained.append(model[key])\n","    except:\n","        #print(index, key)\n","        pretrained.append(np.random.randn(300))\n","        \n","pretrained_vectors = np.vstack(pretrained)\n","#print(pretrained)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0oxRZOC41_XN"},"source":["## Modeling "]},{"cell_type":"markdown","metadata":{"id":"EM0TdEnY1_XP"},"source":["\n","![alttxt](https://cdn.talentsprint.com/aiml/Casestudies_slides/NLP_with_CNN/NLP_with_CNN.png)\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FP8GUFAXyTZ1"},"source":["The above image is borrowed from this [paper.](http://www.aclweb.org/anthology/D14-1181)"]},{"cell_type":"markdown","metadata":{"id":"qPM1HfGg0uM6"},"source":["## Define CNN classifier architecture for classification as per the paper "]},{"cell_type":"code","metadata":{"id":"VU_ZOdDP1_XQ"},"source":["class  CNNClassifier(nn.Module):\n","    \n","    def __init__(self, vocab_size, embedding_dim, output_size, kernel_dim=100, kernel_sizes=(3, 4, 5), dropout=0.5):\n","        super(CNNClassifier,self).__init__()\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_dim, (K, embedding_dim)) for K in kernel_sizes])\n","\n","        # kernal_size = (K,D) \n","        self.dropout = nn.Dropout(dropout)\n","        self.fc = nn.Linear(len(kernel_sizes) * kernel_dim, output_size)\n","    \n","    \n","    def init_weights(self, pretrained_word_vectors, is_static=False):\n","        self.embedding.weight = nn.Parameter(torch.from_numpy(pretrained_word_vectors).float())\n","        if is_static:\n","            self.embedding.weight.requires_grad = False\n","\n","\n","    def forward(self, inputs, is_training=False):\n","        inputs = self.embedding(inputs).unsqueeze(1)\n","        inputs = [F.relu(conv(inputs)).squeeze(3) for conv in self.convs]\n","        inputs = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in inputs]\n","\n","        concated = torch.cat(inputs, 1)\n","\n","        if is_training:\n","            concated = self.dropout(concated)\n","        out = self.fc(concated) \n","        return F.log_softmax(out,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xSKrrNjr1_XT"},"source":["## Training the model "]},{"cell_type":"markdown","metadata":{"id":"Kl7VHDR41_XV"},"source":["##It takes for a while if you use just cpu."]},{"cell_type":"markdown","metadata":{"id":"8YG4suzi1Rfq"},"source":["## Set the parameters"]},{"cell_type":"code","metadata":{"id":"zy1Eum7_1_XZ"},"source":["EPOCH = 5\n","BATCH_SIZE = 50\n","KERNEL_SIZES = [2,2,2]\n","KERNEL_DIM = 100\n","LR = 0.001"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jWkOH8n61Xip"},"source":["## Set up the defined CNN model and  Initialize embedding matrix using pretrained vectors"]},{"cell_type":"code","metadata":{"id":"cJe4E11Q1_Xe"},"source":["model = CNNClassifier(len(word2index), 300, len(target2index), KERNEL_DIM, KERNEL_SIZES)\n","model.init_weights(pretrained_vectors) # initialize embedding matrix using pretrained vectors"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jRpmZbub1nWD"},"source":["## Switch on the CUDA"]},{"cell_type":"code","metadata":{"id":"NqmJozfD1eLj"},"source":["if USE_CUDA:\n","    model = model.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6m9goBXf1u_-"},"source":["## Define loss function and optimizer"]},{"cell_type":"code","metadata":{"id":"CEgXhSv_1g04"},"source":["loss_function = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=LR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3qjJyDZ12RV"},"source":["## Train the data batch wise"]},{"cell_type":"code","metadata":{"id":"t6PGTf7g1_Xg"},"source":["for epoch in range(EPOCH):\n","    losses = []\n","    for i,batch in enumerate(getBatch(BATCH_SIZE, train_data)):\n","        inputs,targets = pad_to_batch(batch)\n","        \n","        model.zero_grad()\n","        preds = model(inputs, True)\n","        \n","        loss = loss_function(preds, targets)\n","        losses.append(loss.data.item())\n","        loss.backward()\n","        \n","        #for param in model.parameters():\n","        #    param.grad.data.clamp_(-3, 3)\n","        \n","        optimizer.step()\n","        \n","        if i % 100 == 0:\n","            print(\"[%d/%d] mean_loss : %0.2f\" %(epoch, EPOCH, np.mean(losses)))\n","            losses = []"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"InMk4BTN1_Xk"},"source":["## Predict the test data with the trained model and calculate the test accuracy "]},{"cell_type":"code","metadata":{"id":"RmbcvYpz1_Xn"},"source":["accuracy = 0\n","for test in test_data:\n","    pred = model(test[0].cuda()).max(1)[1]\n","    pred = pred.data.tolist()[0]\n","    target = test[1].data.tolist()[0][0]\n","    if pred == target:\n","        accuracy += 1\n","\n","print(accuracy/len(test_data) * 100)"],"execution_count":null,"outputs":[]}]}