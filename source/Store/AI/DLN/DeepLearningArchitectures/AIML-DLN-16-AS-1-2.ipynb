{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-16-AS-1-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"t9k5heqzZ6CP"},"source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n"]},{"cell_type":"markdown","metadata":{"id":"f-ObRs5nPoAR"},"source":["## Learning Objectives "]},{"cell_type":"markdown","metadata":{"id":"sqK4U4dRP1H7"},"source":["At the end of the experiment, you will be able to:\n","\n","1.   Generate text which is similar to the writing style of William Shakespeare\n","2.   Understand how to adapt or tune the trained network"]},{"cell_type":"code","metadata":{"id":"EHmt3YzitsW3","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"status":"ok","timestamp":1631004294013,"user_tz":-330,"elapsed":669,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"aa68027b-b8ca-4e78-defe-50bab4f25454"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"800\" height=\"300\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/module_3_week_12_experiment_1.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"800\" height=\"300\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/module_3_week_12_experiment_1.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"Abu-cNx8imcT"},"source":["## Dataset\n"]},{"cell_type":"markdown","metadata":{"id":"3rXIJFmjoBdi"},"source":["###  Description\n","\n","The dataset used in this experiment has partial content of different plays of Shakespeare concatenated into a single plain text file. \n","\n","Shakespeare is a famous English poet , play writer and actor. He is regarded as the greatest writer in the English language and the world's greatest dramatist. He is often called a England's national poet and the Bard of Avon. \n","\n","We have chosen plays of Shakespeare as our dataset mainly for two reasons : \n","\n","1. His work is widely recognized as standard for poetry and language.\n","2. The result of combining of his work provides a sizeable corpus for our model to learn.\n","\n","The plays of Shakespeare are taken from the following url:\n","\n","www.opensourceshakespeare.org/views/plays/plays.php\n"]},{"cell_type":"markdown","metadata":{"id":"rEQTPQUWiRZi"},"source":["## Domain Information"]},{"cell_type":"markdown","metadata":{"id":"6rsRsHXeiivc"},"source":["Music and Art are considered creative in nature and creating them is assumed to be more difficult when compared to writing a book, article or text. But the reality is that creating music and art is less complicated because there are no strict rules like which direction should one paint in or when to pause between the notes. However,  while writing a text one must follow grammatical rules. Hence, writing/generating text task is more related to machine learning and artificial intelligence.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IE8bAdSkwUt3"},"source":["## AI/ML Technique \n","\n"]},{"cell_type":"markdown","metadata":{"id":"LOs7dwuIZ6CT"},"source":["The RNN algorithm is used to generate text which is similar to the writing style of Shakespeare. Let us try to understand the main idea behind using this algorithm.\n","\n","### RNN algorithm\n","\n","The main idea is to use sequential information. In a traditional neural network we assume that all inputs (and outputs) are independent of each other. But for many tasks that’s a bad idea. If you want to predict the next word in a sentence you better know which words came before it. This is possible through RNN. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being depended on the previous computations. \n","\n","*Example:*  Take an example string **“HELLO”**. The vocabulary of the example is made of four letters or characters H,E,L,O. Now, let us apply RNN algorithm on this.\n","\n","Give *'H'* as input to the trained RNN model, it would give us an output *'E'*. In the next stage, this output *'E'* is passed as the new input which would give us the new output *'L'*. As the cycle follows, this output *'L'* is the new input but then what do you think the new output should be, second *‘L’* or *‘O’*? This is the challenge in predicting the next letter or character which RNN can solve. RNN has its own memory which helps it to predict based on the previous characters in this case H, E and L. Hence the output would most probably be *'L'* and not *'O'*.  "]},{"cell_type":"markdown","metadata":{"id":"EAFJ2xOxmPAR"},"source":["![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/7.1.png)"]},{"cell_type":"markdown","metadata":{"id":"Wj_Bik_GtTml"},"source":["In this experiment we will follow below steps:\n","\n","1.   Preparing the data\n","2.   Building the model\n","3.   Defining helper functions\n","4.   Training the model\n","5.    Adapting or Fine-tuning for text generation\n","\n","\n"]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/week10/Exp1/shakespeare.txt\n"],"metadata":{"id":"5SC0tZEepKYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBrFphh3mCTk"},"source":["### Importing required packages\n"]},{"cell_type":"code","metadata":{"id":"11e72PpPZ6Ca"},"source":["import unidecode\n","import string\n","import random\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","\n","import time, math\n","\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T3HMm0p-Z6CX"},"source":["### 1. Preparing the Data\n","\n","The file here is a plain text file. By using the [unidecode](https://pypi.org/project/Unidecode/) package turn any potential unicode characters into plain ASCII"]},{"cell_type":"code","metadata":{"id":"sE5njLNJZ6Cg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631004312159,"user_tz":-330,"elapsed":42,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"d115849d-b1b4-4e19-f1a3-dabde8a65ee3"},"source":["# Code to extract all sets of punctuation, digits, ascii_letters and whitespace characters\n","all_characters = string.printable\n","\n","# Code to find length of all_characters and storing the value in n_characters\n","n_characters = len(all_characters)\n","\n","# Use unidecode to convert unicode characters into plain ASCII\n","file = unidecode.unidecode(open('shakespeare.txt').read())\n","\n","# Code to find length of the file\n","file_len = len(file)\n","\n","# Printing the length of the file\n","print('file_len =', file_len)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["file_len = 1115393\n"]}]},{"cell_type":"markdown","metadata":{"id":"rlqTfzzxwvaU"},"source":["The variable 'file' is a string with 1115393 characters. This is the raw content of the Shakespeare text file (dataset file), including many details like white spaces, line breaks etc. \n","\n","Now to get the sense of the data we print first 1000 characters in the string:"]},{"cell_type":"code","metadata":{"id":"gM3NTD74Z6Cp","colab":{"base_uri":"https://localhost:8080/","height":154},"executionInfo":{"status":"ok","timestamp":1631004312163,"user_tz":-330,"elapsed":35,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"cfdb4018-dd06-4bca-e693-f565a7f7670a"},"source":["file[:1000]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"1BfQ1VWCZ6Cw"},"source":["As the string is large, split it into chunks to provide inputs to the RNN using the function random_chunk()"]},{"cell_type":"code","metadata":{"id":"1TffxtFCZ6Cz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631004312866,"user_tz":-330,"elapsed":730,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"17fce4ff-daaf-4218-a44c-21747ce55a13"},"source":["# Initialization of the chunk length for the number of RNN's in a particular length, so that it can recall up to 200 timesteps backwards.\n","chunk_len = 200\n","\n","# Function to split the string into chunks\n","def random_chunk():\n","    \n","    # Initializing the starting index value of the big string \n","    start_index = random.randint(0, file_len - chunk_len)\n","\n","    # Initializing the ending index of the string \n","    end_index = start_index + chunk_len + 1\n","\n","    # Returning the chunk\n","    return file[start_index:end_index]\n","\n","# Printing the random chunk string\n","print(random_chunk())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["he market-place. We'll attend you there:\n","Where, if you bring not Marcius, we'll proceed\n","In our first way.\n","\n","MENENIUS:\n","I'll bring him to you.\n","Let me desire your company: he must come,\n","Or what is worst wi\n"]}]},{"cell_type":"markdown","metadata":{"id":"RyKv4jyAZ6C6"},"source":["###  2. Building the Model\n","\n","This model will take input as the character for step $t_{-1}$, and is expected to give the output $t$, which is the next character. There are three layers:\n","1. Linear layer that encodes the input character into an internal state\n","2. GRU layer (which may itself have multiple layers) that operates on that internal and hidden state / You can also use the [RNN](https://pytorch.org/docs/master/generated/torch.nn.RNN.html) Layer in place of GRU in this step.\n","3. Decoder layer that outputs the probability distribution"]},{"cell_type":"code","metadata":{"id":"3h1GIrjHZ6C9"},"source":["# Creating recurrent neural network\n","class RNN(nn.Module):\n","    \n","    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n","        super(RNN, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_size = output_size\n","        self.n_layers = n_layers\n","        self.encoder = nn.Embedding(input_size, hidden_size)\n","        self.gru = nn.GRU(hidden_size, hidden_size, n_layers) # If you want to use RNN you can replace GRU with RNN and see the results\n","        self.decoder = nn.Linear(hidden_size, output_size)\n","    \n","    def forward(self, input, hidden):\n","        input = self.encoder(input.view(1, -1))\n","        output, hidden = self.gru(input.view(1, 1, -1), hidden) # Change self.gru to self.rnn if you are constructing the RNN layer\n","        output = self.decoder(output.view(1, -1))\n","        return output, hidden\n","\n","    def init_hidden(self):\n","        # Here we are Initializing the hidden layer to zero everytime\n","        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kP3H8LiSZ6DC"},"source":["### 3. Defining the Helper Functions\n","\n","Let us define some helper functions to:\n","\n","1. Convert the input string chunks into the character tensors\n","2. Evaluate the model"]},{"cell_type":"markdown","metadata":{"id":"cpWdV9iEZ6DD"},"source":["#### Inputs and Targets\n","Each chunk will be turned into a tensor, specifically a LongTensor (used for integer values), by looping through the characters of the string and looking up the index of each character in all_characters."]},{"cell_type":"code","metadata":{"id":"_j2hxN7QZ6DF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631004312869,"user_tz":-330,"elapsed":43,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"f42a1883-7914-4df0-a32a-3ed70940e070"},"source":["# Turn string into list of longs\n","def char_tensor(string):\n","    tensor = torch.zeros(len(string)).long()\n","    for c in range(len(string)):\n","        tensor[c] = all_characters.index(string[c])\n","    return Variable(tensor)\n","\n","# Let us print the tensor value for a given sample string, you can modify the string here...\n","print(char_tensor('abcDEF'))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([10, 11, 12, 39, 40, 41])\n"]}]},{"cell_type":"markdown","metadata":{"id":"TrottX4SZ6DL"},"source":["Finally you assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters up to the end, and the target will be all characters from the first. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."]},{"cell_type":"code","metadata":{"id":"siN-Z8X_Z6DN"},"source":["def random_training_set():    \n","    chunk = random_chunk()\n","    inp = char_tensor(chunk[:-1])\n","    target = char_tensor(chunk[1:])\n","    return inp, target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c4QPs5BqZ6DS"},"source":["#### Evaluating\n","\n","To evaluate the network feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation pass a priming string to start building up the hidden state, from which you then generate one character at a time.\n","\n","In the below function let us assign the default primary string as 'A' and to choose a class with a probability output use the [muiltinomial distribution](https://pytorch.org/docs/master/generated/torch.multinomial.html)"]},{"cell_type":"code","metadata":{"id":"v2m-JfwXZ6DW"},"source":["def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n","    hidden = rnn.init_hidden()\n","    prime_input = char_tensor(prime_str)\n","    predicted = prime_str\n","\n","    # Use priming string to \"build up\" hidden state\n","    for p in range(len(prime_str) - 1):\n","        _, hidden = rnn(prime_input[p], hidden)\n","    inp = prime_input[-1]\n","    \n","    for p in range(predict_len):\n","        output, hidden = rnn(inp, hidden)\n","        \n","        # Applying Softmax & Sample from the network as a multinomial distribution\n","        output_dist = output.data.view(-1).div(temperature).exp()\n","        top_i = torch.multinomial(output_dist, 1)[0]\n","        \n","        # Add predicted character to string and use as next input\n","        predicted_char = all_characters[top_i]\n","        predicted += predicted_char\n","        inp = char_tensor(predicted_char)\n","\n","    return predicted"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yZVmbuZ4Z6De"},"source":["### 4. Training the Model\n","\n","To keep track of how long training takes, let us add a time_since(timestamp) function which returns a human readable string:"]},{"cell_type":"code","metadata":{"id":"rFevLn43Z6Dh"},"source":["# Function to print amount of time passed\n","def time_since(since):\n","    s = time.time() - since\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QXqylIRyZ6Dn"},"source":["#### The main training function"]},{"cell_type":"code","metadata":{"id":"n3vWoTsxZ6Dp"},"source":["def train(inp, target):\n","    # Initialize the hidden representation, gradient, loss to zeros\n","    hidden = rnn.init_hidden()\n","    rnn.zero_grad()\n","    loss = 0\n","\n","    for c in range(chunk_len):\n","        output, hidden = rnn(inp[c], hidden)\n","        '''unsqueeze() is used to add dimension to the tensor'''\n","        loss += criterion(output, target[c].unsqueeze(dim=0))\n","    # Back propagation\n","    loss.backward()\n","    rnn_optimizer.step()\n","\n","    return loss.item() / chunk_len"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dB-y61RZZ6Dt"},"source":["Then define the training parameters, instantiate the model, and start training. In the below cell  try to print the chunk, loss and time taken for every 50th iteration and for every 20th iteration  try to plot the loss vs epochs(iterations)."]},{"cell_type":"code","metadata":{"id":"OPG6aUEeZ6Du","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631004572931,"user_tz":-330,"elapsed":260077,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"f9ae8e1d-7962-4659-ee35-a9aa149439cc"},"source":["n_epochs = 2000 # Number of epochs\n","print_every = 50\n","plot_every = 20\n","hidden_size = 100\n","n_layers = 1\n","lr = 0.005\n","\n","# The input_size & output_size are the total number of n_characters\n","rnn = RNN(n_characters, hidden_size, n_characters, n_layers) # The rnn variable consists of the return values from the RNN model\n","\n","# Optimize\n","rnn_optimizer = torch.optim.Adam(rnn.parameters(), lr=lr)\n","\n","# Loss function\n","criterion = nn.CrossEntropyLoss()\n","\n","start = time.time()\n","all_losses = []\n","loss_avg = 0\n","\n","for epoch in range(1, n_epochs + 1):\n","    loss = train(*random_training_set())       \n","    loss_avg += loss\n","\n","    if epoch % print_every == 0:\n","        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n","        print(evaluate('Wh', 100), '\\n')\n","\n","    if epoch % plot_every == 0:\n","        all_losses.append(loss_avg / plot_every)\n","        loss_avg = 0"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0m 6s (50 2%) 2.3858]\n","Wh goos bes't thtie wist obe bor poto Kerendy haf lon, anth; sor,\n","I thee nos the pereit?\n","KAENENNYwor t \n","\n","[0m 13s (100 5%) 2.1937]\n","Whalk the th, if pay thand, his the seint ath the hec gin sto the to thin gored shos lathe for bounegs \n","\n","[0m 19s (150 7%) 2.2499]\n","Whe wend ard word her breree rovast ord to to he in\n","Rom and, cess be aresou sterer, worst cour you jor \n","\n","[0m 26s (200 10%) 1.9275]\n","Wher.\n","\n","ROINCENTETISCETIR:\n","Eo mery tho gom this led me busthand of you with ont the mout me whis arth s \n","\n","[0m 32s (250 12%) 2.0821]\n","Whave morviold not to lo gerarser heindds you son:\n","Yith'h to mO heve worsse to you ladd som'an my sere \n","\n","[0m 39s (300 15%) 2.2278]\n","Whire, uspine the his fakerels's bet his not bret are leest for eodagent, uampend, would gund, al shal \n","\n","[0m 45s (350 17%) 1.9701]\n","Whery gray, gome a to may his ar dore.\n","As may, hy my an coud-many! \n","Nook bood well we mank ast will yo \n","\n","[0m 52s (400 20%) 2.0703]\n","Whill and hercine!\n","\n","GEROM:\n","That in you fathere frorge\n","The mave and the saderome the in mance bett.\n","\n","CA \n","\n","[0m 58s (450 22%) 2.2079]\n","Whot by sie\n","Thee thoulT how neaur her govess no good is the kindons do will?\n","\n","JUCKINGUS:\n","it the vear t \n","\n","[1m 5s (500 25%) 2.1132]\n","Wha didniy that dast as a there the dot his dadinse?\n","Thing sher that withing slink, and most the sirit \n","\n","[1m 11s (550 27%) 1.9508]\n","Wh it there one a thoue wome at taid they re but me for in thou brother\n","The pledice prife frother will \n","\n","[1m 18s (600 30%) 1.8384]\n","While or worbled till doth.\n","\n","Pood, where as and but of quisgled of grain.\n","Gord, I with arer,\n","To should \n","\n","[1m 25s (650 32%) 2.0637]\n","Why purtuer congal deather heree\n","thish will the peaven, is toot:\n","I me leel you the not\n","Thabs sent. I t \n","\n","[1m 31s (700 35%) 1.9528]\n","Whance.\n","\n","DORWARCK:\n","Unver, to he forst you, not, should me here-my well are kight!\n","\n","Pord so us uspear\n","A \n","\n","[1m 38s (750 37%) 2.1110]\n","Whe het me have arnth\n","No not muse ene dut make\n","Are had, ars, I her to lid; beapse ir opr demitrus this \n","\n","[1m 44s (800 40%) 1.7647]\n","Whe:\n","The with Ruthing brial and CI/fulk Reavise to weash ma-nrow'e.\n","Thee evesely?\n","He the fear dake, no \n","\n","[1m 51s (850 42%) 1.8950]\n","Whe for combsell there with betimoren havled forse:\n","'THe bouth thee love on and not fornest of have wi \n","\n","[1m 58s (900 45%) 1.7979]\n","Whren of yours pare not for the body and couse condire the gague. His fale,\n","That your not my bang many \n","\n","[2m 4s (950 47%) 1.4726]\n","Whrean's bojong\n","Was your will the know have offcle and tis, dodes.\n","\n","THORTESIO:\n","Five that scay the but  \n","\n","[2m 11s (1000 50%) 1.8166]\n","What wonours loves\n","I horst; I not this from, faritald,\n","Claster nout beed sper the rest one shall than  \n","\n","[2m 17s (1050 52%) 1.9458]\n","Whough Antem fathing mecave\n","To not uses be the name.\n","\n","RICHMONDWARD:\n","Why conder we deather, Jurenent no \n","\n","[2m 24s (1100 55%) 1.9599]\n","Whe a stas anthe compper.\n","\n","DUKE VINCENS:\n","You compliess be no lover'd farm is nease have compountelf.\n","N \n","\n","[2m 30s (1150 57%) 1.8335]\n","Whe them alline to no stone:\n","The mine I tenry us enderend a pranciinst your to reat, alten, to may; he \n","\n","[2m 37s (1200 60%) 1.7938]\n","Wh if theick strome and teem:\n","So blot this freak a better you be. \n","RICHARD IS:\n","I beence sping old shea \n","\n","[2m 43s (1250 62%) 1.7905]\n","Whin the for congson me.\n","And so the tell tell of whour and nose donce peave as come my poities.\n","\n","HENRY \n","\n","[2m 50s (1300 65%) 1.8060]\n","Why seckes the eting\n","Or o's put this comes to shall.\n","\n","MIRIA:\n","Time, yours evers hooly hears beinour pit \n","\n","[2m 56s (1350 67%) 1.8705]\n","Whidal all this nother'd;\n","And with must met gain'd wride with of wound ap\n","As is dive our shall that in \n","\n","[3m 3s (1400 70%) 1.6115]\n","Why diemen'd, think\n","Whother monest that spiouan:\n","I'll be leard divef. Of hard thered wataing live,\n","Spa \n","\n","[3m 9s (1450 72%) 2.0081]\n","Wh,\n","And seds heat, he strantal, house.\n","\n","PRISABELLA:\n","I have a down inbroke, where deat bect of her?\n","\n","BR \n","\n","[3m 15s (1500 75%) 1.8527]\n","Why of your say.\n","\n","KING RICHARD II:\n","Whime was is now!--\n","\n","Porciove and thiin her.\n","\n","SICINANZA BOLINGO:\n","St \n","\n","[3m 22s (1550 77%) 1.4450]\n","Whomer the comafter\n","of the gone of dild vill,\n","Frizen'd bid truce soll I dot.\n","\n","KING RICHARD IV:\n","Getorn  \n","\n","[3m 28s (1600 80%) 1.7316]\n","Wh as cannightles your, bether;\n","Hear, now, O hers the king lead, them weard it shollan, he she not sho \n","\n","[3m 35s (1650 82%) 1.7399]\n","Wh she in thou hamseity;\n","They with out to my learst the caughter that and nor with not tood as is wear \n","\n","[3m 41s (1700 85%) 1.6655]\n","Wh'd Citizen her\n","To her the her enoughter'd take all to shall to they heave\n","And come hang thy for, wit \n","\n","[3m 48s (1750 87%) 1.8440]\n","Whep \n","what is would grownviones:\n","And that shall happy camespers trugh be fear,\n","What Maked the detrusti \n","\n","[3m 54s (1800 90%) 1.6785]\n","Wh's sparent\n","She so sheemay: and All cous.\n","\n","PERISCA:\n","Nount do the steep as me was to hope!\n","A dity Roma \n","\n","[4m 1s (1850 92%) 1.7251]\n","Wh that your shall next;\n","Seress rivence! with I leasond nenderus your extolders:\n","I hear, be ristab of  \n","\n","[4m 7s (1900 95%) 1.8008]\n","What good in she and with core you we words!\n","I will him and in would vill well this lose me\n","More me, t \n","\n","[4m 13s (1950 97%) 1.8816]\n","Wh hearty?\n","my her, and will all mery to mustry,\n","Of my suy, this man that, to mades.\n","\n","LADY ANNE:\n","You he \n","\n","[4m 20s (2000 100%) 1.7563]\n","Wh and he'll go\n","Fratrired Good shall she can the done, and all larod.\n","Ay I doth smanter so work?\n","\n","LUCI \n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"xWhQGN73Z6D4"},"source":["#### Plotting the Training Losses\n","\n","Plotting the historical loss from all_losses shows the network learning:"]},{"cell_type":"code","metadata":{"id":"aRMgqaJoZ6D8","colab":{"base_uri":"https://localhost:8080/","height":296},"executionInfo":{"status":"ok","timestamp":1631004574085,"user_tz":-330,"elapsed":1178,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"39632fc5-38c2-4ddb-a093-48db35cb6afb"},"source":["plt.figure()\n","plt.plot(all_losses)\n","plt.xlabel(\"epoch\")\n","plt.ylabel(\"loss\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(0, 0.5, 'loss')"]},"metadata":{},"execution_count":16},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"aDMlqGtnZ6EF"},"source":["### 5. Adapting or Tuning for Text Generation\n","\n","In the evaluate function above, every time a prediction is made, the outputs are divided by the \"temperature\" argument passed. Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs. Using a lower value (less than 1) makes high probabilities contribute more. By turning the temperature towards zero you are choosing only the most likely outputs."]},{"cell_type":"markdown","metadata":{"id":"-hkcqnIlZ6EI"},"source":["Let us see the effects of this by adjusting the temperature argument:"]},{"cell_type":"code","metadata":{"id":"cSwY7UOEZ6EK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631004574108,"user_tz":-330,"elapsed":245,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"88d725c6-9f22-4ad3-b418-a1811c366009"},"source":["print(evaluate('u', 200, temperature=0.8))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["umpher\n","for the sweecelmom pitter, hebed what be gone\n","we pant that it not in the bededast, I their\n","gentle a wat wat sword ay not his good thou have stale\n","me me litdence they not how save to stoniter fro\n"]}]},{"cell_type":"markdown","metadata":{"id":"WIjNrZNSZ6ER"},"source":["Lower temperatures are less varied, choosing only the more probable outputs:"]},{"cell_type":"code","metadata":{"id":"MB_ntU41a74t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631004574111,"user_tz":-330,"elapsed":231,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"68bd1743-0234-4cf1-ab1f-fd1f10484305"},"source":["print(evaluate('Th', 200, temperature=0.2))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The show the forth\n","go the stand the stand the stand the forth the stand\n","I will the conclieve his stand a stands a site and the stand\n","I will this should the stand he so the stand\n","I shall the stand the st\n"]}]},{"cell_type":"markdown","metadata":{"id":"80cSDcEXZ6EU"},"source":["\n","Higher temperatures more varied, choosing less probable outputs:"]},{"cell_type":"code","metadata":{"id":"NXusqAsCZ6EV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631004574118,"user_tz":-330,"elapsed":226,"user":{"displayName":"Amulya Mamindla","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjLa0CmKQl-ZknY-o_q4I3C9cpWkK4ZwW_T9FEFKg=s64","userId":"12783715330151357368"}},"outputId":"9d8fc21a-54d3-4d57-f5ef-3081f9a21db0"},"source":["print(evaluate('how', 200, temperature=1.4))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["how.\n","My trult be is I hemstion,\n","Egesnany woy't. uto, toiciso. Werrothpronks\n","soll, tty hust waid kighartno, him me: i!\n","But then not bo'\n","Well\n","this pauke Fatracky, takeoande! a bontin,\n","I' knot, nob'X\\iod ba\n"]}]},{"cell_type":"markdown","metadata":{"id":"LOkO2rwFZ6Eb"},"source":["### Ungraded Exercise 1:\n","\n","Change the number of epochs to 1000. Calculate the time taken and loss"]},{"cell_type":"code","metadata":{"id":"JZzyyVD63dmJ"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Xn9ZeqjF3DB_"},"source":["### Acknowledgement\n","\n","https://blog.owulveryck.info/2017/10/29/about-recurrent-neural-network-shakespeare-and-go.html"]}]}