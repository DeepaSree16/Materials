{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-16-AN-4-4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uvpnKs2ZpHmQ"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YMRMCLwCpMsS"},"source":["### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"oaiqMmlZfnZK"},"source":["## Visualising CNN"]},{"cell_type":"code","metadata":{"cellView":"form","id":"9RBF8fzHfq7L"},"source":["#@title Case Study Walkthrough\n","#@markdown  Visualising CNN\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"320\" height=\"240\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/visualization_of_cnns.mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wg7Fu8QRpR7v"},"source":["This experiment is based on the Visualization of filters in CNN. "]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/CaseStudies/visualization.zip\n","! sx unzip visualization.zip"],"metadata":{"id":"ru2MMQy8dedW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NptGOPP91vYA"},"source":["###Importing required packages\n"]},{"cell_type":"code","metadata":{"id":"DMYahLuu8GYw"},"source":["import os\n","import numpy as np\n","import torch\n","from torch.nn import ReLU\n","from torch.optim import Adam\n","from torchvision import models\n","from misc_functions import preprocess_image, recreate_image, save_image, convert_to_grayscale, get_example_params\n","from matplotlib import pyplot as plt\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VfbZVPS_8GY3"},"source":["## Gradient visualization with vanilla backpropagation (Standard backpropagation)\n","\n","* Produces gradients generated with vanilla back propagation from the image\n","* We use Alexnet for this Visualization"]},{"cell_type":"code","metadata":{"id":"BLgV1gR68GY5"},"source":["class VanillaBackprop():\n","    \n","    def __init__(self, model):\n","        self.model = model\n","        self.gradients = None\n","        # Put model in evaluation mode\n","        self.model.eval()\n","        # Hook the first layer to get the gradient\n","        self.hook_layers()\n","\n","    def hook_layers(self):\n","        def hook_function(module, grad_in, grad_out):\n","            self.gradients = grad_in[0]\n","\n","        # Register hook to the first layer\n","        first_layer = list(self.model.features._modules.items())[0][1]\n","        first_layer.register_backward_hook(hook_function)\n","\n","    def generate_gradients(self, input_image, target_class):\n","        # Forward\n","        model_output = self.model(input_image)\n","        # Zero grads\n","        self.model.zero_grad()\n","        # Target for backprop\n","        one_hot_output = torch.FloatTensor(1, model_output.size()[-1]).zero_()\n","        one_hot_output[0][target_class] = 1\n","        # Backward pass\n","        model_output.backward(gradient=one_hot_output)\n","        # Convert Pytorch variable to numpy array\n","        # [0] to get rid of the first channel (1,3,224,224)\n","        gradients_as_arr = self.gradients.data.numpy()[0]\n","        return gradients_as_arr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_oNSmpKA_dB"},"source":["* Plot the example image by specifying index\n","\n","* Calculate and plot the  normalized vanilla gradients and grayscale  normalized vanilla gradients of the image\n"]},{"cell_type":"code","metadata":{"id":"bs45ZCbYBLP9"},"source":["# Specify the index\n","target_example = 2  # Spider\n","# Preprocess the image and get the pre-trained alex model for the image\n","(original_image, prep_img, target_class, file_name_to_export, pretrained_model) = get_example_params(target_example)\n","# Plot the image\n","plt.figure(figsize=(10,10))\n","plt.subplot(131)\n","plt.title(file_name_to_export)\n","plt.imshow(original_image)\n","plt.xticks([])\n","plt.yticks([])\n","\n","# Vanilla backprop\n","VBP = VanillaBackprop(pretrained_model)\n","# Generate gradients\n","vanilla_grads = VBP.generate_gradients(prep_img, target_class)\n","# Calculate the normalized gradients\n","vanilla_grads = vanilla_grads - vanilla_grads.min()\n","vanilla_grads /= vanilla_grads.max()\n","# Plot the normalized the vanilla gradients\n","plt.subplot(132)\n","plt.title('vanilla_grads')\n","plt.imshow(vanilla_grads.transpose(1,2,0))\n","plt.xticks([])\n","plt.yticks([])\n","\n","# Convert the vanilla_grads to grayscale\n","grayscale_vanilla_grads = convert_to_grayscale(vanilla_grads)\n","# Normalize the grayscale_vanilla_grads\n","grayscale_vanilla_grads = grayscale_vanilla_grads - grayscale_vanilla_grads.min()\n","grayscale_vanilla_grads /= grayscale_vanilla_grads.max()\n","# Plot the normalized grayscale_vanilla_grads\n","plt.subplot(133)\n","plt.title('grayscale_vanilla_grads')\n","plt.imshow(grayscale_vanilla_grads[0], cmap='gray')\n","plt.xticks([])\n","plt.yticks([])\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YihOlP_I8GZF"},"source":["## Gradient visualization with guided backpropagation\n"]},{"cell_type":"markdown","metadata":{"id":"K-HowDiV8GZH"},"source":["Another way to visualize CNN layers is to visualize activations for a specific input on a specific layer and filter. Below example is obtained from layers/filters of AlexNet for the first image using guided backpropagation. The method is quite similar to backpropagation but instead of guiding the signal from the last layer and a specific target, it guides the signal from a specific layer and filter."]},{"cell_type":"code","metadata":{"id":"ucU551WD8GZI"},"source":["class GuidedBackprop():\n","    def __init__(self, model):\n","        self.model = model\n","        self.gradients = None\n","        self.forward_relu_outputs = []\n","        # Put model in evaluation mode\n","        self.model.eval()\n","        self.update_relus()\n","        self.hook_layers()\n","\n","    def hook_layers(self):\n","        def hook_function(module, grad_in, grad_out):\n","            self.gradients = grad_in[0]\n","        # Register hook to the first layer\n","        first_layer = list(self.model.features._modules.items())[0][1]\n","        first_layer.register_backward_hook(hook_function)\n","\n","    def update_relus(self):\n","        \"\"\"\n","            Updates relu activation functions so that\n","                1- stores output in forward pass\n","                2- imputes zero for gradient values that are less than zero\n","        \"\"\"\n","        def relu_backward_hook_function(module, grad_in, grad_out):\n","            \"\"\"\n","            If there is a negative gradient, change it to zero\n","            \"\"\"\n","            # Get last forward output\n","            corresponding_forward_output = self.forward_relu_outputs[-1]\n","            corresponding_forward_output[corresponding_forward_output > 0] = 1\n","            modified_grad_out = corresponding_forward_output * torch.clamp(grad_in[0], min=0.0)\n","            del self.forward_relu_outputs[-1]  # Remove last forward output\n","            return (modified_grad_out,)\n","\n","        def relu_forward_hook_function(module, ten_in, ten_out):\n","            \"\"\"\n","            Store results of forward pass\n","            \"\"\"\n","            self.forward_relu_outputs.append(ten_out)\n","\n","        # Loop through layers, hook up ReLUs\n","        for pos, module in self.model.features._modules.items():\n","            if isinstance(module, ReLU):\n","                module.register_backward_hook(relu_backward_hook_function)\n","                module.register_forward_hook(relu_forward_hook_function)\n","\n","    def generate_gradients(self, input_image, target_class, cnn_layer, filter_pos):\n","        self.model.zero_grad()\n","        # Forward pass\n","        x = input_image\n","        for index, layer in enumerate(self.model.features):\n","            # Forward pass layer by layer\n","            # x is not used after this point because it is only needed to trigger\n","            # the forward hook function\n","            x = layer(x)\n","            # Only need to forward until the selected layer is reached\n","            if index == cnn_layer:\n","                # (forward hook function triggered)\n","                break\n","        conv_output = torch.sum(torch.abs(x[0, filter_pos]))\n","        # Backward pass\n","        conv_output.backward()\n","        # Convert Pytorch variable to numpy array\n","        # [0] to get rid of the first channel (1,3,224,224)\n","        gradients_as_arr = self.gradients.data.numpy()[0]\n","        return gradients_as_arr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bvJZjbbXLhJX"},"source":["## Visualize activations of the first 30 filters in layer 29 for given input\n","\n"]},{"cell_type":"code","metadata":{"id":"Fs30kl4q8GZN"},"source":["cnn_layer = 29\n","target_example = 2  # Spider\n","(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n","    get_example_params(target_example)\n","\n","layer_29_filters_visualization = {}\n","for filter_pos in range(1,31):\n","    # File export name\n","    title = file_name_to_export + '_layer' + str(cnn_layer) + '_filter' + str(filter_pos)\n","    # Guided backprop\n","    GBP = GuidedBackprop(pretrained_model)\n","    # Get gradients\n","    guided_grads = GBP.generate_gradients(prep_img, target_class, cnn_layer, filter_pos)\n","\n","    guided_grads = guided_grads - guided_grads.min()\n","    guided_grads /= guided_grads.max()\n","    layer_29_filters_visualization[title]=guided_grads\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TNgbQUOp8GZS"},"source":["fig, ax = plt.subplots(nrows=6, ncols=5, figsize = (40,40))\n","\n","for r, row in enumerate(ax):\n","    for c, col in enumerate(row):\n","        fil_name = file_name_to_export + '_layer29_filter' + str(r*5+c+1)\n","        col.imshow(layer_29_filters_visualization[fil_name].transpose(1,2,0))\n","        col.set_xticks([])\n","        col.set_yticks([])\n","        col.set_title(fil_name, fontdict={'fontsize':20})\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N3w2UgRDLnAm"},"source":["## Visualize activations of Filter 0 in the first 10 layers of AlexNet for given input"]},{"cell_type":"code","metadata":{"id":"TN4uFJpT8GZV"},"source":["filter_pos = 0\n","target_example = 2  # Spider\n","(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n","    get_example_params(target_example)\n","\n","filter_0_30layers_visualization = {}\n","for cnn_layer in range(1,11):\n","    # File export name\n","    title = file_name_to_export + '_layer' + str(cnn_layer) + '_filter' + str(filter_pos)\n","    # Guided backprop\n","    GBP = GuidedBackprop(pretrained_model)\n","    # Get gradients\n","    guided_grads = GBP.generate_gradients(prep_img, target_class, cnn_layer, filter_pos)\n","\n","    guided_grads = guided_grads - guided_grads.min()\n","    guided_grads /= guided_grads.max()\n","    filter_0_30layers_visualization[title]=guided_grads\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvlmF1gB8GZb"},"source":["fig, ax = plt.subplots(nrows=2, ncols=5, figsize = (20,20))\n","\n","for r, row in enumerate(ax):\n","    for c, col in enumerate(row):\n","        fil_name = file_name_to_export + '_layer'+str(r*5+c+1)+'_filter0'\n","        col.imshow(filter_0_30layers_visualization[fil_name].transpose(1,2,0))\n","        col.set_xticks([])\n","        col.set_yticks([])\n","        col.set_title(fil_name, fontdict={'fontsize':20})\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FCROOyce8GZk"},"source":["## Convolutional Neural Network Filter Visualization\n","\n","* CNN filters can be visualized when we optimize the input image with respect to output of the specific convolution operation. For this example we used a pre-trained VGG16. Visualizations of layers start with basic color and direction filters at lower levels. As we approach towards the final layer the complexity of the filters also increase. If you employ external techniques like blurring, gradient clipping etc. you will probably produce better images\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aA0ZGPDn8GZo"},"source":["## VGG Net Architecture\n","\n","* Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","* Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","* Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","* Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","* Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"]},{"cell_type":"code","metadata":{"id":"79J_7aa98GZl"},"source":["class CNNLayerVisualization():\n","    def __init__(self, model, selected_layer, selected_filter):\n","        self.model = model\n","        self.model.eval()\n","        self.selected_layer = selected_layer\n","        self.selected_filter = selected_filter\n","        self.conv_output = 0\n","        # Create the folder to export images if not exists\n","        if not os.path.exists('../generated'):\n","            os.makedirs('../generated')\n","\n","    def hook_layer(self):\n","        def hook_function(module, grad_in, grad_out):\n","            # Gets the conv output of the selected filter (from selected layer)\n","            self.conv_output = grad_out[0, self.selected_filter]\n","        # Hook the selected layer\n","        self.model[self.selected_layer].register_forward_hook(hook_function)\n","\n","    def visualise_layer_with_hooks(self):\n","        # Hook the selected layer\n","        self.hook_layer()\n","        # Generate a random image\n","        random_image = np.uint8(np.random.uniform(150, 180, (224, 224, 3)))\n","        # Process image and return variable\n","        processed_image = preprocess_image(random_image, False)\n","        # Define optimizer for the image\n","        optimizer = Adam([processed_image], lr=0.1, weight_decay=1e-6)\n","        for i in range(1, 31):\n","            optimizer.zero_grad()\n","            # Assign create image to a variable to move forward in the model\n","            x = processed_image\n","            for index, layer in enumerate(self.model):\n","                # Forward pass layer by layer\n","                # x is not used after this point because it is only needed to trigger\n","                # the forward hook function\n","                x = layer(x)\n","                # Only need to forward until the selected layer is reached\n","                if index == self.selected_layer:\n","                    # (forward hook function triggered)\n","                    break\n","            # Loss function is the mean of the output of the selected layer/filter\n","            # We try to minimize the mean of the output of that specific filter\n","            loss = -torch.mean(self.conv_output)\n","            #print('Iteration:', str(i), 'Loss:', \"{0:.2f}\".format(loss.data.numpy()))\n","            # Backward\n","            loss.backward()\n","            # Update image\n","            optimizer.step()\n","            # Recreate image\n","            self.created_image = recreate_image(processed_image)\n","            # Save image\n","            if i % 5 == 0:\n","                im_path = '../generated/layer_vis_l' + str(self.selected_layer) + \\\n","                    '_f' + str(self.selected_filter) + '_iter' + str(i) + '.jpg'\n","                save_image(self.created_image, im_path)\n","        return self.created_image  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"DwKCFWbK8GZp"},"source":["cnn_layer = 17\n","filter_visualization_layer_wise = {}\n","# Fully connected layer is not needed\n","for filter_pos in tqdm(range(1,26)):\n","    pretrained_model = models.vgg16(pretrained=True).features\n","    layer_vis = CNNLayerVisualization(pretrained_model, cnn_layer, filter_pos)\n","\n","    # Layer visualization with pytorch hooks\n","    filter_visualization_layer_wise['filter_'+str(filter_pos)]=layer_vis.visualise_layer_with_hooks()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"EzLl0PAg8GZw"},"source":["fig, ax = plt.subplots(nrows=5, ncols=5, figsize = (30,30))\n","\n","for r, row in enumerate(ax):\n","    for c, col in enumerate(row):\n","        #print(r,c,r*5+c+1 )\n","        fil_name = 'filter_'+str((r*5+c+1))\n","        col.imshow(filter_visualization_layer_wise[fil_name])\n","        col.set_xticks([])\n","        col.set_yticks([])\n","        col.set_title(fil_name, fontdict={'fontsize':20})\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Fi2EG3NB8GZz"},"source":["## Inverted Representation\n","\n","* We are using Alexnet for this visualization"]},{"cell_type":"markdown","metadata":{"id":"tWuctSHq8GZ0"},"source":["## Alexnet Architecture"]},{"cell_type":"markdown","metadata":{"id":"aPDlW4ur8GZ1"},"source":["AlexNet:\n","\n","Features Extraction:\n","* Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","* Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","* Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","* ReLU(inplace)\n","* MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n","\n","Classifier-Sequential:\n","* Dropout(p=0.5)\n","* Linear(in_features=9216, out_features=4096, bias=True)\n","* ReLU(inplace)\n","* Dropout(p=0.5)\n","* Linear(in_features=4096, out_features=4096, bias=True)\n","* ReLU(inplace)\n","* Linear(in_features=4096, out_features=1000, bias=True)\n","\n"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"UFo4CXgX8GZ2"},"source":["import torch\n","from torch.autograd import Variable\n","from torch.optim import SGD\n","import os\n","from misc_functions import get_example_params, recreate_image, save_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"NbnbWSjP8GZ7"},"source":["class InvertedRepresentation():\n","    def __init__(self, model):\n","        self.model = model\n","        self.model.eval()\n","        if not os.path.exists('../generated'):\n","            os.makedirs('../generated')\n","\n","    def alpha_norm(self, input_matrix, alpha):\n","        \"\"\"\n","            Converts matrix to vector then calculates the alpha norm\n","        \"\"\"\n","        alpha_norm = ((input_matrix.view(-1))**alpha).sum()\n","        return alpha_norm\n","\n","    def total_variation_norm(self, input_matrix, beta):\n","        \"\"\"\n","            Total variation norm is the second norm in the paper\n","            represented as R_V(x)\n","        \"\"\"\n","        to_check = input_matrix[:, :-1, :-1]  # Trimmed: right - bottom\n","        one_bottom = input_matrix[:, 1:, :-1]  # Trimmed: top - right\n","        one_right = input_matrix[:, :-1, 1:]  # Trimmed: top - right\n","        total_variation = (((to_check - one_bottom)**2 +\n","                            (to_check - one_right)**2)**(beta/2)).sum()\n","        return total_variation\n","\n","    def euclidian_loss(self, org_matrix, target_matrix):\n","        \"\"\"\n","            Euclidian loss is the main loss function in the paper\n","            ||fi(x) - fi(x_0)||_2^2& / ||fi(x_0)||_2^2\n","        \"\"\"\n","        distance_matrix = target_matrix - org_matrix\n","        euclidian_distance = self.alpha_norm(distance_matrix, 2)\n","        normalized_euclidian_distance = euclidian_distance / self.alpha_norm(org_matrix, 2)\n","        return normalized_euclidian_distance\n","\n","    def get_output_from_specific_layer(self, x, layer_id):\n","        \"\"\"\n","            Saves the output after a forward pass until nth layer\n","            This operation could be done with a forward hook too\n","            but this one is simpler\n","        \"\"\"\n","        layer_output = None\n","        for index, layer in enumerate(self.model.features):\n","            x = layer(x)\n","            if str(index) == str(layer_id):\n","                layer_output = x[0]\n","                break\n","        return layer_output\n","\n","    def generate_inverted_image_specific_layer(self, input_image, img_size, target_layer=3, layers=False):\n","        # Generate a random image which we will optimize\n","        opt_img = Variable(1e-1 * torch.randn(1, 3, img_size, img_size), requires_grad=True)\n","        # Define optimizer for previously created image\n","        optimizer = SGD([opt_img], lr=1e4, momentum=0.9)\n","        # Get the output from the model after a forward pass until target_layer\n","        # with the input image (real image, NOT the randomly generated one)\n","        input_image_layer_output = \\\n","            self.get_output_from_specific_layer(input_image, target_layer)\n","\n","        # Alpha regularization parametrs\n","        # Parameter alpha, which is actually sixth norm\n","        alpha_reg_alpha = 6\n","        # The multiplier, lambda alpha\n","        alpha_reg_lambda = 1e-7\n","\n","        # Total variation regularization parameters\n","        # Parameter beta, which is actually second norm\n","        tv_reg_beta = 2\n","        # The multiplier, lambda beta\n","        tv_reg_lambda = 1e-8\n","        inverted_images_for_iterations = {}\n","        for i in range(201):\n","            optimizer.zero_grad()\n","            # Get the output from the model after a forward pass until target_layer\n","            # with the generated image (randomly generated one, NOT the real image)\n","            output = self.get_output_from_specific_layer(opt_img, target_layer)\n","            # Calculate euclidian loss\n","            euc_loss = 1e-1 * self.euclidian_loss(input_image_layer_output.detach(), output)\n","            # Calculate alpha regularization\n","            reg_alpha = alpha_reg_lambda * self.alpha_norm(opt_img, alpha_reg_alpha)\n","            # Calculate total variation regularization\n","            reg_total_variation = tv_reg_lambda * self.total_variation_norm(opt_img,\n","                                                                            tv_reg_beta)\n","            # Sum all to optimize\n","            loss = euc_loss + reg_alpha + reg_total_variation\n","            # Step\n","            loss.backward()\n","            optimizer.step()\n","            # Generate image every 5 iterations\n","            if i % 5 == 0:\n","                print('Iteration:', str(i), 'Loss:', loss.data.numpy())\n","                recreated_im = recreate_image(opt_img)\n","                inverted_images_for_iterations['Iteration'+str(i)]=recreated_im\n","                im_path = '../generated/Inv_Image_Layer_' + str(target_layer) + \\\n","                    '_Iteration_' + str(i) + '.jpg'\n","                save_image(recreated_im, im_path)\n","\n","            # Reduce learning rate every 40 iterations\n","            if i % 40 == 0:\n","                for param_group in optimizer.param_groups:\n","                    param_group['lr'] *= 1/10\n","        if layers:\n","            return recreated_im\n","        return inverted_images_for_iterations\n","            "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"pVKZnjCD8GaE","scrolled":true},"source":["target_example = 2  # 2:spider; 0:snake; 1:dog\n","(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n","    get_example_params(target_example)\n","\n","inverted_representation = InvertedRepresentation(pretrained_model)\n","image_size = 224  # width & height\n","target_layer = 4\n","inverted_images = inverted_representation.generate_inverted_image_specific_layer(prep_img,\n","                                                               image_size,\n","                                                               target_layer)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"r-pm37jv8GaU"},"source":["fig, ax = plt.subplots(nrows=8, ncols=5, figsize = (30,30))\n","\n","for r, row in enumerate(ax):\n","    for c, col in enumerate(row):\n","        fil_name = 'Iteration'+str((r*5+c)*5)\n","        col.imshow(inverted_images[fil_name])\n","        col.set_xticks([])\n","        col.set_yticks([])\n","        col.set_title(fil_name, fontdict={'fontsize':20})\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"3D_nzrLi8Gad"},"source":["target_example = 2  # spider; 0:snake; 1:dog\n","(original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n","    get_example_params(target_example)\n","\n","inverted_representation_per_layer= {}\n","inverted_representation = InvertedRepresentation(pretrained_model)\n","image_size = 224  # width & height\n","for target_layer in range(10):\n","    (original_image, prep_img, target_class, file_name_to_export, pretrained_model) =\\\n","    get_example_params(target_example)\n","    inverted_representation = InvertedRepresentation(pretrained_model)\n","    inverted_image = inverted_representation.generate_inverted_image_specific_layer(prep_img,\n","                                                               image_size,\n","                                                               target_layer, layers= True)\n","    inverted_representation_per_layer[target_layer]= inverted_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"CVfOYqe48Gal"},"source":["fig, ax = plt.subplots(nrows=2, ncols=5, figsize = (30,30))\n","\n","for r, row in enumerate(ax):\n","    for c, col in enumerate(row):\n","        fil_name = (r*5+c)\n","        col.imshow(inverted_representation_per_layer[fil_name])\n","        col.set_xticks([])\n","        col.set_yticks([])\n","        col.set_title('layer_'+str(fil_name+1), fontdict={'fontsize':20})\n","\n","plt.show()"],"execution_count":null,"outputs":[]}]}