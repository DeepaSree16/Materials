{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-13-AN-2-4.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YUAYH8SHdlSI"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Z1B-hn6Ids0P"},"source":["### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"A6EP8rzq82ek"},"source":["### AI/ML Technique"]},{"cell_type":"markdown","metadata":{"id":"LWvT9ccg86ZC"},"source":["Automatic colorization functionality for Real-Time User-Guided Image Colorization with Learned Deep Priors, SIGGRAPH 2017!\n","\n","**Abstract:** A deep learning approach for user-guided image colorization. The system directly maps a grayscale image, along with sparse, local user ``hints\" to an output colorization with a Convolutional Neural Network (CNN). Rather than using hand-defined rules, the network propagates user edits by fusing low-level cues along with high-level semantic information, learned from large-scale data. We train on a million images, with simulated user inputs. To guide the user towards efficient input selection, the system recommends likely colors based on the input image and current user inputs. The colorization is performed in a single feed-forward pass, enabling real-time use. Even with randomly simulated user inputs, we show that the proposed system helps novice users quickly create realistic colorizations, and show large improvements in colorization quality with just a minute of use. In addition, we show that the framework can incorporate other user \"hints\" as to the desired colorization, showing an application to color histogram transfer."]},{"cell_type":"markdown","metadata":{"id":"3m9kCRZceS8Y"},"source":["### Setup Steps"]},{"cell_type":"code","metadata":{"id":"8RvR2Y2DFk6w"},"source":["#@title Please enter your registration id to start:  { run: \"auto\", display-mode: \"form\" }\n","Id = \"P21A17E_test\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9A39Um0FlCl"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"9812345678\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/colorization.zip\n","! unzip colorization.zip"],"metadata":{"id":"LsXitHqNchqE"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RW706KaYtdkW"},"source":["cd /content/colorization"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gDuj3admFKo0"},"source":["#### Importing required packages"]},{"cell_type":"code","metadata":{"id":"yslYZx00wIa-"},"source":["import matplotlib.pyplot as plt\n","\n","from base_color import *\n","from eccv16 import *\n","from siggraph17 import *\n","\n","from PIL import Image\n","import numpy as np\n","from skimage import color\n","import torch\n","import torch.nn.functional as F"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2xaLAO3wcp5w"},"source":["#### Load the image and pre-process\n","\n","**load_img()** function is used to load the image using the path of the image\n","\n","**resize_img()** function is used to resize the image to 256, 256\n","\n","**preprocess_img()**  function is to convert the image from the sRGB color space (IEC 61966-2-1:1999) to the CIE Lab colorspace under the given illuminant and observer return the image in tensor array.\n","\n","**postprocess_tens()** function will take the output of the model and it will make the image suitable to plot using matplotlib"]},{"cell_type":"code","metadata":{"id":"KrjVAXyZGELM"},"source":["def load_img(img_path):\n","\tout_np = np.asarray(Image.open(img_path))\n","\tif(out_np.ndim==2):\n","\t\tout_np = np.tile(out_np[:,:,None],3)\n","\treturn out_np\n","\n","def resize_img(img, HW=(256,256), resample=3):\n","\treturn np.asarray(Image.fromarray(img).resize((HW[1],HW[0]), resample=resample))\n","\n","def preprocess_img(img_rgb_orig, HW=(256,256), resample=3):\n","\t# return original size L and resized L as torch Tensors\n","\timg_rgb_rs = resize_img(img_rgb_orig, HW=HW, resample=resample)\n","\t\n","\timg_lab_orig = color.rgb2lab(img_rgb_orig)\n","\timg_lab_rs = color.rgb2lab(img_rgb_rs)\n","\n","\timg_l_orig = img_lab_orig[:,:,0]\n","\timg_l_rs = img_lab_rs[:,:,0]\n","\n","\ttens_orig_l = torch.Tensor(img_l_orig)[None,None,:,:]\n","\ttens_rs_l = torch.Tensor(img_l_rs)[None,None,:,:]\n","\n","\treturn (tens_orig_l, tens_rs_l)\n","\n","def postprocess_tens(tens_orig_l, out_ab, mode='bilinear'):\n","\t# tens_orig_l \t1 x 1 x H_orig x W_orig\n","\t# out_ab \t\t1 x 2 x H x W\n","\n","\tHW_orig = tens_orig_l.shape[2:]\n","\tHW = out_ab.shape[2:]\n","\n","\t# call resize function if needed\n","\tif HW_orig[0] != HW[0] or HW_orig[1]!=HW[1]:\n","\t\tout_ab_orig = F.interpolate(out_ab, size=HW_orig, mode='bilinear')\n","\telse:\n","\t\tout_ab_orig = out_ab\n","\n","\tout_lab_orig = torch.cat((tens_orig_l, out_ab_orig), dim=1)\n","\treturn color.lab2rgb(out_lab_orig.data.cpu().numpy()[0,...].transpose((1,2,0)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QqmYaX53c21R"},"source":["#### Load the pre-trained models convert to cuda runtime"]},{"cell_type":"code","metadata":{"id":"zGJ697LFc1_V"},"source":["use_gpu = True\n","\n","# load colorizers\n","colorizer_eccv16 = eccv16(pretrained=True).eval()\n","colorizer_siggraph17 = siggraph17(pretrained=True).eval()\n","if use_gpu:\n","\tcolorizer_eccv16.cuda()\n","\tcolorizer_siggraph17.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nS2ytLncf5og"},"source":["specify the image path to pre-process and convert the image tensor array to cuda runtime"]},{"cell_type":"code","metadata":{"id":"QtAWlhEqdKKl"},"source":["img_path = 'BlacknWhite.JPG'\n","\n","# default size to process images is 256x256\n","# grab L channel in both original (\"orig\") and resized (\"rs\") resolutions\n","img = load_img(img_path)\n","(tens_l_orig, tens_l_rs) = preprocess_img(img, HW=(256,256))\n","if(use_gpu):\n","\ttens_l_rs = tens_l_rs.cuda()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nU8bJ8WAdeJl"},"source":["#### Model evaluation and save the output"]},{"cell_type":"code","metadata":{"id":"zQj04blpZUyg"},"source":["# colorizer outputs 256x256 ab map\n","# resize and concatenate to original L channel\n","img_bw = postprocess_tens(tens_l_orig, torch.cat((0*tens_l_orig,0*tens_l_orig),dim=1))\n","out_img_eccv16 = postprocess_tens(tens_l_orig, colorizer_eccv16(tens_l_rs).cpu())\n","out_img_siggraph17 = postprocess_tens(tens_l_orig, colorizer_siggraph17(tens_l_rs).cpu())\n","\n","plt.imsave('s_eccv16.png', out_img_eccv16)\n","plt.imsave('s_siggraph17.png', out_img_siggraph17)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6j0mMwPgdX1Z"},"source":["#### Plot the image"]},{"cell_type":"code","metadata":{"id":"kYmmAKJ5dWxm"},"source":["plt.figure(figsize=(12,8))\n","plt.subplot(2,2,1)\n","plt.imshow(img)\n","plt.title('Original')\n","plt.axis('off')\n","\n","plt.subplot(2,2,2)\n","plt.imshow(img_bw)\n","plt.title('Input')\n","plt.axis('off')\n","\n","plt.subplot(2,2,3)\n","plt.imshow(out_img_eccv16)\n","plt.title('Output (ECCV 16)')\n","plt.axis('off')\n","\n","plt.subplot(2,2,4)\n","plt.imshow(out_img_siggraph17)\n","plt.title('Output (SIGGRAPH 17)')\n","plt.axis('off')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"et8X1j898hto"},"source":["References: https://arxiv.org/abs/1603.08511"]}]}