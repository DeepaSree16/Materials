{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-19-AS-3-4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"cf0a6631c06445ab9f5d23447d349104":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_915f583868eb4ab8a5c496ddab841dc3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f861952ab53e4216be3ae7cec03002c9","IPY_MODEL_ba3915f002164cb58236a103a23e9bed","IPY_MODEL_2d4849de1deb4cefb0466f7ab67c018c"]}},"915f583868eb4ab8a5c496ddab841dc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f861952ab53e4216be3ae7cec03002c9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a19c74c0944c4fadbbfd95cab83369c5","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ac187a28d4ac437696e23c0372c4d587"}},"ba3915f002164cb58236a103a23e9bed":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9d764ae672464c08ae71c83dce807a1a","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":9912422,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":9912422,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b9547090081743bdb34c3bfd3298a762"}},"2d4849de1deb4cefb0466f7ab67c018c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a09ae7e7ad6b42018a74f91a368c8a8d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 9913344/? [00:00&lt;00:00, 19445321.42it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_91c7f7efceaf4a45885c82e5f5b4dc29"}},"a19c74c0944c4fadbbfd95cab83369c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ac187a28d4ac437696e23c0372c4d587":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9d764ae672464c08ae71c83dce807a1a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b9547090081743bdb34c3bfd3298a762":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a09ae7e7ad6b42018a74f91a368c8a8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"91c7f7efceaf4a45885c82e5f5b4dc29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7a45b502b184416ab5543280d978bd42":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d0441cfd23ce4471bcd6120536e11feb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_19b8b56079f44180b2fbc941ade9ebfb","IPY_MODEL_8c869cc41f234d92af9770416998add9","IPY_MODEL_404fd037739042b19fcb8382f2206aea"]}},"d0441cfd23ce4471bcd6120536e11feb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"19b8b56079f44180b2fbc941ade9ebfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8a6bed4574c3471fb6811a0e9801ee64","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ef35830989ff4e16ac9a0f79c8266301"}},"8c869cc41f234d92af9770416998add9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_71d1478a7ad2402d8952c2c9a318fb86","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":28881,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28881,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_970a84a453e5451990988fe80c652cfd"}},"404fd037739042b19fcb8382f2206aea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f85963325a024836a72d43603b6a0e94","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29696/? [00:00&lt;00:00, 597883.36it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f22372cbcdcc49a58fa469aac5544c33"}},"8a6bed4574c3471fb6811a0e9801ee64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ef35830989ff4e16ac9a0f79c8266301":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"71d1478a7ad2402d8952c2c9a318fb86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"970a84a453e5451990988fe80c652cfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f85963325a024836a72d43603b6a0e94":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f22372cbcdcc49a58fa469aac5544c33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"634377a84c854a988f6da598f6e90ba1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_5970b4c489d147b9bfd63fe5c6c19493","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c3c86c38a66544c499eb35561f9f7a65","IPY_MODEL_895d28cf6410434cba2345c110f7a3fc","IPY_MODEL_58e9facfe202410e8816c99e64489f35"]}},"5970b4c489d147b9bfd63fe5c6c19493":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c3c86c38a66544c499eb35561f9f7a65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0d9f82a8af744db99d0acd62ab8c92b0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d7e36805a1c14bcbb0dffd5907e6319e"}},"895d28cf6410434cba2345c110f7a3fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f8b01ed97fae46bfbcbc8e5a416d94c2","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1648877,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1648877,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cb59e3547e3943a0820c4b1c42bd15ae"}},"58e9facfe202410e8816c99e64489f35":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_af748c59beb54f0ab2af2a1ea8bb1ca0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1649664/? [00:00&lt;00:00, 4159424.97it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28c6ff7f6cf748d6a2aab9470186444b"}},"0d9f82a8af744db99d0acd62ab8c92b0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d7e36805a1c14bcbb0dffd5907e6319e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8b01ed97fae46bfbcbc8e5a416d94c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cb59e3547e3943a0820c4b1c42bd15ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"af748c59beb54f0ab2af2a1ea8bb1ca0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"28c6ff7f6cf748d6a2aab9470186444b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c6be0ec977804974b925bd40f8dad100":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0d7d7ffed6394396ab8d0b4fcc34115c","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f6851d5f196342ba982f6b06018155fb","IPY_MODEL_8a7c6610df224d0c9d027edcfb18ed3c","IPY_MODEL_ef875f9a0cf54c3dab3b2a243730dac1"]}},"0d7d7ffed6394396ab8d0b4fcc34115c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f6851d5f196342ba982f6b06018155fb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c216b5b9a1064a2a892386c9c3e1139e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_53b6621527c24f46b19bb057abc8989d"}},"8a7c6610df224d0c9d027edcfb18ed3c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_e4cc198ae79c4386b630f8d5c33f656e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":4542,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":4542,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_13173584e37d4f48b7d3e75d7d06cb37"}},"ef875f9a0cf54c3dab3b2a243730dac1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_709bfdfe7b2f453c8b1851c196170000","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5120/? [00:00&lt;00:00, 131036.01it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_26815b7dbf2b4081984013053c099679"}},"c216b5b9a1064a2a892386c9c3e1139e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"53b6621527c24f46b19bb057abc8989d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e4cc198ae79c4386b630f8d5c33f656e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"13173584e37d4f48b7d3e75d7d06cb37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"709bfdfe7b2f453c8b1851c196170000":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"26815b7dbf2b4081984013053c099679":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"0SI_AEBcqs_S"},"source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"tR1wvEVM6fTG"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"vESOoKZe7DBx"},"source":["At the end of the experiment, you will be able to:\n","\n","\n","* Classify the MNIST dataset using MLP and then quantize the weights of the network \n","* Understand how quantization reduces the storage needs of the network"]},{"cell_type":"code","metadata":{"id":"lfk8109wnqAl","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1632892848915,"user_tz":-330,"elapsed":585,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"ed543a74-c627-4098-a7d3-111611fb16ce"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/March31/uniform_nonuniform_quantization.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/March31/uniform_nonuniform_quantization.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"o3b5-r1S7Fkh"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"tIcN-gMa7KOW"},"source":["\n","###Description\n","\n","\n","1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, which means each digit occurs 6000 times in the training set and 1000 times in the testing set (approximately). \n","2. Each image is Size Normalized and Centered. \n","3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n","4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n","\n","###History\n","\n","Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90’s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n","\n","Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. \n","\n","It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n","\n","\n","###Challenges\n","\n","Now, if you notice the images below, you will find that between 2 characters there are always certain similarities and differences. To teach a machine to recognize these patterns and identify the correct output is intriguing.\n","\n","![altxt](https://www.researchgate.net/profile/Radu_Tudor_Ionescu/publication/282924675/figure/fig3/AS:319968869666820@1453297931093/A-random-sample-of-6-handwritten-digits-from-the-MNIST-data-set-before-and-after.png)\n","\n","Hence, all these challenges make this a good problem to solve in Machine Learning."]},{"cell_type":"markdown","metadata":{"id":"mrRu_oSI7MLm"},"source":["## Domain Information"]},{"cell_type":"markdown","metadata":{"id":"OZJqmAkO7Qm2"},"source":["Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n","\n","![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n","\n","The experiment handles a subset of text recognition, namely recognizing the 10 numerals (0 to 9) from scanned images.\n"]},{"cell_type":"markdown","metadata":{"id":"W_onQyDA7Uoc"},"source":["## AI / ML Technique"]},{"cell_type":"markdown","metadata":{"id":"Qb2Jfqj67XEr"},"source":["### Quantization for Image Classification:\n","\n","In this experiment, you train a neural network on the dataset to classify the images and then reduce the storage requirements of the network by quantizing the weights of the network using compression.\n","\n","Neural network models can take up a lot of space on disk where almost all of that space is taken up by the weights of the neural connections, which are often millions in number in a single model. As the weights are all slightly different floating point numbers, simple compression formats like zip don't compress them well. Quantization is a network compression technique that is used to save the storage for the many parameters of the network by compressing the weights. The weights intially are represented as 8-bit values, so we are using 2 * 8 = 16 in storage. If we are compressing the weights to 1-bit values, we are storing only 1 * 8 = 8 in storage thus reducing our storage needs by half.  Depending on how the weight space is distributed into clusters, there are two types of quantization techniques:\n","\n","1. **Uniform Quantization**: The cluster heads are uniformly spaced.\n","2. **Non-uniform Quantization**:  The cluster heads are non - uniformly spaced using K - Means clustering.\n"," \n","You will understand these in detail while working on the code in the experiment. "]},{"cell_type":"markdown","metadata":{"id":"ELePreM3QBkb"},"source":["### Importing required packages\n","\n","* First, we import pytorch, the deep learning library which we’ll be using, and torchvision, which provides our dataset and data transformations. \n","\n","\n","* We also import torch.nn (pytorch’s neural network library)\n","\n"]},{"cell_type":"code","metadata":{"id":"Mk4-H3VnNSBi"},"source":["import torch \n","import torch.nn as nn\n","\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","\n","from sklearn.cluster import KMeans\n","\n","# Importing python packages\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-dE4fUykNSBm"},"source":["### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"FKpyM4zzNSBm"},"source":["num_epochs = 5\n","batch_size = 100\n","learning_rate = 0.001\n","use_reg = True"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbm-8HxIqvxR"},"source":["### Initializing CUDA\n","\n"]},{"cell_type":"code","metadata":{"id":"x9E-00Slqs_k"},"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mJH3yYJKNSBp"},"source":["### Downloading the MNIST dataset"]},{"cell_type":"code","metadata":{"id":"3inDH-alNSBq","colab":{"base_uri":"https://localhost:8080/","height":444,"referenced_widgets":["cf0a6631c06445ab9f5d23447d349104","915f583868eb4ab8a5c496ddab841dc3","f861952ab53e4216be3ae7cec03002c9","ba3915f002164cb58236a103a23e9bed","2d4849de1deb4cefb0466f7ab67c018c","a19c74c0944c4fadbbfd95cab83369c5","ac187a28d4ac437696e23c0372c4d587","9d764ae672464c08ae71c83dce807a1a","b9547090081743bdb34c3bfd3298a762","a09ae7e7ad6b42018a74f91a368c8a8d","91c7f7efceaf4a45885c82e5f5b4dc29","7a45b502b184416ab5543280d978bd42","d0441cfd23ce4471bcd6120536e11feb","19b8b56079f44180b2fbc941ade9ebfb","8c869cc41f234d92af9770416998add9","404fd037739042b19fcb8382f2206aea","8a6bed4574c3471fb6811a0e9801ee64","ef35830989ff4e16ac9a0f79c8266301","71d1478a7ad2402d8952c2c9a318fb86","970a84a453e5451990988fe80c652cfd","f85963325a024836a72d43603b6a0e94","f22372cbcdcc49a58fa469aac5544c33","634377a84c854a988f6da598f6e90ba1","5970b4c489d147b9bfd63fe5c6c19493","c3c86c38a66544c499eb35561f9f7a65","895d28cf6410434cba2345c110f7a3fc","58e9facfe202410e8816c99e64489f35","0d9f82a8af744db99d0acd62ab8c92b0","d7e36805a1c14bcbb0dffd5907e6319e","f8b01ed97fae46bfbcbc8e5a416d94c2","cb59e3547e3943a0820c4b1c42bd15ae","af748c59beb54f0ab2af2a1ea8bb1ca0","28c6ff7f6cf748d6a2aab9470186444b","c6be0ec977804974b925bd40f8dad100","0d7d7ffed6394396ab8d0b4fcc34115c","f6851d5f196342ba982f6b06018155fb","8a7c6610df224d0c9d027edcfb18ed3c","ef875f9a0cf54c3dab3b2a243730dac1","c216b5b9a1064a2a892386c9c3e1139e","53b6621527c24f46b19bb057abc8989d","e4cc198ae79c4386b630f8d5c33f656e","13173584e37d4f48b7d3e75d7d06cb37","709bfdfe7b2f453c8b1851c196170000","26815b7dbf2b4081984013053c099679"]},"executionInfo":{"status":"ok","timestamp":1632892865373,"user_tz":-330,"elapsed":1883,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"dbc97e7d-206e-4f1d-d8bf-2e496f35d5c7"},"source":["train_dataset = datasets.MNIST(root='../data/',\n","                            train=True, \n","                            transform=transforms.ToTensor(),\n","                            download=True)\n","\n","test_dataset = datasets.MNIST(root='../data/',\n","                           train=False, \n","                           transform=transforms.ToTensor())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cf0a6631c06445ab9f5d23447d349104","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/9912422 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7a45b502b184416ab5543280d978bd42","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/28881 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"634377a84c854a988f6da598f6e90ba1","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/1648877 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c6be0ec977804974b925bd40f8dad100","version_minor":0,"version_major":2},"text/plain":["  0%|          | 0/4542 [00:00<?, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"3cIqGefQNSBw"},"source":["### Dataloader"]},{"cell_type":"code","metadata":{"id":"_3Nxm0bXNSBw"},"source":["train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n","                                          batch_size=batch_size, \n","                                          shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MrX-dz4fNSB1"},"source":["### Define the network"]},{"cell_type":"code","metadata":{"id":"DYXjWlJsNSB2"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.layer1 = nn.Sequential(\n","            nn.Conv2d(1, 16, kernel_size=5, padding=2),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU())\n","        self.layer2 = nn.Sequential(\n","            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(16),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        self.layer3 = nn.Sequential(\n","            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n","            nn.BatchNorm2d(32),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2))\n","        self.fc1 = nn.Linear(7*7*32, 300)\n","        self.fc2 = nn.Linear(300, 10)\n","        \n","    def forward(self, x):\n","        out = self.layer1(x)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = out.view(out.size(0), -1)\n","        out = self.fc1(out)\n","        out = self.fc2(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ne2DrrXiNSB5"},"source":["<b>The below function is called to reinitialize the weights of the network and define the required loss criterion and the optimizer.</b> "]},{"cell_type":"code","metadata":{"id":"7ui3ChcWNSB6"},"source":["def reset_model():\n","    net = Net()\n","    net = net.to(device)\n","\n","    # Loss and Optimizer\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","    return net, criterion, optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EfBSn8rJNSB8"},"source":["### Initializing the model"]},{"cell_type":"code","metadata":{"id":"7vLzLbbnNSB9"},"source":["net, criterion, optimizer = reset_model()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZyXY7Oq6NSCB"},"source":["### Defining a L1 Regularizer"]},{"cell_type":"code","metadata":{"id":"zMLxnmDkNSCC"},"source":["def l1_regularizer(net, loss, beta):\n","    l1_crit = nn.L1Loss(size_average=False)\n","    reg_loss = 0\n","    for param in net.parameters():\n","        target = (torch.FloatTensor(param.size()).zero_()).to(device)\n","        reg_loss += l1_crit(param, target)\n","        \n","    loss += beta * reg_loss\n","    return loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3NogS9pZNSCF"},"source":["### Training function"]},{"cell_type":"code","metadata":{"id":"6hUX_GntNSCG"},"source":["# Train the Model\n","\n","def training(net, reset = True):\n","    if reset == True:\n","        net, criterion, optimizer = reset_model()\n","    else:\n","        criterion = nn.CrossEntropyLoss()\n","        optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n","    \n","    net.train()\n","    for epoch in range(num_epochs):\n","        total_loss = 0\n","        accuracy = []\n","        for i, (images, labels) in enumerate(train_loader):\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            temp_labels = labels\n","          \n","            # Forward + Backward + Optimize\n","            optimizer.zero_grad()\n","            outputs = net(images)\n","            loss = criterion(outputs, labels)\n","\n","            if use_reg == True :\n","                loss = l1_regularizer(net, loss, beta=0.001)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            total_loss += loss.item()\n","            _, predicted = torch.max(outputs.data, 1)\n","            correct = (predicted == temp_labels).sum().item()\n","            accuracy.append(correct/float(batch_size))\n","\n","        print('Epoch: %d, Loss: %.4f, Accuracy: %.4f' %(epoch+1,total_loss, (sum(accuracy)/float(len(accuracy)))))\n","    \n","    return net"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_5fha93LNSCI"},"source":["### Testing function"]},{"cell_type":"code","metadata":{"id":"Kj1AAWuBNSCL"},"source":["# Test the Model\n","def testing(net):\n","    net.eval() \n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","       \n","        outputs = net(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","    print('Test Accuracy of the network on the 10000 test images: %.2f %%' % (100.0 * correct / total))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4WRXM1YNSCP"},"source":["### Training and testing the network"]},{"cell_type":"code","metadata":{"id":"qSfX2-vFNSCP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632892968881,"user_tz":-330,"elapsed":93884,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"6a7c9e9a-e40e-4463-b99f-dcdeecde3e12"},"source":["reset = False\n","net = training(net, reset)\n","testing(net)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 697.0306, Accuracy: 0.9520\n","Epoch: 2, Loss: 277.8888, Accuracy: 0.9753\n","Epoch: 3, Loss: 227.4271, Accuracy: 0.9780\n","Epoch: 4, Loss: 203.2757, Accuracy: 0.9805\n","Epoch: 5, Loss: 189.2433, Accuracy: 0.9812\n","Test Accuracy of the network on the 10000 test images: 98.31 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"b8kc_CbbNSCR"},"source":["### Uniform Quantization\n","\n","The simplest motivation for quantization is to shrink file sizes by storing the min and max for each layer, and then compressing each float value to an eight-bit integer representing the closest real number in a linear set of 256 within the range.\n","\n","In the function below we send 8 bits as input which ressembles that the weights of the network should be represented with only 8 bits while storing to disk. In other words we use only 2^8 or 256 clusters. Hence each weight is represented as a 8-bit integer between 0-255.\n","\n","Thus before using the weights during test time they need to be projected into the original weight space by using the following equation:\n","\n","$$\n","W_{i} = min + \\dfrac{max-min}{255}*W_{index}\n","$$"]},{"cell_type":"code","metadata":{"id":"bkD7PGOCNSCT"},"source":["def uniform_quantize(weight, bits):\n","    print('-------------------------LAYER---------------------------')\n","    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weight))))\n","    n_clusters = 2**bits\n","    \n","    maxim = np.amax(weight)\n","    minim = np.amin(weight)\n","    step = (maxim-minim)/(n_clusters - 1)\n","\n","    clusters=[]\n","\n","    for i in range(0,n_clusters):\n","        clusters.append(minim)\n","        minim += step\n","\n","    for i in range(0,len(weight)):\n","        dist = (clusters-weight[i])**2     \n","        weight[i] = clusters[np.argmin(dist)]\n","        \n","    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weight))))\n","    \n","    return weight  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWEIuI9wNSCU"},"source":["### Uniform Quantization on the weights and biases\n","\n","Different number of bits can be used for representing the weights and biases. The exact number of bits to use is a design choice and may depend on the complexity of the task at hand since using too less number of bits can result in poor performance. Here, we use 8 bits for quantizing the weights and 1 bit for the biases."]},{"cell_type":"code","metadata":{"id":"JpJ1C75qNSCU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632892994714,"user_tz":-330,"elapsed":25320,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"1886d72e-49d2-43e1-d6fa-4b3be4069229"},"source":["for m in net.modules():\n","    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n","        temp_weight = m.weight.data.cpu().numpy()\n","        dims = temp_weight.shape\n","        temp_weight = temp_weight.flatten()\n","        temp_weight = uniform_quantize(temp_weight, 8)\n","        temp_weight=np.reshape(temp_weight,dims)\n","        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n","        \n","        temp_bias = m.bias.data.cpu().numpy()\n","        dims = temp_bias.shape\n","        temp_bias = temp_bias.flatten()\n","        temp_bias = uniform_quantize(temp_bias, 1)\n","        temp_bias = np.reshape(temp_bias,dims)\n","        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 400\n","Number of unique parameters after quantization: 127\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 14\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 2304\n","Number of unique parameters after quantization: 167\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 15\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 16\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 4607\n","Number of unique parameters after quantization: 112\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 28\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 32\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 468979\n","Number of unique parameters after quantization: 163\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 300\n","Number of unique parameters after quantization: 2\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 3000\n","Number of unique parameters after quantization: 101\n","-------------------------LAYER---------------------------\n","Number of unique parameters before quantization: 10\n","Number of unique parameters after quantization: 2\n"]}]},{"cell_type":"markdown","metadata":{"id":"lQsNHoNyNSCW"},"source":["Now that we have replaced the weight matrix with the approximated weight of the nearest cluster, we can test the network with the modified weights."]},{"cell_type":"code","metadata":{"id":"3qYOc_42NSCX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632892995988,"user_tz":-330,"elapsed":1279,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"eef534b0-b0ee-4f56-a186-3cb33d311431"},"source":["testing(net)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy of the network on the 10000 test images: 98.28 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"-sCB4SNmNSCa"},"source":["### Non-uniform quantization\n","\n","We have seen in the previous method that we divide the weight space into equally partitioned cluster heads. However, instead of forcing the cluster heads to be equally spaced it would make more sense to learn them. A common and obvious practice is to learn the weight space as a distribution of cluster centers using k-means clustering. Here, we define a function to perform k-means to the weight values.\n","\n","$$\n","min\\sum_{i}^{mn}\\sum_{j}^{k}||w_{i}-c_{j}||_{2}^{2}\n","$$"]},{"cell_type":"code","metadata":{"id":"GUDAcctdNSCb"},"source":["num_clusters = 8\n","kmeans = KMeans(n_clusters=num_clusters, random_state=0,  max_iter=500, precompute_distances='auto', verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2wWh5lbNSCe"},"source":["def non_uniform_quantize(weights):\n","    print(\"---------------------------Layer--------------------------------\")\n","    print(\"Number of unique parameters before quantization: \" + str(len(np.unique(weights))))\n","    weights = np.reshape(weights,[weights.shape[0],1])\n","    print(weights.shape)\n","    kmeans_fit = kmeans.fit(weights)\n","    clusters = kmeans_fit.cluster_centers_\n","    \n","    for i in range(0,len(weights)):\n","        dist = (clusters-weights[i])**2     \n","        weights[i] = clusters[np.argmin(dist)]\n","        \n","    print(\"Number of unique parameters after quantization: \" + str(len(np.unique(weights))))\n","    \n","    return weights  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"luzXZINVNSCg"},"source":["We reset the model and train the network since we had earlier done uniform quantization on the weight already."]},{"cell_type":"code","metadata":{"id":"IYdxVUsvNSCj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632893091637,"user_tz":-330,"elapsed":95653,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"6af13ede-de0a-44ca-9004-34d8888a54da"},"source":["reset = True\n","net = training(net, reset)\n","testing(net)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 688.5267, Accuracy: 0.9527\n","Epoch: 2, Loss: 257.5833, Accuracy: 0.9762\n","Epoch: 3, Loss: 212.1793, Accuracy: 0.9787\n","Epoch: 4, Loss: 190.7348, Accuracy: 0.9805\n","Epoch: 5, Loss: 175.8892, Accuracy: 0.9827\n","Test Accuracy of the network on the 10000 test images: 98.14 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"WXhYLriUNSCm"},"source":["Non-Uniform quantization on the weights and biases"]},{"cell_type":"code","metadata":{"id":"ygTmNfWsNSCm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632893102795,"user_tz":-330,"elapsed":11165,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"0409a995-a09d-4517-fc23-fc50a3045532"},"source":["for m in net.modules():\n","    if isinstance(m,nn.Conv2d) or isinstance(m,nn.BatchNorm2d) or isinstance(m,nn.Linear):\n","        temp_weight = m.weight.data.cpu().numpy()\n","        dims = temp_weight.shape\n","        temp_weight = temp_weight.flatten()\n","        temp_weight = non_uniform_quantize(temp_weight)\n","        temp_weight=np.reshape(temp_weight,dims)\n","        m.weight.data = (torch.FloatTensor(temp_weight).cuda())\n","        \n","        temp_bias = m.bias.data.cpu().numpy()\n","        dims = temp_bias.shape\n","        temp_bias = temp_bias.flatten()\n","        temp_bias = non_uniform_quantize(temp_bias)\n","        temp_bias = np.reshape(temp_bias,dims)\n","        m.bias.data = (torch.FloatTensor(temp_bias).cuda())"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 400\n","(400, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 2304\n","(2304, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 16\n","(16, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 4608\n","(4608, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 32\n","(32, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 469011\n","(470400, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 300\n","(300, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 3000\n","(3000, 1)\n","Number of unique parameters after quantization: 8\n","---------------------------Layer--------------------------------\n","Number of unique parameters before quantization: 10\n","(10, 1)\n","Number of unique parameters after quantization: 8\n"]}]},{"cell_type":"code","metadata":{"id":"XAZkRYg9NSCp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632893104360,"user_tz":-330,"elapsed":1572,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"5e92fa2d-5a40-4826-eda0-4acd5f720aa5"},"source":["testing(net)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Accuracy of the network on the 10000 test images: 98.13 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"AZME62IFNSCr"},"source":["### Retraining the network\n","\n","Here you see that 8 clusters are too less in order to maintain the network at the same accuracy. One of the solutions is to retrain the network. This helps the other weights to compensate for those weights which on being rounded off to the nearest cluster center have resulted in a drop in performance. Accuracy can be recovered significantly on retraining the network and then non-uniformly quantizing the weights again."]},{"cell_type":"code","metadata":{"id":"hDGjonRNNSCt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1632893200245,"user_tz":-330,"elapsed":95889,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"922cb8a5-8ef7-426b-e132-6564d49399b4"},"source":["reset = False\n","net = training(net, reset)\n","# Perform non-uniform quantization\n","testing(net)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch: 1, Loss: 171.6326, Accuracy: 0.9827\n","Epoch: 2, Loss: 160.5390, Accuracy: 0.9840\n","Epoch: 3, Loss: 154.0688, Accuracy: 0.9847\n","Epoch: 4, Loss: 149.0424, Accuracy: 0.9846\n","Epoch: 5, Loss: 145.3403, Accuracy: 0.9844\n","Test Accuracy of the network on the 10000 test images: 98.41 %\n"]}]},{"cell_type":"markdown","metadata":{"id":"wNL2AuOeNSCx"},"source":["### References\n","\n","1. https://arxiv.org/pdf/1412.6115.pdf"]}]}