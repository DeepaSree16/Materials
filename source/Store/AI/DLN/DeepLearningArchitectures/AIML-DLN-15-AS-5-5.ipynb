{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-15-AS-2-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-zSWyuZTlYS1"},"source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"oU2MBx7JtTyP"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"Ot3pfe3AtWaN"},"source":["At the end of the experiment, you will be able to:\n","\n","* reduce overfitting using regularization method"]},{"cell_type":"code","metadata":{"id":"VDtOUGiIbwT2","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":501},"executionInfo":{"status":"ok","timestamp":1627807484615,"user_tz":-330,"elapsed":604,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"603d724c-ea1d-44d1-ab01-99b9348c10e8"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/Walkthrough_Overfitting_Ants_Bees.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/Walkthrough_Overfitting_Ants_Bees.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"35RBpSDUtYUm"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"SVIILnxWtcKA"},"source":["### Description\n","\n","For this experiment we have choosen a dataset which is subset of Imagenet. We have taken images belonging to ants and bees. The dataset contains 244 training images and 153 validation images. \n","\n","![alt text]( https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/15.png)\n","\n"]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/hymenoptera_data.zip\n","! unzip /content/hymenoptera_data.zip"],"metadata":{"id":"AQAvWOdvnfaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEeo3n0ElYS_"},"source":["### Importing the required packages"]},{"cell_type":"code","metadata":{"id":"6Ydf-Vi2P2r6"},"source":["import torch\n","from torch import nn\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch import optim\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wKWUSRUQuwmW"},"source":["### Defining Transformation\n"]},{"cell_type":"code","metadata":{"id":"zPjq8djgJY7v"},"source":["image_size = (128,128)\n","# Define Transformation for an image\n","transformations = transforms.Compose([\n","                                transforms.Resize(image_size), \n","                                transforms.Grayscale(),\n","                                transforms.ToTensor(), \n","                                transforms.Normalize((0.5,), (0.5,))\n","                                ])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SM5nwIvuu2Nw"},"source":["### Data Loading\n","\n","\n","**torch.utils.data.DataLoader** class represents a Python iterable over a dataset, with following features.\n","\n","1. Batching the data\n","2. Shuffling the data\n","\n","\n","The batches of train and test data are provided via data loaders that provide iterators over the datasets to train our models."]},{"cell_type":"code","metadata":{"id":"1OxC0CShJZZ7"},"source":["train_set = datasets.ImageFolder('/content/hymenoptera_data/train', transform = transformations)\n","trainloader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True)\n","\n","val_set = datasets.ImageFolder('/content/hymenoptera_data/val',transform=transformations)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=100, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ule2ESb15b_g","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627813671560,"user_tz":-330,"elapsed":8,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"0f9b7400-1823-487d-e23a-ec251efc5e9c"},"source":["val_set"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Dataset ImageFolder\n","    Number of datapoints: 153\n","    Root location: /content/hymenoptera_data/val\n","    StandardTransform\n","Transform: Compose(\n","               Resize(size=(128, 128), interpolation=bilinear, max_size=None, antialias=None)\n","               Grayscale(num_output_channels=1)\n","               ToTensor()\n","               Normalize(mean=(0.5,), std=(0.5,))\n","           )"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"pEYZoawOppFN"},"source":["### Defining the Architecture"]},{"cell_type":"markdown","metadata":{"id":"TEhqiDSmfE3C"},"source":["Neural Networks are inherited from the nn.Module class.\n","\n","Now let us define a neural network. Here we are using two functions \\__init__ and forward function.\n","\n","In the \\__init__  function, we define the layers using the provided modules from the nn package. The forward function is called on the Neural Network for a set of inputs, and it passes that input through the different layers that have been defined. \n","\n","\n"]},{"cell_type":"code","metadata":{"id":"oEJHXJD0jTwD"},"source":["class Model(nn.Module):\n","    def __init__(self):\n","        super(Model, self).__init__()\n","\n","        self.linear1 = nn.Linear(16384,4096)\n","        self.linear2 = nn.Linear(4096,1024)\n","        self.linear3 = nn.Linear(1024,256)\n","        self.linear4 = nn.Linear(256,10)\n","        self.linear5 = nn.Linear(10,2)\n","    \n","    def forward(self, x):\n","        out = x.view(x.shape[0],-1)\n","        out = self.linear1(out)\n","        out = self.linear2(out)\n","        out = self.linear3(out)\n","        out = self.linear4(out)\n","        out = self.linear5(out)\n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nw3kD1Kjqvx_"},"source":["### Calling the instances of the network\n","\n","Let us declare an object of class model, and make it a CUDA model if CUDA is available:"]},{"cell_type":"code","metadata":{"id":"F_5p3NM4S9CR"},"source":["# Instantiate the model\n","device = torch.device(\"cuda\")\n","model = Model().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr = 0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F75Z3ABdqvyY"},"source":["### Training and Testing the model\n","\n","In Training Phase, we iterate over a batch of images in the train_loader. For each batch, we perform  the following steps:\n","\n","* First we zero out the gradients using zero_grad()\n","\n","* We pass the data to the model i.e. we perform forward pass by calling the forward()\n","\n","* We calculate the loss using the actual and predicted labels\n","\n","* Perform Backward pass using backward() to update the weights"]},{"cell_type":"code","metadata":{"id":"kG_e4hjdrgs7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627813738933,"user_tz":-330,"elapsed":48244,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"2aa232a7-230f-4eae-84c5-b8e7c2c765d9"},"source":["# No of Epochs\n","epoch = 20\n","\n","# keeping the network in train mode\n","model.train()\n","train_losses,  train_accuracy = [], []\n","val_losses , val_accuracy = [], []\n","\n","# Loop for no of epochs\n","for e in range(epoch):\n","    train_loss = 0\n","    correct = 0\n","\n","    # Iterate through all the batches in each epoch\n","    for images, labels in trainloader:\n","\n","      # Convert the image and label to gpu for faster execution\n","      images = images.to(device)\n","      labels = labels.to(device)\n","\n","      # Zero the parameter gradients\n","      optimizer.zero_grad()\n","\n","      # Passing the data to the model (Forward Pass)\n","      outputs = model(images)\n","\n","      # Calculating the loss\n","      loss = criterion(outputs, labels)\n","      train_loss += loss.item()\n","\n","      # Performing backward pass (Backpropagation)\n","      loss.backward()\n","\n","      # optimizer.step() updates the weights accordingly\n","      optimizer.step()\n","\n","      # Accuracy calculation\n","      _, predicted = torch.max(outputs, 1)\n","      correct += (predicted == labels).sum().item()\n","      \n","    val_loss = 0\n","    val_correct = 0\n","    with torch.no_grad():\n","        # Loop through all of the validation set\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            val_output = model(images)                                                                  \n","            val_loss += criterion(val_output, labels)             \n","            _, predicted = torch.max(val_output, 1)\n","            val_correct += (predicted == labels).sum()\n","\n","    train_losses.append(train_loss/len(train_set))\n","    val_losses.append(val_loss/len(val_set))\n","    train_accuracy.append(100 * correct/len(train_set))\n","    val_accuracy.append(100 * val_correct/len(val_set))\n","    print('epoch: {}, Train Loss:{:.6f} Validation Loss {:.6f} Train Accuracy: {:.2f}, Validation accuracy {:.2f} '.format(e+1,train_losses[-1], val_losses[-1], train_accuracy[-1], val_accuracy[-1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch: 1, Train Loss:0.192346 Validation Loss 0.049841 Train Accuracy: 56.56, Validation accuracy 59.48 \n","epoch: 2, Train Loss:0.070110 Validation Loss 0.103071 Train Accuracy: 52.87, Validation accuracy 45.10 \n","epoch: 3, Train Loss:0.051702 Validation Loss 0.032211 Train Accuracy: 50.41, Validation accuracy 49.67 \n","epoch: 4, Train Loss:0.022001 Validation Loss 0.018249 Train Accuracy: 47.54, Validation accuracy 64.71 \n","epoch: 5, Train Loss:0.026950 Validation Loss 0.027135 Train Accuracy: 63.52, Validation accuracy 55.56 \n","epoch: 6, Train Loss:0.020776 Validation Loss 0.023206 Train Accuracy: 62.70, Validation accuracy 53.59 \n","epoch: 7, Train Loss:0.019316 Validation Loss 0.017162 Train Accuracy: 59.43, Validation accuracy 64.05 \n","epoch: 8, Train Loss:0.015065 Validation Loss 0.015178 Train Accuracy: 65.16, Validation accuracy 45.10 \n","epoch: 9, Train Loss:0.010211 Validation Loss 0.012411 Train Accuracy: 60.25, Validation accuracy 60.13 \n","epoch: 10, Train Loss:0.007451 Validation Loss 0.011305 Train Accuracy: 72.54, Validation accuracy 54.90 \n","epoch: 11, Train Loss:0.006522 Validation Loss 0.009649 Train Accuracy: 73.36, Validation accuracy 62.75 \n","epoch: 12, Train Loss:0.006339 Validation Loss 0.011108 Train Accuracy: 75.82, Validation accuracy 53.59 \n","epoch: 13, Train Loss:0.005636 Validation Loss 0.011495 Train Accuracy: 78.28, Validation accuracy 54.90 \n","epoch: 14, Train Loss:0.004911 Validation Loss 0.012601 Train Accuracy: 80.33, Validation accuracy 52.29 \n","epoch: 15, Train Loss:0.004224 Validation Loss 0.013761 Train Accuracy: 82.79, Validation accuracy 51.63 \n","epoch: 16, Train Loss:0.003492 Validation Loss 0.015183 Train Accuracy: 87.70, Validation accuracy 50.98 \n","epoch: 17, Train Loss:0.003373 Validation Loss 0.015278 Train Accuracy: 87.30, Validation accuracy 52.94 \n","epoch: 18, Train Loss:0.002597 Validation Loss 0.018849 Train Accuracy: 90.98, Validation accuracy 42.48 \n","epoch: 19, Train Loss:0.001961 Validation Loss 0.017949 Train Accuracy: 92.62, Validation accuracy 50.33 \n","epoch: 20, Train Loss:0.001865 Validation Loss 0.018444 Train Accuracy: 94.26, Validation accuracy 49.02 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"R1338yNobnnZ"},"source":["### Data Augmentation\n","\n","\n","\n","Diversity of data and a larger dataset is the easiest way to avoid overfitting of the model. Data augmentation allows you to increase the size of your dataset by performing processes like flipping, cropping, rotation, scaling and translation on the existing images. Data augmentation not only increases the dataset size but also exposes the model to different angles and lighting and reduces the bias in the dataset, thus avoiding chances of overfitting. \n","\n","Added two more transformations to the original data.\n","\n","\n","*   Applied random rotation of $45^o$ using **`transforms.RandomRotation`**\n","*   Applied vertical flip to the images using **`transforms.RandomVerticalFlip()`**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"n7hR9B_pbm2g"},"source":["image_size = (128,128)\n","transformations = transforms.Compose([\n","                                transforms.Resize(image_size), \n","                                transforms.Grayscale(),\n","                                transforms.RandomRotation(45),\n","                                transforms.RandomVerticalFlip(),\n","                                transforms.ToTensor(), \n","                                transforms.Normalize((0.5,), (0.5,)),\n","                                ])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0HdQKsm-bm2i"},"source":["train_set = datasets.ImageFolder('/content/hymenoptera_data/train', transform = transformations)\n","trainloader = torch.utils.data.DataLoader(train_set, batch_size=100, shuffle=True, num_workers=8)\n","\n","val_set = datasets.ImageFolder('/content/hymenoptera_data/val',transform=transformations)\n","val_loader = torch.utils.data.DataLoader(val_set, batch_size=100, shuffle=True, num_workers=8)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tHgEW9-RmdL1"},"source":["#### Regularization\n","\n","Dropouts: Regularization techniques prevent the model from overfitting. Dropout prevents overfitting by modifying the network itself. Every neuron apart from the ones in the output layer is assigned a probability p of being temporarily ignored from calculations. p is also called dropout rate and is initialized to 0.2. where it randomly zeroes some of the elements of the input tensor with given probability p"]},{"cell_type":"markdown","metadata":{"id":"0ZAiX592ZeoG"},"source":["### Optimize the Architecture"]},{"cell_type":"code","metadata":{"id":"Et6X_fnFGR3t"},"source":["class Optimized_Model(nn.Module):\n","    def __init__(self):\n","        super(Optimized_Model, self).__init__()\n","\n","        self.linear1 = nn.Linear(16384,4096)\n","        self.linear2 = nn.Linear(4096,1024)\n","        self.linear3 = nn.Linear(1024,256)\n","        self.linear4 = nn.Linear(256,10)\n","        self.linear5 = nn.Linear(10,2)\n","        self.dropout = nn.Dropout(0.2)\n","\n","    def forward(self, x):\n","        out = x.view(x.shape[0],-1)\n","        out = self.linear1(out)\n","        out = self.linear2(out)\n","        out = self.linear3(out)\n","        out = self.linear4(out)\n","        out = self.dropout(self.linear5(out))       \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Au6wjEGWZADQ"},"source":["#### Initialize the optimized model"]},{"cell_type":"code","metadata":{"id":"kRMMDeiFWlOY"},"source":["# Instantiate the model\n","device = torch.device(\"cuda\")\n","model2 = Optimized_Model().to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model2.parameters(), lr = 0.001)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xEmKN7nRvDwm"},"source":["### Training the optimized model\n","\n","In Training Phase, we iterate over a batch of images in the train_loader. For each batch, we perform  the following steps:\n","\n","* First we zero out the gradients using zero_grad()\n","\n","* We pass the data to the model i.e. we perform forward pass by calling the forward()\n","\n","* We calculate the loss using the actual and predicted labels\n","\n","* Perform Backward pass using backward() to update the weights"]},{"cell_type":"code","metadata":{"id":"gt_oZCN3P8rd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1627813797340,"user_tz":-330,"elapsed":57621,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"b836c252-815f-4709-d2ee-040debdf4218"},"source":["# No of Epochs\n","epoch = 20\n","\n","model2.train()\n","\n","train_losses_opt,  train_accuracy_opt = [], []\n","val_losses_opt , val_accuracy_opt = [], []\n","    \n","for e in range(epoch):\n","    otrain_loss = 0\n","    ocorrect = 0\n","    # Iterate through all the batches in each epoch\n","    for images, labels in trainloader:\n","      \n","      # Convert the image and label to gpu for faster execution\n","      images = images.to(device)\n","      labels = labels.to(device)\n","      \n","      # Zero the parameter gradients\n","      optimizer.zero_grad()\n","      \n","      # Passing the data to the model (Forward Pass)\n","      outputs = model2(images)\n","      \n","      # Calculating the loss\n","      loss = criterion(outputs, labels)\n","      otrain_loss += loss.item()\n","\n","      # Performing backward pass (Backpropagation)\n","      loss.backward()\n","\n","      # optimizer.step() updates the weights accordingly\n","      optimizer.step()\n","\n","      # Accuracy calculation\n","      _, predicted = torch.max(outputs, 1)\n","      ocorrect += (predicted == labels).sum().item()\n","      \n","    oval_loss = 0\n","    oval_correct = 0\n","    with torch.no_grad():\n","        # Loop through all of the validation set\n","        for images, labels in val_loader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            val_output = model2(images)                                                                  \n","            oval_loss += criterion(val_output, labels)             \n","            _, predicted = torch.max(val_output, 1)\n","            oval_correct += (predicted == labels).sum()\n","\n","    train_losses_opt.append(otrain_loss/len(train_set))\n","    val_losses_opt.append(oval_loss/len(val_set))\n","    train_accuracy_opt.append(100 * ocorrect/len(train_set))\n","    val_accuracy_opt.append(100 * oval_correct/len(val_set))\n","    print('epoch: {}, Train Loss:{:.6f} Test Loss {:.6f} Train Accuracy: {:.2f}, Test accuracy {:.2f} '.format(e+1,train_losses_opt[-1], val_losses_opt[-1], train_accuracy_opt[-1], val_accuracy_opt[-1]))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["epoch: 1, Train Loss:0.116887 Test Loss 0.316489 Train Accuracy: 51.64, Test accuracy 45.75 \n","epoch: 2, Train Loss:0.166032 Test Loss 0.086978 Train Accuracy: 42.21, Test accuracy 60.78 \n","epoch: 3, Train Loss:0.086636 Test Loss 0.031068 Train Accuracy: 52.87, Test accuracy 46.41 \n","epoch: 4, Train Loss:0.036917 Test Loss 0.039632 Train Accuracy: 53.28, Test accuracy 44.44 \n","epoch: 5, Train Loss:0.042593 Test Loss 0.033842 Train Accuracy: 54.51, Test accuracy 50.33 \n","epoch: 6, Train Loss:0.034824 Test Loss 0.023810 Train Accuracy: 47.13, Test accuracy 47.71 \n","epoch: 7, Train Loss:0.032700 Test Loss 0.030458 Train Accuracy: 51.23, Test accuracy 56.21 \n","epoch: 8, Train Loss:0.027614 Test Loss 0.030973 Train Accuracy: 48.77, Test accuracy 52.29 \n","epoch: 9, Train Loss:0.031312 Test Loss 0.012686 Train Accuracy: 52.05, Test accuracy 43.14 \n","epoch: 10, Train Loss:0.018431 Test Loss 0.015030 Train Accuracy: 47.13, Test accuracy 45.75 \n","epoch: 11, Train Loss:0.011291 Test Loss 0.013979 Train Accuracy: 52.46, Test accuracy 53.59 \n","epoch: 12, Train Loss:0.011033 Test Loss 0.009516 Train Accuracy: 63.52, Test accuracy 52.94 \n","epoch: 13, Train Loss:0.011187 Test Loss 0.011822 Train Accuracy: 49.18, Test accuracy 58.82 \n","epoch: 14, Train Loss:0.010537 Test Loss 0.009871 Train Accuracy: 50.41, Test accuracy 52.29 \n","epoch: 15, Train Loss:0.009075 Test Loss 0.009732 Train Accuracy: 58.61, Test accuracy 50.98 \n","epoch: 16, Train Loss:0.008393 Test Loss 0.009734 Train Accuracy: 62.30, Test accuracy 50.33 \n","epoch: 17, Train Loss:0.008882 Test Loss 0.010131 Train Accuracy: 54.92, Test accuracy 47.71 \n","epoch: 18, Train Loss:0.008874 Test Loss 0.009395 Train Accuracy: 55.33, Test accuracy 51.63 \n","epoch: 19, Train Loss:0.008222 Test Loss 0.008828 Train Accuracy: 61.07, Test accuracy 54.25 \n","epoch: 20, Train Loss:0.008324 Test Loss 0.009053 Train Accuracy: 60.25, Test accuracy 51.63 \n"],"name":"stdout"}]}]}