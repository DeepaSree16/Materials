{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-13-AS-2-5.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"kMJOHiQHqvxF"},"source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"AiW1A8kokv1U"},"source":["## Learning Objective"]},{"cell_type":"markdown","metadata":{"id":"xfejswS7kyMw"},"source":["At the end of the experiment, you will be able to:\n","\n","* understand the output at each layer in convolutional neural network\n","* implement the ConvNet using PyTorch\n"]},{"cell_type":"code","metadata":{"id":"5Hs6I2Vgxfa4","cellView":"form"},"source":["#@title Experiment Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/Single_Image_Convolution_Walkthrough.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Parrot1.jpg\n"],"metadata":{"id":"ASLNjM0FcyGc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NLECiqpcdQTQ"},"source":["### Basic Pytorch packages\n","\n","**nn:**  This package provides an easy and modular way to build and train simple or complex neural networks.\n","\n","**torch.nn.functional:** This package includes non-linear functions like ReLu and sigmoid\n","\n","**torchvision:**  This package is used to load and prepare the dataset. Using this package we can perform/apply transformations on the input data.\n","\n","**transforms:**  This package is  used to perform preprocessing on images and operations sequentially. \n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Flq7IZ42fikI"},"source":["from PIL import Image             # Python Image library\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torchvision import transforms\n","import numpy as np\n","from matplotlib import pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3qnzMBCEOQEV"},"source":["### Loading Image using PIL Package"]},{"cell_type":"code","metadata":{"id":"9BWieny7fxMP"},"source":["image = Image.open('/content/Parrot1.jpg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4d5no4i4zJGJ"},"source":["# The height and width of the image\n","image.size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R9slFqNxHYEE"},"source":["**Defining Transformations**\n","\n","We will use `transform.compose()` to combine all the image transformations at one place.\n","\n","`transforms.ToTensor()` is used to convert the images into tensor values. \n","\n","`transforms.Resize(`) is used to resize the image."]},{"cell_type":"code","metadata":{"id":"MUN3mxOzOs7B"},"source":["data_transforms = transforms.Compose([\n","        transforms.Resize((256, 256)),\n","        transforms.ToTensor(),\n","        ])\n","img = data_transforms(image)\n","\n","# The original image is reshaped from (600, 600) to (256, 256)\n","img.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J8Vgguij01Ju"},"source":["# Printing the tensor values of an image\n","img"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KXKd_s57gnaG"},"source":["### Visualize the Image\n","\n","permute() function is used to re-arrange the order of dimension of the image. matplotlib works fine even without conversion to numpy array. But PyTorch Tensors (Image tensors) are channel first, so to use them with matplotlib need to reshape it:"]},{"cell_type":"code","metadata":{"id":"iP1PC6R6z-Wq"},"source":["print(img.shape)\n","print(img.permute(1, 2, 0).shape)\n","\n","plt.imshow(img.permute(1, 2, 0)) # In permute(1, 2, 0), 2 is the width; 1 is the height; 0 is the channel of the image\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g34bIWS1Z-CX"},"source":["### Building a Convolutional Neural Network with PyTorch\n","\n","ConvNet is a sequence of layers, and every layer of a ConvNet transforms one volume of activations to another through a differentiable function. We use three main types of layers to build ConvNet architectures: Convolutional Layer, Pooling Layer, and Fully-Connected Layer. We will stack these layers to form a full ConvNet architecture.\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GkWsOrHi1dT9"},"source":["\n","**Convolution Layer**   is the first filter applied as part of the feature-engineering step which applies a filter to our image. We pass over a mini image, usually called a kernel, and output the resulting, filtered subset of our image.\n","\n","Output formula for convolutional,   $O = \\frac{W- K + 2P}{S} +1$\n","\n","*   O : output height/length\n","*   W : input height/length\n","*   K : filter size (kernel size)\n","*   P : padding \n","*   S : stride\n","\n","\n","**Pooling Layer**  function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network, and hence also control overfitting. The Pooling Layer operates independently on every depth slice of the input and resizes it spatially, using the MAX operation.\n","\n","Output formula for Pooling, $O = \\frac{W- K}{S} +1$   \n","\n","If using PyTorch default stride (default stride is same as kernel size), then output formula for pooling will result,  $O = \\frac{W}{K}$"]},{"cell_type":"markdown","metadata":{"id":"D1GHmyGz11a5"},"source":["\n","![Capture](https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Single_Image_Colvolution.PNG)"]},{"cell_type":"markdown","metadata":{"id":"gmru4imbk0Sv"},"source":["Output after Convolutional layer 1 = $\\frac{256-3+ 2(1)}{1}+1 = 256$\n","\n","Output after Convolutional layer 2 = $\\frac{256-3+ 2(1)}{1}+1 = 256$\n","\n","Output after Maxpool layer 1 = $\\frac{256-3}{2}+1 = 127$"]},{"cell_type":"markdown","metadata":{"id":"y0e6sebUoToO"},"source":["#### Let's understand how each layer of CNN works while passing an Image\n","\n","Output of convolutional layer with Stride 1 and padding 1"]},{"cell_type":"code","metadata":{"id":"eayDkAdMY_RM"},"source":["# Defining a 2D conv layer with input_channels= 3, out_channel= 3, filter= 3, stride= 1 and padding= 1\n","cnn2d = nn.Conv2d(in_channels= 3, out_channels= 3, kernel_size= 3, stride= 1, padding= 1)\n","\n","cnn1_output = cnn2d(img.unsqueeze(0))\n","print(cnn1_output.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f4xekqXrQij2"},"source":["Output of Maxpool Layer"]},{"cell_type":"code","metadata":{"id":"ltvH0jTBZClj"},"source":["# Defining a maxpool layer with filter size= 3\n","Max_pool = nn.MaxPool2d(kernel_size=3)\n","maxpool_output = Max_pool(cnn1_output)\n","maxpool_output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6XeigblORB3H"},"source":["#### Defining ConvNet Architecture\n","\n","Considering two convolutional layers, 1 maxpool layer and 1 fully connected layer for building an architecture. The output of first conv layer will be the input of the second conv layer. Applying relu for the output of the second conv layer and then maxpool.\n"]},{"cell_type":"code","metadata":{"id":"tSf03J2IiCYP"},"source":["class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        \n","        # Initialze convolutional layer1 with filter size 3, stride 1 and padding 1\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=1)\n","\n","        self.conv2 = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=3, stride=1, padding=1)\n","\n","        self.maxpool1 = nn.MaxPool2d(kernel_size=3, stride=2) \n","\n","    def forward(self, x):\n","\n","        # Convolutional layer1 \n","        out = self.conv1(x)\n","        print(\"conv1\", out.shape)\n","\n","        # Convolutional layer2\n","        out = self.conv2(out)\n","        print(\"conv2\", out.shape)\n","\n","        # Activation Function \n","        out = F.relu(out)\n","\n","        # Maxpool layer\n","        out = self.maxpool1(out)\n","        \n","        return out\n","\n","# Initializing the network by creating an instance\n","net = Net()\n","print(net)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iTJpjvwqhHVe"},"source":["Pass the image through the model and get the output"]},{"cell_type":"code","metadata":{"id":"7Xz6DlrNFfp3"},"source":["output = net(img.unsqueeze(0))\n","output.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5nDJtcZ_3itO"},"source":["# Color image after maxpooling. Multiple execution will result in different intensity of the colors.\n","plt.imshow((output[0].permute(1,2,0)).data.numpy())\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isr6wnTQebiH"},"source":["**Summary:** In the above experiment we have seen representation of images using ConvNet architecture (filters at different layers and the output of each layer i.e. convolution of image at each layer). CNN extracts the feature of image and convert it into lower dimension without loosing its characteristics. Image classification using CNN can be seen in further experiments"]}]}