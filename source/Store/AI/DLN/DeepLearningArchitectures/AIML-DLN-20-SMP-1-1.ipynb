{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-20-SMP-1-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"-Va8_ShN-EKe"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n"]},{"cell_type":"markdown","metadata":{"id":"vTR13JSwqdIz"},"source":["## Objectives"]},{"cell_type":"markdown","metadata":{"id":"7y71HP1yqIVh"},"source":["\n","Sales forecasting is the process of estimating future sales. Accurate sales forecasts enable companies to make sound business decisions and predict short-term or long-term performance. Forecasts could be based on data such as past sales, industry-wide comparisons, and economic trends.\n","\n","\n","A leading retailer in the USA wants to forecast sales for their product categories in their store, based on the sales history of each category. Sales or revenue forecasting is very important for retail operations. Forecasting of retail sales helps the retailer to take necessary measures to plan their budgets or investments in a period (monthly, yearly) among different product categories like women's clothing, men's clothing, and other clothing. Further, they can plan to minimize revenue loss from the unavailability of products by investing accordingly.\n","\n","**Note: This data is proprietary. Please DO NOT share the dataset with anyone. The solution python notebook and test solution will not be provided.** </br>"]},{"cell_type":"code","metadata":{"id":"k3gUqgqPruR2","cellView":"form"},"source":["#@title Mini Hackathon Walkthrough Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"500\" height=\"300\" controls>\n","  <source src=\"https://cdn.exec.talentsprint.com/content/mini_hackathon_walkthrough.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gd9eW6oErumQ"},"source":["## Kaggle link and deadline:\n","\n","\n","### 1. Link to the Kaggle problem: https://www.kaggle.com/t/655beb4ca16149639522c5998d0a9770\n","\n","### 2. Deadlines:\n","  - **Competition closes at** 6:00 PM IST or 12:30 PM UTC, 18th Sep 2021 \n","  - **Submit this Colab file with code to aimlkaggle@gmail.com:** \n","      \n","      7.00 PM IST or 1:30 PM UTC 18th Sep 2021"]},{"cell_type":"markdown","metadata":{"id":"weuRWUlsJJYZ"},"source":["## Instructions:\n","\n","- Refer to the document **MiniHackathon- Kaggle Team Creation** for creating a Kaggle account. After login into the Kaggle account, access the kaggle problem. Follow the steps for Team creation in Kaggle.\n","- Under the 'Data' tab within the Kaggle competition page (link above), you can find four datasets. Their attributes are given in the \"Attributes description\".\n","- Follow **Stage 1** for downloading the data \n","- Combine the datasets and apply data-preprocessing to obtain a clean training dataset\n","- Build your own model using any algorithms learned till now\n","- **Get the Sales predictions for 2015 month-wise and product-wise** (36 rows)\n","- Copy and paste the predictions in column B (Sales(In ThousandDollars)) of the **Sample_Submission csv file** (ignore the headers)\n","- Upload the Sample_Submission csv file into Kaggle by clicking on Submit Predictions in Kaggle.\n","- The leaderboard takes and reflects your best submission until the specified deadline (maximum of 20 submissions only acceptable per day based on UTC (0:00) timing). \n","\n","### **Important: Only the Public Leaderboard rankings are valid, not the Private Leaderboard rankings.**"]},{"cell_type":"markdown","metadata":{"id":"-eiP0XYwsC1F"},"source":["## Evaluation: \n","The evaluation will be done based on the teams placed on the Kaggle leaderboard\n","\n","**TotalMarks=20**\n","\n","- The top 4 teams will be awarded 20 marks\n","- 5-8 teams will be awarded 18\n","- 9-12 teams will be awarded 16\n","- The rest of the teams will be awarded 14\n","- **0 Marks in case of 0 submissions**\n"," "]},{"cell_type":"markdown","metadata":{"id":"xdToUNf_7tuL"},"source":[" ## Finally...\n","    Don't cheat!\n","    Apply yourself!\n","    Have fun!\n"]},{"cell_type":"markdown","metadata":{"id":"p7aNpuWy-qMy"},"source":["## **Stage1:** Setting up colab for Kaggle competitions \n","This setup helps you directly access the datasets etc of the Kaggle competition."]},{"cell_type":"markdown","metadata":{"id":"Uliba9i7bGFB"},"source":["### 1. Create an API key in Kaggle.\n","\n","To do this, go to kaggle.com/ and open your user settings page. Click My Account.\n","\n","![alt text](https://i.stack.imgur.com/jxGQv.png\n",")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DkzGffHdbwX2"},"source":["### 2. Next, scroll down to the API access section and click generate to download an API key. \n","![alt text](https://i.stack.imgur.com/Hzlhp.png)"]},{"cell_type":"markdown","metadata":{"id":"WtETuXx8b-OC"},"source":["### 3. Upload your kaggle.json file using the following snippet in a code cell:\n","\n"]},{"cell_type":"code","metadata":{"id":"-1pfXBDxWl0Y"},"source":["from google.colab import files\n","files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GCV_T6MMW4eX"},"source":["#If successfully uploaded in the above step, the 'ls' command here should display the kaggle.json file.\n","%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JbukdzJ6cE32"},"source":["### 4. Install the Kaggle API using the following command\n"]},{"cell_type":"code","metadata":{"id":"dMj1n1MJcqzN"},"source":["!pip install -q kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Vpy9P1nchhd"},"source":["### 5. Move the kaggle.json file into ~/.kaggle, which is where the API client expects your token to be located:\n","\n"]},{"cell_type":"code","metadata":{"id":"yQbPsDOLZ0b4"},"source":["!mkdir -p ~/.kaggle\n","!cp kaggle.json ~/.kaggle/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BenAWlpI73sm"},"source":["#Execute the following command to verify whether the kaggle.json is stored in the appropriate location: ~/.kaggle/kaggle.json\n","!ls ~/.kaggle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vm2jGsCradOS"},"source":["!chmod 600 /root/.kaggle/kaggle.json #run this command to ensure your Kaggle API token is secure on colab"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32unPZKzdI72"},"source":["### 6. Now download the data"]},{"cell_type":"code","metadata":{"id":"YxOfqP10I6k8"},"source":["!mkdir data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ppuy5gRKHFwv"},"source":["**NOTE: If you get a '404 - Not Found' error after running the cell below, it is most likely that the user (whose kaggle.json is uploaded above) has not 'accepted' the rules of the competition and therefore has 'not joined' the competition.**"]},{"cell_type":"code","metadata":{"id":"TY40TmgfHq0s"},"source":["#If you get a forbidden link, you have most likely not joined the competition.\n","!kaggle competitions download -c retail-case-study-batch17 -p data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vBEOG2swe2rd"},"source":["## **Stage 2:** YOUR CODE to crack the Kaggle problem here. \n","\n","1.  Get the Sales prediction for the 2015 month-wise and product-wise (which give 36 rows). The product order for every month prediction can be as per the test_kaggle.csv file.\n","\n","2.  Copy and paste the predictions in Sample_Submission.csv (in Sales(In ThousandDollars)) and upload  them in to Kaggle.\n","\n","After uploading the predictions in Kaggle, the RMSE score will be displayed on the leaderboard.\n","\n","Understand the RMSE score [here](https://medium.com/analytics-vidhya/forecast-kpi-rmse-mae-mape-bias-cdc5703d242d) with an example.\n","\n","**Note: It is best advised to write all the code here. (If for any reason you are using other colab files, you could cut and paste the code from there into this notebook)**"]},{"cell_type":"markdown","metadata":{"id":"DsbcTMvEM3E6"},"source":["#**Weather Data**"]},{"cell_type":"code","metadata":{"id":"OVQ4x2fw1RZQ"},"source":["import pandas as pd\n","import numpy as np\n","\n","Wther2009=pd.read_excel('data/WeatherData.xlsx','2009')\n","Wther2010=pd.read_excel('data/WeatherData.xlsx','2010')\n","Wther2011=pd.read_excel('data/WeatherData.xlsx','2011')\n","Wther2012=pd.read_excel('data/WeatherData.xlsx','2012')\n","Wther2013=pd.read_excel('data/WeatherData.xlsx','2013')\n","Wther2014=pd.read_excel('data/WeatherData.xlsx','2014')\n","Wther2015=pd.read_excel('data/WeatherData.xlsx','2015')\n","Wther2016=pd.read_excel('data/WeatherData.xlsx','2016')\n","\n","#CONCATENATING ALL SHEETS\n","Wther=pd.concat([Wther2009,Wther2010,Wther2011,Wther2012,Wther2013,Wther2014,Wther2015,Wther2016],ignore_index=True)\n","  \n","#DROPPING COLUMNS\n","Weather=Wther.drop(['Temp high (°C)','Temp low (°C)','Dew Point high (°C)','Dew Point low (°C)','Humidity\\xa0(%) high','Humidity\\xa0(%) low','Sea Level Press.\\xa0(hPa) high','Sea Level Press.\\xa0(hPa) low','Visibility\\xa0(km) high','Visibility\\xa0(km) low','Wind\\xa0(km/h) high','Wind\\xa0(km/h) low','WeatherEvent','Precip.\\xa0(mm) sum'],axis=1)\n","\n","#DEALING WITH EMPTY VALUES\n","Weather=Weather.replace('-',0)\n","Weather=Weather.replace('avg',0)\n","Weather = Weather.dropna()\n","Weather['Avg_weather'] =  Weather.iloc[:, 3:9].mean(axis=1)\n","#Weather = Weather.drop(['Temp avg (°C)', 'Dew Point avg (°C)', 'Humidity (%) avg', 'Sea Level Press. (hPa) avg', 'Visibility (km) avg', 'Wind (km/h) avg'])\n","\n","#YEAR & MONTH LABELS \n","year={2009.0:1,2010.0:2,2011.0:3,2012.0:4,2013.0:5,2014.0:6,2015.0:7,2016.0:8}\n","month={'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9,'Oct':10, 'Nov':11, 'Dec':12}\n","Weather[\"Year\"] = Weather[\"Year\"].map(year)\n","Weather[\"Month\"] = Weather[\"Month\"].map(month)\n","\n","#AVG_WEATHER\n","Weather_avg = Weather.groupby(['Year','Month'])['Avg_weather'].mean()\n","Weather_avg = Weather_avg.round(1)\n","\n","#PRINTING DATA\n","print(Weather_avg.shape)\n","\n","Weather_avg.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ap4ITrqlry0"},"source":["#**Macro_Economic_Data**"]},{"cell_type":"code","metadata":{"id":"OAYfmJHplxHh"},"source":["macroEco=pd.read_excel('data/MacroEconomicData.xlsx')\n","macroEco['Year'] = [each.split()[0] for each in macroEco['Year-Month']]\n","macroEco['Month'] = [each.split()[2] for each in macroEco['Year-Month']]\n","macroEco = macroEco.drop(['PartyInPower','Year-Month'],axis=1)\n","\n","month={'Jan':1, 'Feb':2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7, 'Aug':8, 'Sep':9,'Oct':10, 'Nov':11, 'Dec':12}\n","year={2009:1,2010:2,2011:3,2012:4,2013:5,2014:6,2015:7,2016:8}\n","macroEco[\"Month\"] = macroEco[\"Month\"].map(month)\n","\n","minimumAdExpence = min([each for each in list(macroEco['AdvertisingExpenses (in Thousand Dollars)']) if type(each) == int])\n","\n","macroEco=macroEco.replace('?',minimumAdExpence)\n","macroEco = macroEco.dropna()\n","\n","macroEco[\"Year\"] = [year[int(each)] for each in macroEco['Year']]\n","\n","macroEco = macroEco.groupby(['Year','Month'])['Monthly Nominal GDP Index (inMillion$)', 'Monthly Real GDP Index (inMillion$)', 'CPI', 'unemployment rate', 'CommercialBankInterestRateonCreditCardPlans', 'Finance Rate on Personal Loans at Commercial Banks, 24 Month Loan', 'Earnings or wages  in dollars per hour', 'AdvertisingExpenses (in Thousand Dollars)', 'Cotton Monthly Price - US cents per Pound(lbs)', 'Change(in%)', 'Average upland planted(million acres)', 'Average upland harvested(million acres)', 'yieldperharvested acre', 'Production (in  480-lb netweright in million bales)', 'Mill use  (in  480-lb netweright in million bales)', 'Exports'].mean()\n","\n","print(macroEco.shape)\n","macroEco.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vAkL1ur1M8z4"},"source":["#**Holidays Data**"]},{"cell_type":"code","metadata":{"id":"dex5DnK15_Rs"},"source":["holi=pd.read_excel('data/Events_HolidaysData.xlsx')\n","year={2009.0:1,2010.0:2,2011.0:3,2012.0:4,2013.0:5,2014.0:6,2015.0:7,2016.0:8}\n","\n","holi[\"Year\"] = holi[\"Year\"].map(year)\n","holi['Month'] = [each.month for each in holi['MonthDate']]\n","holi_fin=holi.groupby(['Year', 'Month']).count()[['Event']]\n","print(holi_fin.shape)\n","holi_fin.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LCijOeweM6Fc"},"source":["#**Kaggle Train Data**"]},{"cell_type":"code","metadata":{"id":"JZMu58144FKX"},"source":["import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","data=pd.read_csv('data/Train_Kaggle.csv')\n","sns.catplot(x= \"ProductCategory\",y= \"Sales(In ThousandDollars)\", data=data)\n","plt.show()\n","\n","data.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XKpXkFd04Tig"},"source":["* *Individual Clothing*"]},{"cell_type":"code","metadata":{"id":"ZPruKQNR4Sqk"},"source":["data_Women=data[data['ProductCategory']=='WomenClothing']\n","data_WomenMean = int(data_Women['Sales(In ThousandDollars)'].mean())\n","data_Women=data_Women.fillna(value=data_WomenMean)\n","\n","data_Other=data[data['ProductCategory']=='OtherClothing']\n","data_OtherMean = int(data_Other['Sales(In ThousandDollars)'].mean())\n","data_Other=data_Other.fillna(value=data_OtherMean)\n","\n","data_Men=data[data['ProductCategory']=='MenClothing']\n","data_MenMean = int(data_Men['Sales(In ThousandDollars)'].mean())\n","data_Men=data_Men.fillna(value=data_MenMean)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6z4WCiKw5jFV"},"source":["* *Combined Final Training Data*"]},{"cell_type":"code","metadata":{"id":"R5kecg1d5RdC"},"source":["year={2009:1,2010:2,2011:3,2012:4,2013:5,2014:6}\n","dept={'MenClothing':1,'WomenClothing':2,'OtherClothing':3}\n","\n","fin_data=pd.concat([data_Men,data_Women,data_Other])\n","fin_data[\"Year\"] = fin_data[\"Year\"].map(year)\n","fin_data[\"ProductCategory\"] = fin_data[\"ProductCategory\"].map(dept)\n","\n","fin_data.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GnR3Vs9s53NF"},"source":["* *Duplicating The Training Data*"]},{"cell_type":"code","metadata":{"id":"MF3ru6Y55v9D"},"source":["train_fin=fin_data\n","train_fin_Men=train_fin[train_fin['ProductCategory']==1].reset_index(drop=True)\n","train_fin_Women=train_fin[train_fin['ProductCategory']==2].reset_index(drop=True)\n","train_fin_Others=train_fin[train_fin['ProductCategory']==3].reset_index(drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2srhtBAFQhLw"},"source":["#**COMBINING ALL DATASETS**"]},{"cell_type":"code","metadata":{"id":"DBXXaJsafPtp"},"source":["full_men1=pd.merge(train_fin_Men,holi_fin, on=['Year','Month'],how=\"left\")\n","full_women1=pd.merge(train_fin_Women,holi_fin, on=['Year','Month'],how=\"left\")\n","full_others1=pd.merge(train_fin_Others,holi_fin, on=['Year','Month'],how=\"left\")\n","\n","macro_weather=pd.merge(macroEco,Weather_avg, on=['Year','Month'],how=\"left\")\n","\n","full_men=pd.merge(full_men1,macro_weather, on=['Year','Month'],how=\"left\")\n","full_women=pd.merge(full_women1,macro_weather, on=['Year','Month'],how=\"left\")\n","full_others=pd.merge(full_others1,macro_weather, on=['Year','Month'],how=\"left\")\n","\n","full_men = full_men.fillna(full_men.mean())\n","full_women = full_women.fillna(full_women.mean())\n","full_others = full_others.fillna(full_others.mean())\n","\n","Full_data=pd.concat([full_men,full_women,full_others],ignore_index=True)\n","Full_data.fillna(Full_data.mean())\n","df = Full_data.round(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a64YoSNkf9z6"},"source":["#REMOVING HIGHLY CORELATED FEATURES\n","\n","# Create correlation matrix\n","corr_matrix = df.corr().abs()\n","\n","# Select upper triangle of correlation matrix\n","upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n","\n","# Find index of feature columns with correlation greater than 0.85\n","to_drop = [column for column in upper.columns if any(upper[column] > 0.85)]\n","\n","# Drop features \n","finalTrainData = df.drop(df[to_drop], axis=1)\n","\n","finalTrainData.head(2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WB95_IFbGgPm"},"source":["finalTrainData = finalTrainData.drop(['Finance Rate on Personal Loans at Commercial Banks, 24 Month Loan', 'Cotton Monthly Price - US cents per Pound(lbs)', 'Change(in%)','Average upland planted(million acres)', 'Average upland harvested(million acres)', 'yieldperharvested acre', 'Mill use  (in  480-lb netweright in million bales)'],axis=1)\n","finalTrainData.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aVerIpo9WDx7"},"source":["#**EXTRACT FEATURES AND LABELS**"]},{"cell_type":"code","metadata":{"id":"kpmjQJ45ABCe"},"source":["filtered_df = finalTrainData[finalTrainData['Sales(In ThousandDollars)'].notnull()]\n","features = finalTrainData.loc[:, finalTrainData.columns != 'Sales(In ThousandDollars)']\n","labels = finalTrainData['Sales(In ThousandDollars)']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gkRHRxUyCJhX"},"source":["from sklearn.preprocessing import MinMaxScaler\n","scaler = MinMaxScaler()\n","scaled_features = scaler.fit_transform(features)\n","scaled_features.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a3y25QAnAPCe"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import GradientBoostingRegressor\n","\n","train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.3)\n","gbr = GradientBoostingRegressor(n_estimators=100)\n","gbr.fit(train_features, train_labels)\n","predictions = gbr.predict(test_features)\n","mean_squared_error(test_labels, predictions, squared=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rSMD3bZJBGZ6"},"source":["#**TESTING**"]},{"cell_type":"code","metadata":{"id":"oW5gGPCFlyAj"},"source":["test_data_set=pd.read_csv('data/Test_Kaggle.csv')\n","\n","year={2009:1,2010:2,2011:3,2012:4,2013:5,2014:6,2015:7}\n","dept={'MenClothing':1,'WomenClothing':2,'OtherClothing':3}\n","test_data_set[\"Year\"] = test_data_set[\"Year\"].map(year)\n","test_data_set[\"ProductCategory\"] = test_data_set[\"ProductCategory\"].map(dept)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6g-t1Ksr46Kl"},"source":["my_list = [each for each in Weather_avg.index if each[0] == 7]\n","Weather_test = Weather_avg[Weather_avg.index.isin(my_list)]\n","\n","my_list = [each for each in macroEco.index if each[0] == 7]\n","macroEco_test = macroEco[macroEco.index.isin(my_list)]\n","\n","my_list = [each for each in holi_fin.index if each[0] == 7]\n","holi_fin_test = holi_fin[holi_fin.index.isin(my_list)]\n","\n","dummy=pd.merge(Weather_test,macroEco_test, on=['Year','Month'],how=\"left\")\n","support_data =pd.merge(dummy,holi_fin_test, on=['Year','Month'],how=\"left\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iEkjGbExpnrL"},"source":["toDrop = []\n","for each in support_data.columns:\n","  if not each in features.columns:\n","    toDrop.append(each)\n","\n","support_data = support_data.drop(toDrop,axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"De1SY-zpBYxE"},"source":["temp = []\n","indices = []\n","columns = list(test_data_set.columns) + (list(support_data.columns))\n","\n","for i,j in zip(test_data_set['Year'], test_data_set['Month']):\n","  indices.append((i,j))\n","\n","for i in range(len(test_data_set)):\n","  data = list(test_data_set.loc[i])+list(support_data.loc[indices[i]])  \n","  temp.append(data)\n","\n","df = pd.DataFrame(temp)\n","df.columns = columns\n","\n","order = features.columns\n","df = df[order]\n","filtered_test_df_final = df.fillna(df.mean())\n","filtered_test_df_final.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IiHmEpqkBXFR"},"source":["predictions = gbr.predict(filtered_test_df_final)\n","\n","finalTestDataset = pd.DataFrame(pd.Series(predictions))\n","finalTestDataset.index += 1\n","finalTestDataset = finalTestDataset.reset_index()\n","finalTestDataset.columns = ['Year','Sales(In ThousandDollars)']\n","finalTestDataset = finalTestDataset.set_index('Year')\n","\n","finalTestDataset['Sales(In ThousandDollars)'] = finalTestDataset['Sales(In ThousandDollars)'].astype(int)\n","finalTestDataset.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3X2VrqzlG4zr"},"source":["finalTestDataset.to_csv(\"test_gbr.csv\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YGVFlq5S-5lu"},"source":["## **Stage 3:** Each time you submit in kaggle, ensure that the code given by you in Stage2 gives the same result. Follow the steps for the validation:\n","### a) Enter your Kaggle RMSE in the form below \n","### b) After entering RMSE below, go to File->'Save and pin revision' (To ensure you do so, you are asked to mark 'Yes' to the instruction asking the same)\n","**Note: The Shortcut for 'Save and pin revision' is Ctrl+M+S**</br>\n","**Note: You can check if the action has succeeded by going to File->Revision History and you'll find the \"PIN\" checkbox checked if successful.** \n","\n","\n","- This action ensures there is 'proof of code' for each submission you make.\n","- If you submit your results in Kaggle, and get a leaderboard RMSE score, but you don't follow the steps asked above, then your **score will NOT be considered**, as we don't have the proof of your code. (We map the 'proof of code' by mapping it to your \"RMSE+Time of save+pin\"). In other words, if you want your RMSE score to be considered you have to follow the process. \n","- However, for trial submission (RMSE scores you don't care about being considered, as you're still experimenting in your initial attempts) you don't have to follow the process above.\n","- **One member from your team can collect all your team-members colab shared links and email them to aimlkaggle@gmail.com as per deadlines.** Ensure to give edit access to aimlkaggle@gmail.com.\n","- **FINALLY: \"Do NOT download and reupload this file as all the revision history will be lost\"**\n","\n","\n"]}]}