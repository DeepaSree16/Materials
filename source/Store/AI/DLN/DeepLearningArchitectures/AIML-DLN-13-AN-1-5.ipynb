{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-13-AN-1-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"H7W6IuZjFVoD"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RQL9oiyvFYck"},"source":["### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"sMLbbBYXLMfJ"},"source":["## Vanishing Gradients"]},{"cell_type":"code","metadata":{"id":"5DZhIGmjLPPR","cellView":"form"},"source":["#@title Case Study Walkthrough\n","#@markdown  Vanishing Gradients\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/vanishing_gradient.mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/fruits_weight_sphercity.csv"],"metadata":{"id":"RRE7CAghbY31"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDjDP1SkFYWb"},"source":["## Vanishing Gradients"]},{"cell_type":"markdown","metadata":{"id":"Yv0zcJrOA9Hu"},"source":["#### In this notebook, we will demonstrate the difference between using sigmoid and ReLU nonlinearities in a simple neural network with two hidden layers."]},{"cell_type":"code","metadata":{"id":"dLRkDIE9A9Hw"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n","plt.rcParams['image.interpolation'] = 'nearest'\n","plt.rcParams['image.cmap'] = 'gray'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AJC1BkAThmRp"},"source":["#### for auto-reloading external modules\n","#### see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython"]},{"cell_type":"code","metadata":{"id":"gizpeXjXhkD4"},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ozg36_12A9H0"},"source":["# generate random data -- not linearly separable \n","np.random.seed(0)\n","N = 100 # number of points per class\n","D = 2 # dimensionality\n","K = 3 # number of classes\n","X = np.zeros((N*K,D))\n","num_train_examples = X.shape[0]\n","y = np.zeros(N*K, dtype='uint8')\n","for j in range(K):\n","  ix = range(N*j,N*(j+1))\n","  r = np.linspace(0.0,1,N) # radius\n","  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # theta\n","  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n","  y[ix] = j\n","fig = plt.figure()\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n","plt.xlim([-1,1])\n","plt.ylim([-1,1])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"67BQMqtjA9H6"},"source":["The sigmoid function \"squashes\" inputs to lie between 0 and 1. Unfortunately, this means that for inputs with sigmoid output close to 0 or 1, the gradient with respect to those inputs is close to zero. This leads to the phenomenon of vanishing gradients, where gradients drop close to zero, and the net does not learn well.\n","\n","On the other hand, the relu function (max(0, x)) does not saturate with input size. Plot these functions to gain intuition. "]},{"cell_type":"code","metadata":{"id":"3AWr7-luA9H6"},"source":["def sigmoid(x):\n","    x = 1/(1+np.exp(-x))\n","    return x\n","\n","def sigmoid_grad(x):\n","    return (x)*(1-x)\n","\n","def relu(x):\n","    return np.maximum(0,x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c8vri9wDA9H_"},"source":["Let's try and see now how the two kinds of nonlinearities change deep neural net training in practice. Below, we build a very simple neural net with three layers (two hidden layers), for which you can swap out ReLU/ sigmoid nonlinearities."]},{"cell_type":"code","metadata":{"id":"s2TEPzYyA9IA"},"source":["# function to train a three layer neural net with either RELU or sigmoid nonlinearity via vanilla grad descent\n","\n","def three_layer_net(NONLINEARITY,X,y, model, step_size, reg):\n","    # parameter initialization\n","    \n","    h= model['h']\n","    h2= model['h2']\n","    W1= model['W1']\n","    W2= model['W2']\n","    W3= model['W3']\n","    b1= model['b1']\n","    b2= model['b2']\n","    b3= model['b3']\n","    \n","    \n","    # some hyperparameters\n","\n","    # gradient descent loop\n","    num_examples = X.shape[0]\n","    plot_array_1=[]\n","    plot_array_2=[]\n","    for i in range(50000):\n","\n","        #  cvxFOWARD PROP\n","\n","        if NONLINEARITY== 'RELU':\n","            hidden_layer = relu(np.dot(X, W1) + b1)\n","            hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)\n","            scores = np.dot(hidden_layer2, W3) + b3\n","\n","        elif NONLINEARITY == 'SIGM':\n","            hidden_layer = sigmoid(np.dot(X, W1) + b1)\n","            hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)\n","            scores = np.dot(hidden_layer2, W3) + b3\n","\n","        exp_scores = np.exp(scores)\n","        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n","\n","        # compute the loss: average cross-entropy loss and regularization\n","        corect_logprobs = -np.log(probs[range(num_examples),y])\n","        data_loss = np.sum(corect_logprobs)/num_examples\n","        reg_loss = 0.5*reg*np.sum(W1*W1) + 0.5*reg*np.sum(W2*W2)+ 0.5*reg*np.sum(W3*W3)\n","        loss = data_loss + reg_loss\n","        if i % 1000 == 0:\n","            print(\"iteration %d: loss %f\" % (i, loss))\n","\n","\n","        # compute the gradient on scores\n","        dscores = probs\n","        dscores[range(num_examples),y] -= 1\n","        dscores /= num_examples\n","\n"," \n","        # BACKPROP HERE\n","        dW3 = (hidden_layer2.T).dot(dscores)\n","        db3 = np.sum(dscores, axis=0, keepdims=True)\n","\n","\n","        if NONLINEARITY == 'RELU':\n","\n","            #backprop ReLU nonlinearity here\n","            dhidden2 = np.dot(dscores, W3.T)\n","            dhidden2[hidden_layer2 <= 0] = 0\n","            dW2 =  np.dot( hidden_layer.T, dhidden2)\n","            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))\n","            db2 = np.sum(dhidden2, axis=0)\n","            dhidden = np.dot(dhidden2, W2.T)\n","            dhidden[hidden_layer <= 0] = 0\n","            \n","        elif NONLINEARITY == 'SIGM':\n","\n","            #backprop sigmoid nonlinearity here\n","            dhidden2 = dscores.dot(W3.T)*sigmoid_grad(hidden_layer2)\n","            dW2 = (hidden_layer.T).dot(dhidden2)\n","            plot_array_2.append(np.sum(np.abs(dW2))/np.sum(np.abs(dW2.shape)))\n","            db2 = np.sum(dhidden2, axis=0)\n","            dhidden = dhidden2.dot(W2.T)*sigmoid_grad(hidden_layer)\n","\n","        \n","        dW1 =  np.dot(X.T, dhidden)\n","        plot_array_1.append(np.sum(np.abs(dW1))/np.sum(np.abs(dW1.shape)))\n","        db1 = np.sum(dhidden, axis=0)\n","\n","        # add regularization\n","        dW3+= reg * W3\n","        dW2 += reg * W2\n","        dW1 += reg * W1\n","        \n","        #option to return loss, grads -- uncomment next comment\n","        grads={}\n","        grads['W1']=dW1\n","        grads['W2']=dW2\n","        grads['W3']=dW3\n","        grads['b1']=db1\n","        grads['b2']=db2\n","        grads['b3']=db3\n","        #return loss, grads\n","        \n","        \n","        # update\n","        W1 += -step_size * dW1\n","        b1 += -step_size * db1\n","        W2 += -step_size * dW2\n","        b2 += -step_size * db2\n","        W3 += -step_size * dW3\n","        b3 += -step_size * db3\n","    # evaluate training set accuracy\n","    if NONLINEARITY == 'RELU':\n","        hidden_layer = relu(np.dot(X, W1) + b1)\n","        hidden_layer2 = relu(np.dot(hidden_layer, W2) + b2)\n","    elif NONLINEARITY == 'SIGM':\n","        hidden_layer = sigmoid(np.dot(X, W1) + b1)\n","        hidden_layer2 = sigmoid(np.dot(hidden_layer, W2) + b2)\n","    scores = np.dot(hidden_layer2, W3) + b3\n","    predicted_class = np.argmax(scores, axis=1)\n","    print('training accuracy: %.2f' % (np.mean(predicted_class == y))) \n","    #return cost, grads\n","    return plot_array_1, plot_array_2, W1, W2, W3, b1, b2, b3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8vtaK0KyA9ID"},"source":["#### Train net with sigmoid nonlinearity first"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"a3k7is_DA9IE"},"source":["#Initialize toy model, train sigmoid net\n","\n","N = 100 # number of points per class\n","D = 2 # dimensionality\n","K = 3 # number of classes\n","h=50\n","h2=50\n","num_train_examples = X.shape[0]\n","\n","model={}\n","model['h'] = h # size of hidden layer 1\n","model['h2']= h2# size of hidden layer 2\n","model['W1']= 0.1 * np.random.randn(D,h)\n","model['b1'] = np.zeros((1,h))\n","model['W2'] = 0.1 * np.random.randn(h,h2)\n","model['b2']= np.zeros((1,h2))\n","model['W3'] = 0.1 * np.random.randn(h2,K)\n","model['b3'] = np.zeros((1,K))\n","\n","(sigm_array_1, sigm_array_2, s_W1, s_W2,s_W3, s_b1, s_b2,s_b3) = three_layer_net('SIGM', X,y,model, step_size=1e-1, reg=1e-3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ADrcyCJsA9IK"},"source":["#### Now train net with ReLU nonlinearity "]},{"cell_type":"code","metadata":{"scrolled":true,"id":"YgiHDhaGA9IK"},"source":["# Re-initialize model, train relu net\n","\n","model={}\n","model['h'] = h # size of hidden layer 1\n","model['h2']= h2# size of hidden layer 2\n","model['W1']= 0.1 * np.random.randn(D,h)\n","model['b1'] = np.zeros((1,h))\n","model['W2'] = 0.1 * np.random.randn(h,h2)\n","model['b2']= np.zeros((1,h2))\n","model['W3'] = 0.1 * np.random.randn(h2,K)\n","model['b3'] = np.zeros((1,K))\n","\n","(relu_array_1, relu_array_2, r_W1, r_W2,r_W3, r_b1, r_b2,r_b3) = three_layer_net('RELU', X,y,model, step_size=1e-1, reg=1e-3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wqTvYGHlA9IO"},"source":["# The Vanishing Gradient Issue"]},{"cell_type":"markdown","metadata":{"id":"kzapxJBJA9IP"},"source":["We can use the sum of the magnitude of gradients for the weights between hidden layers as a cheap heuristic to measure the speed of learning (you can also use the magnitude of gradients for each neuron in the hidden layer here). Intuitively, when the magnitude of the gradients of the weight vectors or of each neuron is large, the net is learning faster."]},{"cell_type":"code","metadata":{"id":"uzikneHwA9IP"},"source":["plt.plot(np.array(sigm_array_1))\n","plt.plot(np.array(sigm_array_2))\n","plt.xlabel('Iterations')\n","plt.ylabel('Sum of magnitudes of gradients')\n","plt.title('Vanishing gradients -- SIGM weights')\n","plt.legend((\"sigm first layer\", \"sigm second layer\"))\n","plt.savefig('Vanishing_gradients_SIGM_weights.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ib6Ci-yA9IT"},"source":["plt.plot(np.array(relu_array_1))\n","plt.plot(np.array(relu_array_2))\n","plt.xlabel('Iterations')\n","plt.ylabel('Vanishing gradients')\n","plt.title('Sum of magnitudes of gradients -- ReLU weights')\n","plt.legend((\"relu first layer\", \"relu second layer\"))\n","plt.savefig('Vanishing_gradients_ReLU_weights.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yfn7cnNsA9IY"},"source":["# Overlaying the two plots to compare\n","plt.plot(np.array(relu_array_1))\n","plt.plot(np.array(relu_array_2))\n","plt.plot(np.array(sigm_array_1))\n","plt.plot(np.array(sigm_array_2))\n","plt.title('Sum of magnitudes of gradients -- hidden layer neurons')\n","plt.legend((\"relu first layer\", \"relu second layer\",\"sigm first layer\", \"sigm second layer\"))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBRSKX9AA9Ic"},"source":["#### Feel free to play around with this notebook to gain intuition. Things you might want to try:\n","\n","- Adding additional layers to the nets and seeing how early layers continue to train slowly for the sigmoid net \n","- Experiment with hyperparameter tuning for the nets -- changing regularization and gradient descent step size\n","- Experiment with different nonlinearities -- Leaky ReLU, Maxout. How quickly do different layers learn now?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K2uLkWfpA9Ic"},"source":["We can see how well each classifier does in terms of distinguishing the toy data classes. As expected, since the ReLU net trains faster, for a set number of epochs it performs better compared to the sigmoid net. "]},{"cell_type":"code","metadata":{"id":"qB88wrY2A9Id"},"source":["# plot the classifiers- SIGMOID\n","h = 0.02\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","Z = np.dot(sigmoid(np.dot(sigmoid(np.dot(np.c_[xx.ravel(), yy.ravel()], s_W1) + s_b1), s_W2) + s_b2), s_W3) + s_b3\n","Z = np.argmax(Z, axis=1)\n","Z = Z.reshape(xx.shape)\n","fig = plt.figure()\n","plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wg__zf0jA9Ii"},"source":["# plot the classifiers-- RELU\n","h = 0.02\n","x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n","                     np.arange(y_min, y_max, h))\n","Z = np.dot(relu(np.dot(relu(np.dot(np.c_[xx.ravel(), yy.ravel()], r_W1) + r_b1), r_W2) + r_b2), r_W3) + r_b3\n","Z = np.argmax(Z, axis=1)\n","Z = Z.reshape(xx.shape)\n","fig = plt.figure()\n","plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n","plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\n","plt.xlim(xx.min(), xx.max())\n","plt.ylim(yy.min(), yy.max())"],"execution_count":null,"outputs":[]}]}