{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CDS-DLN-04-AS-5-6.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"cells":[{"cell_type":"markdown","metadata":{"id":"A_PeZBwm2qGD"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 6: PCA and t-SNE"]},{"cell_type":"markdown","metadata":{"id":"doENGez0k03C"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"IsVole-a4AgQ"},"source":["At the end of the experiment, you will be able to\n","\n","* understand terms like projection and manifold learning\n","* know different dimensionality reduction techniques\n","* perform principal component analysis (PCA) on MNIST dataset\n","* perform Kernel PCA on Swiss Roll dataset\n","* know and implement t-SNE algorithm"]},{"cell_type":"markdown","metadata":{"id":"t08TchktpXQS"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"mjfpo3prpXQT"},"source":["Many machine learning problems involve a vast number of features for each training instance, making the training extremely slow and harder to find a good solution. This problem is called the **curse of dimensionality**. Therefore, we need dimensionality reduction techniques, that transform data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains most meaningful properties of the original data. Dimensionality reduction speeds up training and is also extremely useful for data visualization. Two main approaches to reducing dimensionality are: **projection** and **manifold learning**."]},{"cell_type":"markdown","metadata":{"id":"BfIs5MkNpXQT"},"source":["#### Projection\n","\n","In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated. As a result, all training instances lie within a much lower-dimensional subspace of the high-dimensional space.\n","\n","In the figure below a 3D dataset is represented by circles.\n","\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/pca1.png\" width=450px/>\n","</center>\n","\n","$\\hspace{8.6cm} \\text{A 3D dataset lying close to a 2D subspace}$\n","<br><br>\n","\n","Notice that all training instances lie close to a plane which is a lower-dimensional (2D) subspace of the high-dimensional (3D) space. \n","\n","After projecting every training instance perpendicularly onto this subspace, we get the new 2D dataset as shown in the figure below. By that, we reduced the dataset’s dimensionality from 3D to 2D."]},{"cell_type":"markdown","metadata":{"id":"MRIH-eafpXQU"},"source":["<br><br>\n","<center>\n","<img src=\"data:image/png;base64,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\" width=350px/>\n","</center>\n","\n","$\\hspace{10cm} \\text{The new 2D dataset after projection}$\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"m8bgtUXXpXQY"},"source":["The most common dimensionality reduction method, that applies projection is:\n","\n","* PCA (Principal Component Analysis)\n","\n","However, projection is not always the best approach to dimensionality reduction. In many cases, the subspace may twist and turn, such as in the \n","Swiss roll toy dataset represented in the figure below.\n","<br><br>\n","<center>\n","<img src=\"https://cdn-images-1.medium.com/max/1024/1*4HkBjs6YhR18fl9ETBhfzw.png\" width=450px/>\n","</center>\n","\n","$\\hspace{10cm} \\text{Swiss roll dataset}$\n","<br><br>\n","\n","Simply projecting onto a plane (e.g., by dropping $X_3$) would squash different layers of the Swiss roll together, as shown on the left side of the below figure. What we really want is to unroll the Swiss roll to obtain the 2D dataset on the right side of the below figure.\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/swiss_unroll.png\" width=550px/>\n","</center>\n","\n","$\\hspace{6cm} \\text{Squashing by projecting onto a plane (left) versus unrolling the Swiss roll (right)}$\n","<br><br>"]},{"cell_type":"markdown","metadata":{"id":"QubleuL4pXQa"},"source":["#### Manifold Learning\n","\n","The manifold hypothesis states that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. Many dimensionality reduction algorithms rely on modeling the manifold on which the training instances lie; this is called **Manifold Learning**.\n","\n","For example, in the top row of the below figure, the Swiss roll is split into two classes: in the 3D space (on the left), the decision boundary would be fairly complex, but in the 2D unrolled manifold space (on the right), the decision boundary is a straight line.\n","\n","However, this implicit assumption does not always hold. For example, in the bottom row of the below figure, the decision boundary is located at $X_1\n"," = 5$. This decision boundary looks very simple in the original 3D space (a vertical plane), but it looks more complex in the unrolled manifold (a collection of four independent line segments).\n","\n","<br><br>\n","<center>\n","<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/pca2.png\" width=500px/>\n","</center>\n","\n","$\\hspace{6.5cm} \\text{The decision boundary may not always be simpler with lower dimensions}$\n","<br><br>\n","\n","Therefore, reducing the dimensionality of your training set before training a model will usually speed up training, but it may not always lead to a better or simpler solution; it all depends on the dataset.\n","\n","A popular manifold learning method is:\n","\n","* t-distributed Stochastic Neighbor Embedding (t-SNE)"]},{"cell_type":"markdown","metadata":{"id":"JPrQ5122pXQb"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"9bNb3viQpXQc"},"source":["import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.datasets import load_digits, make_swiss_roll               \n","from tensorflow.keras.datasets import mnist\n","from sklearn.decomposition import PCA, IncrementalPCA, KernelPCA\n","from sklearn.manifold import TSNE\n","import warnings\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E0Xfje2DpXQc"},"source":["### Principal Component Analysis (PCA)"]},{"cell_type":"markdown","metadata":{"id":"qz_FDvUQpXQd"},"source":["PCA is a technique which helps in extracting a new set of variables from an existing large set of variables. These newly extracted variables are called Principal Components. \n","\n","Some of the key points about PCA are:\n","\n","* A principal component is a linear combination of the original variables\n","\n","* Principal components are extracted in such a way that the first principal component explains maximum variance in the dataset\n","\n","* Second principal component tries to explain the remaining variance in the dataset and is uncorrelated to the first principal component\n","\n","* Third principal component tries to explain the variance which is not explained by the first two principal components and so on\n","\n","Scikit-Learn’s `PCA` class uses SVD decomposition to implement PCA.\n","\n","To know more about Scikit-Learn’s `PCA` class, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n","\n","Let's implement PCA on MNIST dataset:"]},{"cell_type":"code","metadata":{"id":"kc2tg1HjpXQe"},"source":["# Load mnist dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()\n","plt.imshow(X_train[0], cmap=\"Greys\")\n","X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GElEcdAMpXQf"},"source":["# Reshape X_train\n","X_train = X_train.reshape(X_train.shape[0], 28*28)\n","X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X-ap4_u-pXQg"},"source":["# Perform PCA\n","pca = PCA(n_components = 400)\n","pca.fit_transform(X_train);"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eaFL_-yapXQg"},"source":["<font color='blue'>**Discussion 1:** Differentiate between PCA(n_components=400) and PCA(n_components= 0.95). </font>"]},{"cell_type":"markdown","metadata":{"id":"W_MKgNR4pXQg"},"source":["#### Explained Variance Ratio"]},{"cell_type":"markdown","metadata":{"id":"PNf0RpTipXQg"},"source":["The ratio indicates the proportion of the dataset’s variance that lies along with each principal component. It is available via the `explained_variance_ratio_` variable."]},{"cell_type":"code","metadata":{"id":"05ZBBLxlpXQh"},"source":["# Explained variance of first 10 principal components\n","pca.explained_variance_ratio_[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j_vROg9kpXQh"},"source":["#### Choosing the Right Number of Dimensions"]},{"cell_type":"markdown","metadata":{"id":"R6teIYGppXQh"},"source":["Instead of arbitrarily choosing the number of dimensions to reduce down to, we can either\n","\n","* select 2 or 3 dimensions for data visualization\n","\n","* choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., `PCA(n_components=0.95)` for 95% variance)\n","\n","* plot the cumulative explained variance as a function of the number of dimensions and check for an elbow in the curve"]},{"cell_type":"code","metadata":{"id":"KiMVCVURpXQi"},"source":["# Visualize Explained variance as a function of the number of dimensions\n","plt.plot(np.cumsum(pca.explained_variance_ratio_))\n","plt.xlabel('Dimensions')\n","plt.ylabel('Cumulative explained variance');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wn7TASd3pXQi"},"source":["From the above plot, we can see that reducing the dimensionality down to about 100 dimensions wouldn’t lose too much explained variance."]},{"cell_type":"markdown","metadata":{"id":"RzhSet-bpXQi"},"source":["#### PCA for Compression"]},{"cell_type":"markdown","metadata":{"id":"yGIQvDqkpXQj"},"source":["After dimensionality reduction, the training set takes up much less space. This size reduction can speed up a classification algorithm tremendously.\n","\n","We can also decompress the reduced dataset back to 784 dimensions by applying the inverse transformation of the PCA projection.\n","\n","The following code first compresses the MNIST dataset to 100 dimensions and then decompresses it back to 784 dimensions using the `inverse_transform()` method:"]},{"cell_type":"code","metadata":{"id":"hhKPaDWwpXQj"},"source":["# Compress-decompress dataset\n","pca = PCA(n_components = 100)\n","X_reduced = pca.fit_transform(X_train)\n","X_recovered = pca.inverse_transform(X_reduced)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oc04Gu86pXQj"},"source":["# Images before and after compress-decompress operation\n","fig, ax = plt.subplots(2,2, figsize=(10,6))\n","\n","ax[0][0].imshow(X_train[0].reshape(28,28), cmap=\"Greys\")\n","ax[0][1].imshow(X_train[5].reshape(28,28), cmap=\"Greys\")\n","ax[1][0].imshow(X_recovered[0].reshape(28,28), cmap=\"Greys\")\n","ax[1][1].imshow(X_recovered[5].reshape(28,28), cmap=\"Greys\")\n","\n","print(\"Images before and after compress-decompress operation:\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j-DscuwepXQj"},"source":["From the above plot, we can see that we won’t get back the original data, since the projection lost a bit of information, but it is close to the original data."]},{"cell_type":"markdown","metadata":{"id":"kg1nNu-ypXQl"},"source":["#### Kernel PCA"]},{"cell_type":"markdown","metadata":{"id":"fhc7M3yHpXQl"},"source":["Through kernel trick, we know that a linear decision boundary in the high-dimensional feature space corresponds to a complex nonlinear decision boundary in the original space. The same trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called Kernel PCA (kPCA).\n","\n","It preserves clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.\n","\n","Here, we use Scikit-Learn’s `KernelPCA` class to perform kPCA algorithm on Swiss roll dataset with two kernels: RBF (radial basis function) and sigmoid."]},{"cell_type":"code","metadata":{"id":"7Ui-9NKIpXQl"},"source":["# Load swiss roll dataset\n","n_points = 1000\n","X_swiss, y_swiss = make_swiss_roll(n_points, random_state=0)\n","X_swiss.shape, y_swiss.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6VHkU34pXQl"},"source":["# Visualize in 3D\n","fig = plt.figure(figsize=(10,8))\n","ax = plt.axes(projection='3d')\n","ax.scatter(X_swiss[:, 0], X_swiss[:, 1], X_swiss[:, 2], c=y_swiss, s=20, cmap='autumn')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n-booglfpXQm"},"source":["# KernelPCA with RBF kernel\n","rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04)\n","X_swiss_reduced = rbf_pca.fit_transform(X_swiss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"foszVJ7ApXQm"},"source":["# Visualize reduced data\n","plt.scatter(X_swiss_reduced[:, 0], X_swiss_reduced[:, 1], c=y_swiss, s=20, cmap='autumn')\n","plt.title(\"Using RBF kernel\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kOoRxSIApXQm"},"source":["# KernelPCA with 'sigmoid' kernel\n","sig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001)\n","X_swiss_reduced = sig_pca.fit_transform(X_swiss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E00PlfoZpXQm"},"source":["# Visualize reduced data\n","plt.scatter(X_swiss_reduced[:, 0], X_swiss_reduced[:, 1], c=y_swiss, s=20, cmap='autumn')\n","plt.title(\"Using Sigmoid kernel\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fI3XZmzIpXQn"},"source":["<font color='blue'>**Discussion 2:** What is the underlying principle of the working of the RBF kernel? </font>"]},{"cell_type":"markdown","metadata":{"id":"_ixEMJZGpXQn"},"source":["Other than KernelPCA we have more dimensionality reduction algorithms for non-linear datasets such as t-SNE."]},{"cell_type":"markdown","metadata":{"id":"0ZFLP2wJ4Lga"},"source":["### Introduction to t-SNE"]},{"cell_type":"markdown","metadata":{"id":"B6KrJt4sDEqk"},"source":["t-Distributed Stochastic Neighbor Embedding (t-SNE) is a popular choice for high-dimensional visualization. At a high level, t-SNE constructs a probability distribution for the high-dimensional samples in such a way that similar samples have a high likelihood of being picked while dissimilar points have an extremely small likelihood of being picked. Then, t-SNE defines a similar distribution for the points in the low-dimensional embedding. Finally, t-SNE minimizes the Kullback–Leibler divergence between the two distributions with respect to the locations of the points in the embedding.\n","\n","To know more about t-SNE technique, click [here](https://towardsdatascience.com/t-sne-python-example-1ded9953f26)."]},{"cell_type":"markdown","metadata":{"id":"gloEWlAANQ9h"},"source":["### Implement t-SNE "]},{"cell_type":"markdown","metadata":{"id":"FJwEQ7NVNZ67"},"source":["For this example, we’ll be working with hand drawn digits. The Scikit-Learn library provides a method for importing them into our program.\n","\n","We’re going to want to select either 2 or 3 for the number of components given that t-SNE is strictly used for visualization and we can only see things in up to 3 dimensions. On the other hand, perplexity is related to the number of nearest neighbors used in the algorithm. A different perplexity can cause drastic changes in the end results.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"fuTyu1fZQbEu"},"source":["#### Load Digits Dataset"]},{"cell_type":"markdown","metadata":{"id":"DDPf_33JQeeU"},"source":["Digits is a dataset of handwritten digits. Each feature is the intensity of one pixel of an 8 x 8 image (64 dimensions).\n","\n","Each data point is an 8x8 image of a digit.\n","\n","\n","\n","Dataset Characterstics      | Value \n","---------------------------|------------------\n","Classes                    | 10\n","Samples per Class          | ~180\n","Samples total              | 1797\n","Dimensionality             | 64\n","Features                   | integers 0-16\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"B85nDxBFdPyy"},"source":["# Load dataset\n","X, y = load_digits(return_X_y=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mL396tsjOONf"},"source":["Using the Scikit-Learn implementation of t-SNE."]},{"cell_type":"code","metadata":{"id":"IDvug7uHOTFg"},"source":["# Instantiate tsne from sklearn\n","tsne = TSNE()\n","\n","# Implement tsne\n","X_embedded = tsne.fit_transform(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jF3pCYkIOx3b"},"source":["# Visualize reduced dataset\n","sns.set(rc={'figure.figsize':(11.7,8.27)})\n","palette = sns.color_palette(\"bright\", 10)\n","\n","sns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y, legend='full', palette=palette)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MLnsvKCOPvjB"},"source":["As we can see, the model managed to take a 64-dimensional dataset and project it onto a 2-dimensional space in such a way that similar samples cluster together."]},{"cell_type":"markdown","metadata":{"id":"bFtNbv8XpXQr"},"source":["### Theory Questions"]},{"cell_type":"markdown","metadata":{"id":"NH1YCQYmpXQs"},"source":["1. What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?\n","\n"," The main motivations for dimensionality reduction are:\n","    \n","    * To speed up a subsequent training algorithm (in some cases it may even\n","remove noise and redundant features, making the training algorithm perform\n","better) \n","    * To visualize the data and gain insights on the most important features\n","    * To save space (compression)\n","\n"," The main drawbacks are:\n","\n","    * Some information is lost, possibly degrading the performance of subsequent\n","training algorithms.\n","    * It can be computationally intensive.\n","    * It adds some complexity to your Machine Learning pipelines.\n","    * Transformed features are often hard to interpret.\n","\n","2. What is the curse of dimensionality?\n","\n"," The curse of dimensionality refers to the fact that many problems that do not\n","exist in low-dimensional space arise in high-dimensional space. In Machine\n","Learning, one common manifestation is the fact that randomly sampled high-dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data.\n","\n","3. Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why?\n","\n"," Once a dataset’s dimensionality has been reduced using one of the algorithms we\n","discussed, it is almost always impossible to perfectly reverse the operation,\n","because some information gets lost during dimensionality reduction. Moreover,\n","while some algorithms (such as PCA) have a simple reverse transformation\n","procedure that can reconstruct a dataset relatively similar to the original, other algorithms (such as t-SNE) do not.\n","\n","4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?\n","\n"," PCA can be used to significantly reduce the dimensionality of most datasets, even if they are highly nonlinear, because it can at least get rid of useless dimensions. However, if there are no useless dimensions, as in a Swiss roll dataset, then reducing dimensionality with PCA will lose too much information. You want to unroll the Swiss roll, not squash it.\n","\n","5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained\n","variance ratio to 95%. How many dimensions will the resulting dataset have?\n","\n"," That’s a trick question: it depends on the dataset. Let’s look at two extreme examples. First, suppose the dataset is composed of points that are almost perfectly aligned. In this case, PCA can reduce the dataset down to just one dimension while still preserving 95% of the variance. Now imagine that the dataset is composed of perfectly random points, scattered all around the 1,000 dimensions. In this case roughly 950 dimensions are required to preserve 95% of the variance. So the answer is, it depends on the dataset, and it could be any number between 1 and 950. Plotting the explained variance as a function of the number of dimensions is one way to get a rough idea of the dataset’s intrinsic dimensionality.\n","\n","6. How can you evaluate the performance of a dimensionality reduction algorithm\n","on your dataset?\n","\n"," Intuitively, a dimensionality reduction algorithm performs well if it eliminates a lot of dimensions from the dataset without losing too much information. One way to measure this is to apply the reverse transformation and measure the reconstruction error. However, not all dimensionality reduction algorithms provide a reverse transformation. Alternatively, if you are using dimensionality reduction as a preprocessing step before another Machine Learning algorithm (e.g., a Random Forest classifier), then you can simply measure the performance of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset."]}]}