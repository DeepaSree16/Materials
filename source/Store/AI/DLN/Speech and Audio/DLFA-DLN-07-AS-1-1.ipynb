{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "M3_AST_21_Text_to_Speech_Models_C.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgLag1Euy3H"
      },
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment 21 : Implementation of Text-to-Speech (TTS) Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tdtrlAhvIHY"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment, you will be able to\n",
        "\n",
        "* understand Tacotron, Fastspeech, and Glowtts\n",
        "* use pretrained models to build Text-to-speech model and evaluate them\n",
        "* perform model training for Tacotron2\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AdonhbE2xWt4"
      },
      "source": [
        "### **Dataset Description**\n",
        "\n",
        "The LJSpeech dataset is a public domain speech dataset consisting of 13,100 short audio clips of a single speaker reading passages from 7 non-fiction books. A transcription is provided for each clip. Clips vary in length from 1 to 10 seconds and have a total length of approximately 24 hours.\n",
        "\n",
        "The texts were published between 1884 and 1964, and are in the public domain. The audio was recorded in 2016-17 by the LibriVox project and is also in the public domain.\n",
        "\n",
        "The datasets can be downloaded from the following [link](https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhdMHT-qRiqs"
      },
      "source": [
        "#### **About DataSet**\n",
        "\n",
        "1. The audio clips range in length from approximately 1 second to 10 seconds. They were segmented automatically based on silences in the recording. Clip boundaries generally align with sentence or clause boundaries, but not always.\n",
        "\n",
        "2. The text was matched to the audio manually, and a QA pass was done to ensure that the text accurately matched the words spoken in the audio.\n",
        "\n",
        "3. The original LibriVox recordings were distributed as 128 kbps MP3 files. As a result, they may contain artifacts introduced by the MP3 encoding.\n",
        "\n",
        "#### **File Format**\n",
        "\n",
        "Metadata is provided in transcripts.csv. This file consists of one record per line, delimited by the pipe character (0x7c). The fields are:\n",
        "\n",
        "1. **ID**: This is the name of the corresponding .wav file\n",
        "2. **Transcription**: Words spoken by the reader (UTF-8)\n",
        "3. **Normalized Transcription**: Transcription with numbers, ordinals, and monetary units expanded into full words (UTF-8).\n",
        "\n",
        "Each audio file is a single-channel 16-bit PCM WAV with a sample rate of 22050 Hz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8Onl8YF9mR9"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exG368oL8jv2",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "  \n",
        "notebook= \"M3_AST_21_Text_to_Speech_Models_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    #ipython.magic(\"sx wget \")\n",
        "    #ipython.magic(\"sx wget \")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "    \n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None        \n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "    \n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:        \n",
        "        print(r[\"err\"])\n",
        "        return None   \n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "    \n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional: \n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional  \n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "  \n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "  \n",
        "  \n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "  \n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError \n",
        "    else: \n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "  \n",
        "\n",
        "def getId():\n",
        "  try: \n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup \n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup() \n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZbawFqbHqGU"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzFogiCkM8kQ"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "def warn(*args, **kwargs):\n",
        "    pass\n",
        "import warnings\n",
        "warnings.warn = warn\n",
        "import os, sys\n",
        "from os.path import exists, join, basename, splitext\n",
        "\n",
        "# Downloading the github repository\n",
        "\n",
        "tacotron_git_repo_url = 'https://github.com/NVIDIA/tacotron2.git'\n",
        "fastspeech_git_repo_url = 'https://github.com/Talentsprint2011/FastSpeech.git'\n",
        "glowtts_git_repo_url = 'https://github.com/Talentsprint2011/glowtts.git'\n",
        "TTS_utils_repo_url = 'https://github.com/Talentsprint2011/ASR_TTS.git'\n",
        "\n",
        "# Splitting the text to get the name of the model\n",
        "tacotron_project_name = splitext(basename(tacotron_git_repo_url))[0]\n",
        "fastspeech_project_name = splitext(basename(fastspeech_git_repo_url))[0]\n",
        "glowtts_project_name = splitext(basename(glowtts_git_repo_url))[0]\n",
        "TTS_utils_project_name = splitext(basename(TTS_utils_repo_url))[0]\n",
        "\n",
        "if not exists(tacotron_project_name):\n",
        "  !git clone -q --recursive {tacotron_git_repo_url}\n",
        "  !cd {tacotron_project_name}/waveglow && git checkout 9168aea\n",
        "  !pip install -q librosa unidecode\n",
        "\n",
        "if not exists(fastspeech_project_name):\n",
        "    !git clone -q --recursive {fastspeech_git_repo_url}\n",
        "\n",
        "if not exists(glowtts_project_name):\n",
        "    !git clone -q --recursive {glowtts_git_repo_url}\n",
        "\n",
        "if not exists(TTS_utils_project_name):\n",
        "    !git clone -q --recursive {TTS_utils_repo_url}\n",
        "\n",
        "!pip install git+https://github.com/Talentsprint2011/ASR_TTS.git\n",
        "sys.path.append(join(tacotron_project_name, 'waveglow/'))\n",
        "sys.path.append(tacotron_project_name)\n",
        "sys.path.append(fastspeech_project_name)\n",
        "sys.path.append(glowtts_project_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gmp1M5EddwH4"
      },
      "source": [
        "import json\n",
        "import time\n",
        "import torch\n",
        "import random\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import IPython.display as ipd\n",
        "import matplotlib.pylab as plt\n",
        "from scipy.io import wavfile\n",
        "from IPython.display import clear_output\n",
        "import ipywidgets as widgets\n",
        "from tqdm import tqdm\n",
        "from ASR_TTS import tts\n",
        "from ASR_TTS.utils import plot_data, plot_data_3x, download_all_models\n",
        "# Text processing toolkit\n",
        "!pip install unidecode\n",
        "plt.rcParams[\"axes.grid\"] = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brd5HqaP1cfG"
      },
      "source": [
        "### Loading the pretrained models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDpHm4VpDVxW"
      },
      "source": [
        "def download_fn(file_id, file_name):\n",
        "  !rm -f ./cookie\n",
        "  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id={file_id}\" > /dev/null\n",
        "  confirm_text = !awk '/download/ {print $NF}' ./cookie\n",
        "  confirm_text = confirm_text[0]\n",
        "  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm={confirm_text}&id={file_id}\" -o {file_name}\n",
        "tacotron2_pretrained, fastspeech_pretrained, glowtts_pretrained, waveglow_pretrained  = download_all_models(download_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--MOeTVE_uxo"
      },
      "source": [
        "# Evaluate TTS model with different datasets\n",
        "tts.list_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NznfmT5q_vMv"
      },
      "source": [
        "# Listing the model names, the pretrained model and links used \n",
        "tts.list_models()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAhPRCBxdQ2o"
      },
      "source": [
        "### **TACOTRON2**\n",
        "\n",
        "A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. Here, we are using **Tacotron**, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given `<text, audio>` pairs, the model can be trained completely from scratch with random initialization. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness.It does not require phoneme-level alignment, so it can easily scale to using large amounts of acoustic data with transcripts. In addition, since Tacotron generates speech at the frame level, it’s substantially faster than sample-level autoregressive methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnF76trxQZYF"
      },
      "source": [
        "#### Model Architecture\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/tacotron2%20architecture1.png\" width=600px height=400px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "To know more about the architecture click [here](https://arxiv.org/pdf/1712.05884.pdf).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNzx8MvoVXqR"
      },
      "source": [
        "Let us import all the packages required to implement tacotron2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59i_tuYTgkbn"
      },
      "source": [
        "from hparams import create_hparams\n",
        "from model import Tacotron2\n",
        "from layers import TacotronSTFT\n",
        "from audio_processing import griffin_lim\n",
        "from text import text_to_sequence\n",
        "from denoiser import Denoiser\n",
        "from glow import WaveGlow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm1no0XHVdLK"
      },
      "source": [
        "Initialize **waveglow** with the pretrained model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpkzY6xx_6tH"
      },
      "source": [
        "# Loading waveglow with tacotron\n",
        "waveglow_config = json.load(open('%s/waveglow/config.json' % tacotron_project_name))['waveglow_config']\n",
        "\n",
        "# Loading the hyperparameters\n",
        "waveglow = WaveGlow(**waveglow_config)\n",
        "\n",
        "# Loading the pretrained model from its path\n",
        "waveglow.load_state_dict(torch.load(waveglow_pretrained)['model'].state_dict())\n",
        "_ = waveglow.cuda().eval()\n",
        "for k in waveglow.convinv:\n",
        "    k.float()\n",
        "denoiser = Denoiser(waveglow) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeQB9fvKNTq"
      },
      "source": [
        "Further, we will initialize tacotron2 with the pretrained model. Here, we are defining the hyperparameter and loading the model according to the hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__HB_1gL2w08"
      },
      "source": [
        "torch.set_grad_enabled(False)\n",
        "hparams = create_hparams()\n",
        "hparams.sampling_rate = 22050\n",
        "tacotron_model = Tacotron2(hparams)\n",
        "tacotron_model.load_state_dict(torch.load(tacotron2_pretrained)['state_dict'])\n",
        "_ = tacotron_model.cuda().eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3jnejf5yKWBZ"
      },
      "source": [
        "Synthesize a text or replace the TEXT with your text if you want try out another one.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nkICTcoX8U2"
      },
      "source": [
        "TEXT = \"Today's assignment is on text to speech\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tChQ8J9YWKl"
      },
      "source": [
        "Here, we will creates a tensor representation of the input text sequence `(example_text)` using `tacotron2.text_to_sequence()`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMjSfLkaX8KE"
      },
      "source": [
        "sequence = np.array(text_to_sequence(TEXT, ['english_cleaners']))[None, :]\n",
        "sequence = torch.autograd.Variable(torch.from_numpy(sequence)).long()\n",
        "sequence = sequence.cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52bVbnNDU83X"
      },
      "source": [
        "#### Mel Spectogram\n",
        "\n",
        "Before moving ahead, you should know about **Mel Spectogram**.\n",
        "\n",
        "Mel spectrogram is a spectrogram that is converted to a Mel scale. Then, what is the spectrogram and The Mel Scale? Let us have a look at it.\n",
        "\n",
        "**The Mel Scale**\n",
        "\n",
        "Studies have shown that humans do not perceive frequencies on a linear scale. We are better at detecting differences in lower frequencies than higher frequencies. For example, we can easily tell the difference between 500 and 1000 Hz, but we will hardly be able to tell a difference between 10,000 and 10,500 Hz, even though the distance between the two pairs are the same.\n",
        "In 1937, Stevens, Volkmann, and Newmann proposed a unit of pitch such that equal distances in pitch sounded equally distant to the listener. This is called the **mel scale**. We perform a mathematical operation on frequencies to convert them to the mel scale.\n",
        "\n",
        "<center>\n",
        "<img src=\"\n",
        "https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/mel%20scale.gif\" width=400px height=350px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "**The Mel Spectrogram**\n",
        "\n",
        "A mel spectrogram is a spectrogram where the frequencies are converted to the mel scale and the representation of a mel spectogram is given below.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/mel%20spectogram.png\" width=400px height=300px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLQOphVRKkdB"
      },
      "source": [
        "Now, we will convert the text into mel spectrogram using tacotron2 and plot it. It is a commonly used feature representation which acts as the target in the speech synthesis.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0CD0a7ZKhX3"
      },
      "source": [
        "mel_outputs, mel_outputs_postnet, _, alignments = tacotron_model.inference(sequence)\n",
        "plot_data_3x((mel_outputs.data.cpu().numpy()[0],\n",
        "           mel_outputs_postnet.data.cpu().numpy()[0],\n",
        "           alignments.data.cpu().numpy()[0].T))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL_g_5qoGxDw"
      },
      "source": [
        "**First plot**: It is the final output of the tacotron2 model. \n",
        "\n",
        "**Second plot**: It is the output before the postnet layer (last layer in the model architecture).\n",
        "\n",
        "**Third plot**: It is the alignment matrix and it is representing how the input text represents and the output speech feature frames are aligned. \n",
        "Here, the `x-axis` is the speech part and `y-axis` is the text part."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdpAKywQYnsp"
      },
      "source": [
        "`waveglow.infer()` - Waveglow generates sound given the mel spectrogram."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UurAMiN-cqBd"
      },
      "source": [
        "audio = waveglow.infer(mel_outputs_postnet, sigma=0.666)\n",
        "# Using the audio and playing it\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qswLWJpRd5Yl"
      },
      "source": [
        "# Removing waveglow bias: Removing any noise from waveglow predictions\n",
        "audio_denoised = denoiser(audio, strength=0.01)[:, 0]\n",
        "ipd.Audio(audio_denoised.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgjUsof3dM7U"
      },
      "source": [
        "### **FastSpeech**\n",
        "\n",
        "A non-autoregressive text to speech (TTS) models such as **FastSpeech** can synthesize speech significantly faster than previous autoregressive models with comparable quality.\n",
        "\n",
        "Here, we will understand the architecture design of FastSpeech. To generate a target melspectrogram sequence in parallel, we design a novel feed-forward structure, instead of using the encoder-attention-decoder based architecture as adopted by most sequence to sequence based autoregressive [14, 22, 25] and non-autoregressive [7, 8, 26] generation. The overall model architecture of\n",
        "FastSpeech is shown in the below figure. Going further, We describe the components in detail.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/fastspeech.png\" width=600px height=300px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "The overall architecture for FastSpeech is as follows: \n",
        "\n",
        "1. The feed-forward Transformer\n",
        "\n",
        "2. The feed-forward Transformer block\n",
        "\n",
        "3. The length regulator\n",
        "\n",
        "4. The duration predictor. \n",
        "\n",
        "MSE loss denotes the loss between predicted and extracted duration, which only exists in the training process. NOw, let us discuss about these architecture elements.\n",
        "\n",
        "**1. Feed-Forward Transformer**\n",
        "\n",
        "The architecture for FastSpeech is a feed-forward structure based on self-attention in Transformer and 1D convolution . We call this structure as Feed-Forward Transformer (FFT), as shown in the figure 1(a). Feed-Forward Transformer stacks multiple **FFT blocks** for phoneme to mel-spectrogram transformation, with N blocks on the phoneme side, and N blocks on the mel-spectrogram side, with a length regulator in between to bridge the length gap between the phoneme and mel-spectrogram sequence. Each FFT block consists of a self-attention and 1D convolutional network, as shown in Figure 1(b). The self-attention network consists of a multi-head attention to extract the cross-position information. Different from the 2-layer dense network in\n",
        "Transformer, we use a 2-layer 1D convolutional network with ReLU activation. The motivation is that the adjacent hidden states are more closely related in the character/phoneme and mel-spectrogram sequence in speech tasks. We evaluate the effectiveness of the 1D convolutional network in the experimental section. Following Transformer, residual connections, layer normalization, and dropout are added after the self-attention network and 1D convolutional network respectively.\n",
        "\n",
        "**2. Length Regulator**\n",
        "\n",
        "The length regulator (Figure 1c) is used to solve the problem of length mismatch between the phoneme and spectrogram sequence in the Feed-Forward Transformer, as well as to control the voice speed. The length of a phoneme sequence is usually smaller than that of its mel-spectrogram sequence, and each phoneme corresponds to several mel-spectrograms. We refer to the length of the mel-spectrograms that corresponds to a phoneme as the phoneme duration \n",
        "\n",
        "**3. Duration Predictor**\n",
        "\n",
        "Phoneme duration prediction is important for the length regulator. As shown in Figure 1d, the duration predictor consists of a 2-layer 1D convolutional network with ReLU activation, each followed by the layer normalization and the dropout layer, and an extra linear layer to output a scalar, which\n",
        "is exactly the predicted phoneme duration. Note that this module is stacked on top of the FFT blocks on the phoneme side and is jointly trained with the FastSpeech model to predict the length of 4 mel-spectrograms for each phoneme with the mean square error (MSE) loss. We predict the length in\n",
        "the logarithmic domain, which makes them more Gaussian and easier to train. Note that the trained duration predictor is only used in the TTS inference phase, because we can directly use the phoneme duration extracted from an autoregressive teacher model in training.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVd2U3L2ZSls"
      },
      "source": [
        "Importing the required packages for implementation FastSpeech. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm0kmzL9lhVz"
      },
      "source": [
        "from FastSpeech.fs_eval import get_DNN \n",
        "from FastSpeech import fs_hparams as hp\n",
        "from FastSpeech import text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO9rl1IPtGtq"
      },
      "source": [
        "# Loading the FastSpeech model\n",
        "fastspeech_model = get_DNN(fastspeech_pretrained)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNpmNOQZZPQy"
      },
      "source": [
        "Synthesize a text or replace the TEXT with your text if you want try out another one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncZkAS6lMDXi"
      },
      "source": [
        "TEXT = [\"Today's assignment is on text to speech\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeQCuAk3MDLK"
      },
      "source": [
        "# Creating a tensor representation of the input text sequence\n",
        "TEXT = [text.text_to_sequence(t, hp.text_cleaners) for t in TEXT]\n",
        "text_ = np.array(TEXT[0])\n",
        "text_ = np.stack([text_])\n",
        "\n",
        "# Extracting the number of the character in the text sequence\n",
        "src_pos = np.array([i+1 for i in range(text_.shape[1])])\n",
        "src_pos = np.stack([src_pos])\n",
        "\n",
        "# Converting numpy array into torch tensor and loading it to GPU\n",
        "sequence = torch.from_numpy(text_).cuda().long()\n",
        "src_pos = torch.from_numpy(src_pos).cuda().long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NllE8aPilm36"
      },
      "source": [
        "# Predicting the mel sepctogram from tensor representation of text\n",
        "with torch.no_grad():\n",
        "    _, mel = fastspeech_model.module.forward(sequence, src_pos, alpha=1)\n",
        "mel = mel.contiguous().transpose(1, 2).squeeze().cpu().numpy()\n",
        "plot_data(mel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5kee_jfMaK7"
      },
      "source": [
        "# Convert the generated mel spectrogram into an audio\n",
        "audio = waveglow.infer(torch.from_numpy(mel).unsqueeze(0).cuda(), sigma=0.666)\n",
        "ipd.Audio(audio[0].data.cpu().numpy(), rate=hparams.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGaaSzDZ2uq2"
      },
      "source": [
        "### **Glow-TTS: A Generative Flow for Text-to-Speech**\n",
        "\n",
        "Let us first understand why there is a necessity of another model which is Glow-TTS.\n",
        "\n",
        "**Performance of other models**\n",
        "\n",
        "Text-to-Speech (TTS) is the task to generate speech from text, and deep-learning-based TTS models have succeeded in producing natural speech indistinguishable from human speech. Among neural TTS models, autoregressive models such as Tacotron 2 show the state-of-the-art performance. Based on these autoregressive models, there have been many advances in generating diverse speech in terms of modelling different speaking styles or various prosodies.\n",
        "\n",
        "**Problem with Autoregressive TTS models**\n",
        "\n",
        "espite the high quality of autoregressive TTS models, there are a few difficulties in deploying end-to-end autoregressive models directly in real-time services. As the synthesizing time of the models grows linearly with the output length, undesirable delay caused by generating long speech can be propagated to the multiple pipelines of TTS systems without designing sophisticated frameworks. In addition, most of the autoregressive models show lack of robustness in some cases. For example, when the input text includes the repeated words, autoregressive TTS models often produce serious attention errors.\n",
        "\n",
        "To overcome such limitations of the autoregressive TTS models, parallel TTS models such as **FastSpeech** have been proposed. These models can synthesize mel-spectrogram significantly faster than the autoregressive TTS models. In addition to the fast sampling, FastSpeech reduces the failure cases for the extremely hard sentences by enforcing its alignment monotonic.\n",
        "\n",
        "**Problem with FastSpeech**\n",
        "\n",
        "However, these strengths of parallel TTS models come from the well-aligned attention map between text and speech, which is extracted from their external aligner. Recently proposed parallel models address these challenges by extracting attention maps from the pre-trained autoregressive models. Therefore, the performance of the parallel TTS models critically depends on that of the autoregressive TTS models. Furthermore, since the parallel TTS models assume this alignment to be given during training, they cannot be trained without the external aligners.\n",
        "\n",
        "Here, our goal is to eliminate the necessity of any external aligner from the training procedure of parallel TTS models. Therefore, **Glow-TTS** comes into consideration.\n",
        "\n",
        "**What is Glow-TTS?**\n",
        "\n",
        "Glow-TTS is a flow-based generative model for parallel TTS that can internally learn its own alignment. Glow-TTS is directly trained to maximize the log-likelihood of speech given text, and its sampling process is totally parallel due to the properties of the generative flow. In order to eliminate any dependency on other networks, we introduce Monotonic Alignment Search (MAS), a novel method to search for the most probable monotonic alignment with only text and latent representation of speech. This internal alignment search algorithm simplifies the entire training procedure of our parallel TTS models so that it requires only 3 days for training on two GPUs.\n",
        "\n",
        "Without any external aligner, our parallel TTS model can generate mel-spectrograms 15.7 times faster than the autoregressive TTS model, Tacotron 2, while maintaining the comparable performance. Glow-TTS also provides diverse speech synthesis, in contrast to other TTS models, which have their stochasticities only in dropout operations. We can control some properties of synthesized samples from Glow-TTS by altering the latent variable of normalizing flows.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4p2nbF8bvPSS"
      },
      "source": [
        "#### Loading all the prerequisites for Glow-TTS "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB8riQXLrkrH"
      },
      "source": [
        "%cd glowtts/monotonic_align \n",
        "!python setup.py build_ext --inplace\n",
        "%cd /content/\n",
        "from text import text_to_sequence, cmudict\n",
        "from text.symbols import symbols\n",
        "from glowtts import glowtts_attentions, glowtts_modules, glowtts_models, glowtts_utils, glowtts_commons\n",
        "from glowtts.text import text_to_sequence\n",
        "# Copying the configuration files to be used by the pretrained models\n",
        "!cp glowtts/configs/base.json glowtts/configs/config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrWDWQCnroaq"
      },
      "source": [
        "# Loading the hyperparameter\n",
        "hps = glowtts_utils.get_hparams_from_dir('glowtts/configs/')\n",
        "\n",
        "# Normalizing the output audio\n",
        "def normalize_audio(x, max_wav_value=hps.data.max_wav_value):\n",
        "    return np.clip((x / np.abs(x).max()) * max_wav_value, -32768, 32767).astype(\"int16\")\n",
        "\n",
        "# Defining the Glow-TTS model\n",
        "glowtts_model = glowtts_models.FlowGenerator(\n",
        "    len(symbols) + getattr(hps.data, \"add_blank\", False),\n",
        "    out_channels=hps.data.n_mel_channels,\n",
        "    **hps.model).to(\"cuda\")\n",
        "\n",
        "# Loading a pretrained Glow-TTS model\n",
        "glowtts_utils.load_checkpoint(glowtts_pretrained, glowtts_model)\n",
        "glowtts_model.decoder.store_inverse() \n",
        "_ = glowtts_model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEH87CR_K9vC"
      },
      "source": [
        "Synthesize a text or replace the TEXT with your text if you want try out another one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-dJ4Xg-NvFt"
      },
      "source": [
        "tst_stn = \"Today's assignment is on text to speech\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6MX-Q5Yxirm"
      },
      "source": [
        "Preprocssing steps for input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKvhyRK3erET"
      },
      "source": [
        "cmu_dict = cmudict.CMUDict('glowtts/data/cmu_dictionary')\n",
        "#cmudict = cmudict.CMUDict('glow_tts/data/cmu_dictionary')\n",
        "if getattr(hps.data, \"add_blank\", False):\n",
        "    text_norm = text_to_sequence(tst_stn.strip(), ['english_cleaners'], cmu_dict)\n",
        "    text_norm = commons.intersperse(text_norm, len(symbols))\n",
        "else:\n",
        "    tst_stn = \" \" + tst_stn.strip() + \" \"\n",
        "    text_norm = text_to_sequence(tst_stn.strip(), ['english_cleaners'], cmu_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2qLHY2QUxs1l"
      },
      "source": [
        "Tensor representation of the text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhXauMdDrt2X"
      },
      "source": [
        "sequence = np.array(text_norm)[None, :]\n",
        "x_tst = torch.autograd.Variable(torch.from_numpy(sequence)).cuda().long()\n",
        "x_tst_lengths = torch.tensor([x_tst.shape[1]]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai11-iMMxwh6"
      },
      "source": [
        "Predicting the mel sepctogram from tensor representation of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJNk9Tebrvvo"
      },
      "source": [
        "with torch.no_grad():\n",
        "  noise_scale = .667\n",
        "  length_scale = 1.0\n",
        "  (y_gen_tst, *_), *_, (attn_gen, *_) = glowtts_model(x_tst, x_tst_lengths, gen=True, noise_scale=noise_scale, length_scale=length_scale)\n",
        "plot_data(y_gen_tst.squeeze().cpu().numpy())\n",
        "plot_data(attn_gen.squeeze().cpu().numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk9BovQqM8o4"
      },
      "source": [
        "# Convert the generated mel spectrogram into an audio\n",
        "audio = waveglow.infer(y_gen_tst, sigma=.666)\n",
        "ipd.Audio(normalize_audio(audio[0].clamp(-1,1).data.cpu().float().numpy()), rate=hps.data.sampling_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3qVVk5mrwdx"
      },
      "source": [
        "Defining a function `batch_process` which takes the input as the `model name` and the `TEXT`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ljrvif5p_el8"
      },
      "source": [
        "# Preprocessing the input text for any given model\n",
        "def batch_process(modelname, TEXT, device='cuda'): # device = 'cuda' or 'cpu'\n",
        "    text_sequences = []\n",
        "    text_positions = []\n",
        "    if modelname == 'tacotron2':\n",
        "        for TEXT_ in TEXT:\n",
        "            sequence = np.array(text_to_sequence(TEXT_, ['english_cleaners']))[None, :]\n",
        "            sequence = torch.autograd.Variable(torch.from_numpy(sequence)).long()\n",
        "            text_sequences.append(sequence.to(device))\n",
        "            \n",
        "    elif modelname == 'fastspeech':\n",
        "        for TEXT_ in TEXT:\n",
        "            TEXT_ = np.stack([np.array(text.text_to_sequence(TEXT_, hp.text_cleaners))])\n",
        "            src_pos = np.stack([np.array([i+1 for i in range(TEXT_.shape[1])])])\n",
        "            text_positions.append(torch.from_numpy(src_pos).cuda().long())\n",
        "            text_sequences.append(torch.from_numpy(TEXT_).cuda().long())\n",
        "\n",
        "    elif modelname == 'glowtts':\n",
        "        cmu_dict = cmudict.CMUDict('glowtts/data/cmu_dictionary')\n",
        "        for TEXT_ in TEXT: \n",
        "            if getattr(hps.data, \"add_blank\", False):\n",
        "                text_norm = text_to_sequence(TEXT_.strip(), ['english_cleaners'], cmu_dict)\n",
        "                text_norm = commons.intersperse(text_norm, len(symbols))\n",
        "            else:\n",
        "                TEXT_ = \" \" + TEXT_.strip() + \" \"\n",
        "                text_norm = text_to_sequence(TEXT_.strip(), ['english_cleaners'], cmu_dict)\n",
        "            sequence = np.array(text_norm)[None, :]\n",
        "            text_sequences.append(torch.autograd.Variable(torch.from_numpy(sequence)).to(device).long())\n",
        "            text_positions.append(torch.tensor([x_tst.shape[1]]).to(device))\n",
        "    else:\n",
        "        raise NotImplementedError\n",
        "    return text_sequences, text_positions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZwsS_-AsPkm"
      },
      "source": [
        "Making predictions for the model using the input text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEOzIt5AsO4X"
      },
      "source": [
        "# Predictions from the preprocessed text into mel spectograms and waveforms\n",
        "def predict(modelname, input_text, positions):\n",
        "    predicted = []\n",
        "    wav = []\n",
        "    num_samples = len(input_text)\n",
        "    for i in tqdm(range(num_samples)):\n",
        "        if modelname == 'tacotron2':\n",
        "            predited_, _, _, _ = tacotron_model.inference(input_text[i])\n",
        "            predicted.append(predited_)\n",
        "        elif modelname == 'fastspeech':\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                _, predicted_ = fastspeech_model.module.forward(input_text[i], positions[i], alpha=1)\n",
        "                predicted.append(predicted_.contiguous().transpose(1, 2))\n",
        "        elif modelname == 'glowtts':\n",
        "            (predicted_, *_), *_, (_, *_) = glowtts_model(input_text[i], positions[i], gen=True, noise_scale=0.667, length_scale=1.0)\n",
        "            predicted.append(predicted_)\n",
        "        else:\n",
        "            raise NotImplementedError\n",
        "        \n",
        "        wav.append(waveglow.infer(predicted[-1], sigma=.666).cpu().numpy())\n",
        "    return predicted, wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JUsgWyNbJ0IV"
      },
      "source": [
        "#### Downloading the LJSpeech Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_YQ-ljtsZyh"
      },
      "source": [
        "if not os.path.exists('LJSpeech-1.1/'):\n",
        "    !wget https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/LJSpeech-1.1.tar.bz2\n",
        "    !tar xf LJSpeech-1.1.tar.bz2\n",
        "    \n",
        "ljspeech_path = 'LJSpeech-1.1/wavs/'\n",
        "\n",
        "test_paths = {'common_voice_sentences':'ASR_TTS/ASR_TTS/common_voice_sentences_for_TTS.txt',\n",
        "              'fastspeech_hard_sentences':'ASR_TTS/ASR_TTS/tts_hard_sentences_fs.txt',\n",
        "              'ljspeech_sentences':'tacotron2/filelists/ljs_audio_text_test_filelist.txt'}\n",
        "\n",
        "test_dataset = 'fastspeech_hard_sentences'\n",
        "modelname = 'fastspeech'\n",
        "\n",
        "# Creating a dictionary of input text as well as the audios\n",
        "test_dict = tts.extract_test_pairs(test_dataset, test_paths, ljspeech_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnZOwL5L_jf7"
      },
      "source": [
        "# Calling the batch process function to get text_sequences, text positions\n",
        "text_sequences, text_positions = batch_process(modelname=modelname, TEXT=test_dict.values())\n",
        "# Calling the predict function to get predicted_mel and wavs\n",
        "predicted_mel, wavs = predict(modelname=modelname, input_text=text_sequences, positions=text_positions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZktWiGCONzc0"
      },
      "source": [
        "# Here we can listen the generated audio and the index represents which text file to play\n",
        "index = 35\n",
        "test_ = list(test_dict.values())[index]\n",
        "print(test_)\n",
        "ipd.Audio(wavs[index], rate=22050)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5F3UIWtNt5H"
      },
      "source": [
        "**MOS Score** \n",
        "\n",
        " **MOS, or, Mean Opinion Score**, is a measure of voice quality and is a quality measure that has been used in telephony for decades as a way to assess human users’ opinion of call quality. The test is used widely in VoIP networks to ensure quality voice transmission, test for quality issues, and provides a metric by which to measure voice degradation and performance.  \n",
        "\n",
        "The most commonly used rating scale is the Absolute Category Ranking (ACR) scale, which ranges from 1 to 5. The levels of the Absolute Category Ranking are:\n",
        "\n",
        "5 - Excellent\n",
        "\n",
        "4 - Good\n",
        "\n",
        "3 - Fair\n",
        "\n",
        "2 - Poor\n",
        "\n",
        "1 - Bad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y70HCntmhEnq"
      },
      "source": [
        "# Preparing the test set predictions for MOS evaluation\n",
        "# Here, the num_samples is the number of files to be considered for evaluation\n",
        "\n",
        "args = tts.Args(test_dataset, modelname,num_samples=40)\n",
        "\n",
        "audio_for_mos, text_for_mos = args.prepare_mos_files(wavs, list(test_dict.values()), ljspeech_path)   \n",
        "mos_scoring = tts.mos_scoring\n",
        "def clicked(arg):\n",
        "    if args.i > len(audio_for_mos)-1: \n",
        "        clear_output()\n",
        "        print(f'Evaluation done on {args.num_samples} samples!')\n",
        "        args.score.append(mos_scoring.value)\n",
        "        args.final_scores()\n",
        "    else:\n",
        "        args.score.append(mos_scoring.value)\n",
        "        clear_output()\n",
        "        print(text_for_mos[args.i])\n",
        "        display_audio = ipd.Audio(audio_for_mos[args.i], rate=22050)\n",
        "        display(display_audio)\n",
        "        display(mos_scoring)\n",
        "        args.i += 1\n",
        "        display(next_button)\n",
        "    \n",
        "# Both the generated audio as well as actual audio from LJSpeech are mixed together. Once the MOS score have been provided\n",
        "# for all the files the average MOS score for the generated as well as actual audio will be displayed\n",
        "\n",
        "next_button = widgets.Button(description = 'Next')   \n",
        "next_button.on_click(clicked)\n",
        "display(next_button)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKX_WV5occ3u"
      },
      "source": [
        "### Training the TTS Model : Tacotron2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4KaQFJSPLcN"
      },
      "source": [
        "Previously, we just evaluated different pretrained models based on the predictions (audio). Now, we have 78 text audio pairs that we will consider for training the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElXht1BvbHA-"
      },
      "source": [
        "# Cloning the repository which has the training code\n",
        "%tensorflow_version 1.x\n",
        "!git clone https://github.com/Talentsprint2011/tacotron2_train_batch.git\n",
        "!pip install unidecode"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emKWqAvqbKRS"
      },
      "source": [
        "%cd /content/tacotron2_train_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRUTotjVQSbR"
      },
      "source": [
        "Here, the 78 audio text pairs are trained. At the end of each epoch the generated mel spectogram as well as their alignment matrix for one sample is stored.\n",
        "\n",
        "Keep traing  the model for atleast 1 or 2 hrs and look into the results periodically to undertand the imporovement.\n",
        "\n",
        "'''\n",
        "mel spec folder: /content/tacotron2_train_batch/predicted_mel\n",
        "\n",
        "alignment folder: /content/tacotron2_train_batch/alignment_plots\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSfImUOnbVlw"
      },
      "source": [
        "!python3 train.py --output_directory=outdir --log_directory=logdir"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULZBRxmFRlTU"
      },
      "source": [
        "Finally, this gives you an idea about how to train **Text-to-speech** models and how to monitor the progress and see if the model is learning or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHDYadgG-RHl",
        "cellView": "form"
      },
      "source": [
        "#@title Q.1. State True or False : Deep Voice 3 uses RNN in place of  residual gated convolution to capture contextual information.  \n",
        "Answer1 = \"\" #@param [\"\",\"True\",\"False\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Qu3sufDD7pu1"
      },
      "source": [
        "#@title Q.2. What does the CBHG network include? \n",
        "Answer2 = \"\" #@param [\"\",\"highway network, 2-D convolutional bank and Unidirectional-GRU\",\"highway network, 1-D convolutional bank and Bidirectional-GRU\",\"highway network, 1-D convolutional bank and Unidirectional-GRU\",\"highway network, 2-D convolutional bank and Bidirectional-GRU\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"\" #@param {type:\"string\"}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-"
      },
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}