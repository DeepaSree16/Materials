{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-23-AS-2-4.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"zM2PmW98nc3U"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"jFUjPQMAn_n3"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"0Xq1LLdMoDwa"},"source":["At the end of the experiment you will be able to :\n","\n","- perform video classification with Keras and Deep Learning\n"]},{"cell_type":"code","metadata":{"id":"OvDJsz_mnH9j","cellView":"form"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"840\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/Sports_Video_Classification.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5xJZ6po7p17R"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"DnBNowAkXGVL"},"source":["### Description\n","\n","\n","This is a hand-made dataset which contains images of various sports activities.\n","\n","The dataset comprises of 3 sports activities:\n","\n","    boxing\n","    swimming\n","    table tennis\n","\n","There are around 2,100 images, in which each category comprises of around 700 pictures.\n","\n","Additionally to test the model, **two sample videos** were provided to perform classification"]},{"cell_type":"markdown","metadata":{"id":"e6aDGyYbOeOy"},"source":["### Transfer Learning\n","\n","Transfer learning is a machine learning technique in which a network that has been trained to perform a specific task is being reused (repurposed) as a starting point for another similar task."]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/mvsr/videodataset.zip\")\n","! unzip videodataset.zip\")"],"metadata":{"id":"p9dSc5hFZTtw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GFPkgHv_RULY"},"source":["### 1. Importing required packages"]},{"cell_type":"code","metadata":{"id":"QUhD2KVj4TIp"},"source":["import cv2\n","import math\n","import statistics\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from keras.applications.vgg16 import VGG16\n","from keras.models import Sequential\n","from keras.layers import Dense, Flatten, Conv2D, Dropout\n","from keras.preprocessing.image import ImageDataGenerator"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6XWrer9It5R"},"source":["### 2.  Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"SfSX9-Qcsy6O"},"source":["# Specify root data directory\n","train_data_dir = \"data\"\n","\n","img_height = 224\n","img_width = 224\n","batch_size = 10\n","\n","image_datagen = ImageDataGenerator(validation_split=0.2) # set validation split"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFGjczpBNpJ4"},"source":["#### Loading the data"]},{"cell_type":"code","metadata":{"id":"BFw_D0Yao-GA"},"source":["train_generator = image_datagen.flow_from_directory(train_data_dir,\n","                                                    target_size = (img_height, img_width),\n","                                                    batch_size = batch_size,\n","                                                    subset='training') # set as training data\n","\n","print(\"Class names and respective labels: \",train_generator.class_indices)\n","print(\"Image Shape: \", train_generator.image_shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nyV_SI4atmmH"},"source":["validation_generator = image_datagen.flow_from_directory(train_data_dir, # same directory as training data\n","                                                         target_size = (img_height, img_width),\n","                                                         batch_size = batch_size,\n","                                                         subset='validation') # set as validation data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UOuWTFqiO3DC"},"source":["#### labels Translator"]},{"cell_type":"code","metadata":{"id":"5m2wgKQPO4uK"},"source":["labels = {i:v for v, i in validation_generator.class_indices.items()}\n","print(\"Dictionary of Labels: \",labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2RnRtZ4tbSoH"},"source":["### 3. Loading VGG16 model with pretrained weights"]},{"cell_type":"code","metadata":{"id":"pAiKE9dS5t08"},"source":["vgg_base = VGG16(weights='imagenet',    # use weights for ImageNet\n","                 include_top=False,     # drop the Dense layers!\n","                 input_shape=(img_height, img_width, 3))\n","\n","vgg_base.trainable = False # Freezing the weights"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_tEYes3Ia9-6"},"source":["### 4. Fine-tuning VGG16\n","\n","* Copy the first trained layers (base model) and then add a new custom layers in the sequential layer to perform classification on a specific task"]},{"cell_type":"code","metadata":{"id":"s-7v2AB3pF96"},"source":["model = Sequential([\n","        # our vgg16_base model added as a layer\n","        vgg_base,\n","        \n","        # here is our custom prediction layer \n","        Flatten(),\n","        Dropout(0.50),\n","        Dense(1048, activation='relu'),\n","        Dropout(0.10),         \n","        Dense(units=3, activation='softmax')\n","    ])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cgi7zylgdpzC"},"source":["### 5. Train the deep learning model"]},{"cell_type":"code","metadata":{"id":"OS94yTMypapd"},"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","nb_epochs = 5\n","\n","# Quantity to be monitored is validation accuracy\n","monitor = EarlyStopping(monitor='val_accuracy')\n","\n","model.compile(optimizer= 'adam', loss= 'categorical_crossentropy', metrics= ['accuracy'])\n","\n","model.fit(train_generator, validation_data = validation_generator, callbacks=[monitor], epochs = nb_epochs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ssHGDRMEd1cT"},"source":["### 6. Evaluate the trained deep learning model\n","\n","<br>\n","\n","**List of VideoCapture Methods utilized**\n","\n","    \"cap.get()\" - Access some of the features of this video using cap.get(propId) method, where propId is a number from 0 to 18.\n","    Each number denotes a property of the video and full details can be seen here\n","    For example, I can check the frame width and height by cap.get(3) and cap.get(4) and the frame-rate using cap.get(5)    \n","\n","    \"cap.isOpened()\" - Check whether cap is initialized or not by the method cap.isOpened(), proceed If it is True\n","\n","    \"cap.read()\" - Capture a frame-by-frame\n","    returns a bool (True/False). If frame is read correctly, it will be True.\n","\n","    \"cap.release()\" - Release everything if job is finished\n","    In order to create another instance cap2 = cv2.VideoCapture(0) release the existing resource"]},{"cell_type":"code","metadata":{"id":"LoX3OVxLuYl9"},"source":["def classify(videoPath):\n","    cap = cv2.VideoCapture(videoPath)   # capturing the video from the given path\n","    frameRate = cap.get(5) #frame rate\n","    frameResults = []\n","\n","    while (cap.isOpened()):\n","        frameId = cap.get(1) #current frame number\n","\n","        ret, frame = cap.read()\n","        if (ret != True):\n","            break\n","\n","        if (frameId % math.floor(frameRate) == 0):\n","            # cv2.cvtColor() method is used to convert an image from one color space to another\n","            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","            frame = cv2.resize(frame, (224, 224))\n","            plt.imshow(frame)\n","            preds = model.predict(np.expand_dims(frame, axis=0))[0]\n","            frameResults.append(labels[np.argmax(preds)])\n","\n","    cap.release()\n","    return \"Activity: \", statistics.mode(frameResults)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NuE4n-YMx_6f"},"source":["# Pass the testing video path as input to the classify method\n","classify(\"sample_clips/test1.mp4\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHULtG4ttvYJ"},"source":["classify(\"sample_clips/test2.mp4\")"],"execution_count":null,"outputs":[]}]}