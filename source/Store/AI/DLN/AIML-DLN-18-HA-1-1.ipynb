{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-18-HA-1-1.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SZIubkln0AI2"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"4LNbxek40AI4"},"source":["# Hackathon: Voice commands based E-commerce ordering system\n","The goal of the hackathon is to train your model on different types of voice data (such as studio data and your own team data) and able to place order based on user preferences."]},{"cell_type":"markdown","metadata":{"id":"Fms7Yt7byCuQ"},"source":["## Grading = 40 Marks"]},{"cell_type":"markdown","metadata":{"id":"zUtVl7cBHlIh"},"source":["### **Objectives:**\n","\n","Stage 0 - Obtain Features from Audio samples\n","\n","Stage 1 (22 Marks) - Define and train a CNN model on Studio data and deploy the model in the server \n","\n","Stage 2 (18 Marks) - Collect your voice samples (team data) and refine the classifier trained on Studio_data. Deploy the model in the server."]},{"cell_type":"markdown","metadata":{"id":"BYm_60PiPSsq"},"source":["## Dataset Description"]},{"cell_type":"markdown","metadata":{"id":"qiAu1XJx3lCJ"},"source":["The data contains voice samples of classes - Zero, One, Two, Three, Four, Five. Each class is denoted by a numerical label from 0 to 5.\n","\n","The audio files collected in a Studio dataset contain very few noise samples and all the files are in wav format.\n","\n","The audio files recorded for the studio are saved with the following naming convention: \n","\n","● Class Representation + user_id + sample_ID (or noise + sample_ID)\n","\n","> For example: The voice sample by the user b2 recorded “Zero”, it is saved as 0_b2_35.wav. Here 35 is sample ID, 2 is the user id and ‘0’ is the label of that sample.\n","\n","\n"]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Hackathon_data/B17_studio_rev_data.zip\n","! unzip B17_studio_rev_data.zip"],"metadata":{"id":"wLNPaXAFSCbc"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DqNBNvC25WNV"},"source":["import os\n","import sys\n","import glob\n","import torch\n","import librosa\n","import warnings\n","import numpy as np\n","import torch.nn as nn\n","from time import sleep\n","from torch import optim\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lEg2PYXrOjnZ"},"source":["## **Stage 0:** Obtain Features from Audio samples\n","---\n","\n","### Generate features from an audio sample of '.wav' format\n","- Code is available to extract the features"]},{"cell_type":"code","metadata":{"id":"eTtb2zAj5k0-"},"source":["# Caution: Do not change the default parameters\n","def get_features(filepath, sr=8000, n_mfcc=30, n_mels=128, frames = 15):\n","    # The following function contains code to produce features of the audio sample.  \n","    y, sr = librosa.load(filepath, sr=sr)\n","    D = np.abs(librosa.stft(y))**2\n","    S = librosa.feature.melspectrogram(S=D)\n","    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n","    log_S = librosa.power_to_db(S,ref=np.max)\n","    features = librosa.feature.mfcc(S=log_S, n_mfcc=n_mfcc)\n","    if features.shape[1] < frames :\n","        features = np.hstack((features, np.zeros((n_mfcc, frames - features.shape[1]))))\n","    elif features.shape[1] > frames:\n","        features = features[:, :frames]\n","\n","    # Find 1st order delta_mfcc\n","    delta1_mfcc = librosa.feature.delta(features, order=1)\n","\n","    # Find 2nd order delta_mfcc\n","    delta2_mfcc = librosa.feature.delta(features, order=2)\n","\n","    # Stacking delta_mfcc features in sequence horizontally (column wise)\n","    features = np.hstack((delta1_mfcc.flatten(), delta2_mfcc.flatten()))\n","\n","    # Increase the dimension by inserting an axis along second dimension\n","    features = features.flatten()[:,np.newaxis]\n","    \n","    # Convert the numpy.ndarray to a Tensor object\n","    features = Variable(torch.from_numpy(features)).float()\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NhLFY4n6BwIj"},"source":["All the voice samples needed for training are present in the folder `\"studio_data\"`"]},{"cell_type":"code","metadata":{"id":"lMF1AqHZhl1h"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a2AAbFp5KORl"},"source":["##**Stage 1**:  Define and train a CNN model on Studio data and deploy the model in the server\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"SB-LowDuCMUL"},"source":["### a) Extract features of Studio data (4 Marks)\n","\n"," Load 'Studio data' and extract mfcc features\n","\n"," **Evaluation Criteria:**\n","\n"," * Complete the code in the load_data function\n"," * The function should take path of the folder containing audio samples as input\n"," * It should return features of all the audio samples present in the specified folder into single array (list of lists or 2-d numpy array) and their respective labels should be returned too"]},{"cell_type":"code","metadata":{"id":"qDzCa-532EUj"},"source":["def load_data(folder_path):\n","    #YOUR CODE HERE\n","    #return features, labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7673ezpxFEfM"},"source":["Load data from studio_data folder for extracting all features and labels"]},{"cell_type":"code","metadata":{"id":"u5CjrlPVPjNs"},"source":["studio_recorded_features, studio_recorded_labels = load_data('/content/studio_data')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"krshAu69Hy8-"},"source":["Use train_test_split for splitting the train and test data"]},{"cell_type":"code","metadata":{"id":"0LV83ruiHvfO"},"source":["from sklearn.model_selection import train_test_split\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"43M0H5Z23rnh"},"source":["Load the dataset with DataLoader\n","- Refer to [torch.utils.data.TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset)\n","- Refer to [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"]},{"cell_type":"code","metadata":{"id":"ls6gI08XH2ak"},"source":["# YOUR CODE HERE for the DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BGq6XpvhFynP"},"source":["### b) Define your CNN architecture (4 Marks)\n","\n","[Hint](https://pytorch.org/docs/stable/generated/torch.nn.Conv1d.html)"]},{"cell_type":"code","metadata":{"id":"VU5hdERsFw5o"},"source":["## Define your CNN Architecture\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        # Sample Convolution Layer 1 \n","        self.conv1 = nn.Conv1d(in_channels=900, out_channels=400, kernel_size=1)\n","        self.bn1 = nn.BatchNorm1d(400)\n","        self.relu1 = nn.ReLU()\n","\n","        # Sample Maxpool for the Convolutional Layer 1\n","        self.maxpool1 = nn.MaxPool1d(1)\n","\n","        # Sample Dropout Layer\n","        self.dropout = nn.Dropout(p=0.25)\n","\n","        # YOUR CODE HERE for defining more number of Convolutional layers with Maxpool as required (Hint: Use at least 2 more convolutional layers for better performance)\n","\n","        \n","        # YOUR CODE HERE for defining the Fully Connected Layer and also define LogSoftmax\n","\n","    def forward(self, x):\n","        # Convolution Layer 1, Maxpool and Dropout\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu1(out)\n","        out = self.maxpool1(out)\n","        out = self.dropout(out)\n","        # YOUR CODE HERE for the Convolutional Layers and Maxpool based on the defined Convolutional layers\n","\n","        # YOUR CODE HERE for flattening the output of the final pooling layer to a vector. Flattening is simply arranging the 3D volume of numbers into a 1D vector\n","        \n","        # YOUR CODE HERE for returning the output of LogSoftmax after applying Fully Connected Layer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OFWuGmq05ZK"},"source":["# To run the training on GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wkt8lKQtCIWD"},"source":["model = Net()\n","model = model.to(device)\n","print(model)\n","\n","#criterion = # YOUR CODE HERE : Explore and declare loss function\n","\n","#optimizer = # YOUR CODE HERE : Explore on the optimizer and define with the learning rate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T5nF5pwKQ2t1"},"source":["### c) Train and classify on the studio_data (3 Marks)\n","\n","The goal here is to train the Model on voice samples collected in studio data and validate it continuously to calculate the loss and accuracy for the train dataset across each epoch.\n","\n","Iterate over images in the train_loader and perform the following steps. \n","\n","1. First, zero out the gradients using zero_grad()\n","\n","2. Pass the data to the model. Convert the data to GPU before passing data  to the model\n","\n","3. Calculate the loss using a Loss function\n","\n","4. Perform Backward pass using backward() to update the weights\n","\n","5. Optimize and predict by using the torch.max()\n","\n","6. Calculate the accuracy of the train dataset\n"]},{"cell_type":"code","metadata":{"id":"2Ot89MxKavVy"},"source":["# YOUR CODE HERE. This will take time\n","\n","# Record loss and accuracy of the train dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5BJIQzgHa0k"},"source":["### d) Testing Evaluation for CNN model (3 Marks)\n","\n","Evaluate model with the given test data\n","\n","1. Transform and load the test images.\n","\n","2. Pass the test data through the model (network) to get the outputs\n","\n","3. Get the predictions from a maximum value using torch.max\n","\n","4. Compare with the actual labels and get the count of the correct labels\n","\n","5. Calculate the accuracy based on the count of correct labels\n","\n","### **Expected testing accuracy is above 80%**"]},{"cell_type":"code","metadata":{"id":"RnKZ-gP-30xR"},"source":["# YOUR CODE HERE to test the model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qqIP3Y17byDq"},"source":["### e) Save and download your model (2 Marks)\n","\n","**Save your model trained on studio data**\n","\n","* Save the state dictionary of the classifier (use pytorch only), It will be useful in\n","integrating model to the web application\n","\n"," [Hint](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"]},{"cell_type":"code","metadata":{"id":"A7KAIpLsI4Uj"},"source":["### YOUR CODE HERE for saving the CNN model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsCHKXubHAJB"},"source":["Download your trained model using the code below\n","* Give the path of model file to download through the browser"]},{"cell_type":"code","metadata":{"id":"BDmWXfPaHJZG"},"source":["from google.colab import files\n","files.download('<model_file_path>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hl3Ins3vSTQx"},"source":["### f) Deploy and evaluate your model trained on Studio Data in the server (6 Marks).\n","\n","(This can be done on the day of the Hackathon once the login username and password provided by the mentors in the lab) \n","\n","Deploy your model on the server, check the hackathon document (2-Server Access and File transfer For Voice based e-commerce ordering.pdf) for details. \n","\n","To order product in user interface, go through the document (3-Hackathon_II Application Interface Documentation.pdf) for details.\n","\n","\n","**Evaluation Criteria: Four consecutive utterances should be predicted correctly by the model**\n","\n","- There are two stages in the e-commerce ordering application    \n","    - Ordering Product\n","    - Selecting the e-commerce platform\n","- If both the stages are cleared as per the evaluation criteria you will get\n","complete marks Otherwise, you will see a reduction in the marks"]},{"cell_type":"markdown","metadata":{"id":"GIXBC0aYKhKX"},"source":["## **Stage 2:** Collect your voice samples and refine the classifier trained on studio_data and Team_data\n","---"]},{"cell_type":"markdown","metadata":{"id":"pmSoJN11_kMR"},"source":["### a) Collect your Team Voice Samples and extract features (6 Marks)\n","\n","(This can be done on the day of the Hackathon once the login username and password is given by mentors in the lab)\n","\n","* In order to collect the team data, ensure the server is active (2-Server Access and File transfer For Voice based e-commerce ordering.pdf)\n","\n","* Refer document \"3-Hackathon_II Application Interface Documentation.pdf\" for collecting your team voice samples. These will get stored in your server\n","\n","**Evaluation Criteria:**\n","* Load 'Team_data' and extract features\n","* Combine features of team data with the extracted features of studio data\n","* Split the combined features into train and test data\n","* Load the dataset with DataLoader"]},{"cell_type":"code","metadata":{"id":"nv3I24flWlLq"},"source":["!mkdir team_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gB_bSllKWJ5U"},"source":["# Replace <YOUR_GROUP_ID> with your Username given in the lab\n","!wget -r -A .wav https://aiml-sandbox1.talentsprint.com/audio_recorder/<YOUR_GROUP_ID>/team_data/ -nH --cut-dirs=100  -P ./team_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2G17PFkgI02J"},"source":["# YOUR CODE HERE to Load data from teamdata folder for extracting all features and labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_sh6DCbJKhg"},"source":["# Combine the features of all voice samples (studio_data and teamdata)\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UcSfoU9hyst4"},"source":["# YOUR CODE HERE to split the combined features into train and test data (Hint: Use train_test_split)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQ8gSTYg1dSp"},"source":["# YOUR CODE HERE to load the dataset with DataLoader"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pSE9E9mDF7Az"},"source":["### b) Classify and download the model (6 Marks)\n","\n","The goal here is to train and test your model on all voice samples collected in studio and team data\n","\n","**Evaluation Criteria:**\n","* Refine your classifier (if needed)\n","* Train your model on the extracted train data\n","* Test your model on the extracted test data\n","* Save and download the trained model\n","\n","### **Expected testing accuracy is above 80%**"]},{"cell_type":"code","metadata":{"id":"D1EtSEwDG-q4"},"source":["# YOUR CODE HERE for refining your classifier (if needed)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nz0UbGJrz59Z"},"source":["# YOUR CODE HERE to train your model\n","\n","# Record loss and accuracy of the train dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8CkqFppJ0Fha"},"source":["# YOUR CODE HERE to test your model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snSilDehQcKv"},"source":["**Save your trained model**\n","\n","* Save the state dictionary of the classifier (use pytorch only), It will be useful in\n","integrating model to the web application\n","\n"," [Hint](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"]},{"cell_type":"code","metadata":{"id":"HGiaYmCnQcKz"},"source":["### YOUR CODE HERE for saving the CNN model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7TSFyHRFQcK1"},"source":["Download your trained model using the code below\n","* Give the path of model file to download through the browser"]},{"cell_type":"code","metadata":{"id":"dF2kGMjAQcK2"},"source":["from google.colab import files\n","files.download('<model_file_path>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AfhmbXkFx6is"},"source":["### c) Deploy and evaluate your model trained on Studio Data + Team Data in the server (6 Marks).\n","\n","(This can be done on the day of the Hackathon once the login username and password provided by the mentors in the lab) \n","\n","Deploy your model on the server, check the hackathon document (2-Server Access and File transfer For Voice based e-commerce ordering.pdf) for details. \n","\n","To order product in user interface, go through the document (3-Hackathon_II Application Interface Documentation.pdf) for details.\n","\n","\n","**Evaluation Criteria: Four consecutive utterances should be predicted correctly by the model**\n","\n","- There are two stages in the e-commerce ordering application    \n","    - Ordering Product\n","    - Selecting the e-commerce platform\n","- If both the stages are cleared as per the evaluation criteria you will get\n","complete marks Otherwise, you will see a reduction in the marks"]},{"cell_type":"code","metadata":{"id":"4kIo3R5hQnrh"},"source":[""],"execution_count":null,"outputs":[]}]}