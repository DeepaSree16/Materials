{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-14-AS-1-1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hH3UvBtnW755"},"source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"KXubZhEt6g3u"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"0shlrdB36iZs"},"source":["At the end of the experiment you will be able to :\n","\n","-  understand how to implement neural networks on MFCC features\n"]},{"cell_type":"code","metadata":{"id":"ftfEjck9eqp4","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":501},"outputId":"d3b2f45b-92f6-4624-a9c1-9d0387b399dc"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/MFCC_Pytorch_Walkthrough.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"850\" height=\"480\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/MFCC_Pytorch_Walkthrough.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"OgP2LVgh625u"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"AVSiGXI67ARp"},"source":["### Description\n","\n","In this experiment we will use TensorFlowâ€™s Speech Commands Datasets which includes 1lakh+ samples in which each sample is a one-second-long utterance of 30 short commands. This dataset has been curated using thousands of people and is opensource under a Creative Commons BY 4.0 license.\n","\n","Example commands: 'Yes', 'No', 'Up', 'Down', 'Left', etc.\n"]},{"cell_type":"markdown","metadata":{"id":"Zj2Xw8qA7Syd"},"source":["## Domain Information\n","\n","When we listen to an audio sample it changes constantly. This means that speech is non-stationary signal. Therefore, normal signal processing techniques cannot be applied to get features from audio. However, if the speech signal is observed using a very small duration window, the speech content in that small duration appears to be  stationary. That brought in the concept of short-time processing of speech. \n","\n","MFCC is a technique for short-time processing of speech. \n"]},{"cell_type":"code","source":["! wget  https://cdn.talentsprint.com/aiml/Experiment_related_data/week3/Exp1/AIML_DS_AUDIO_STD.zip\n","! unzip AIML_DS_AUDIO_STD.zip"],"metadata":{"id":"FLaVK7jFLhcH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zROXmFsgX10F"},"source":["### Importing required packages\n"]},{"cell_type":"code","metadata":{"id":"CDGLPWSfW76D"},"source":["import scipy.io as sio\n","\n","# Importing torch packages\n","import torch\n","import torch.nn as nn      \n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","# Importing python packages\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJXbJhvkPfVn"},"source":["### Load the Dataset\n","\n","The dataset is of ~10GB in size and operating directly on it will take a lot of time, therefore we have included that as a Homework Exercise for those who are interested to go into that detail.\n","Our team has instead precomputed the features which can be loaded directly and computed on.\n","\n","Dataset is available to download using the below link: <br>[http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz ](http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz)\n"]},{"cell_type":"markdown","metadata":{"id":"2q2QUvXFFwNR"},"source":["### Loading MFCC features\n","\n","In this experiment assume that the term Validation (short name: val) is the same as 'Test' dataset. Here we have two-way Train/Val(same as test) split\n","\n","**Note:** Refer to [sio.loadmat](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html)"]},{"cell_type":"code","metadata":{"id":"UqKF01iTW77W","colab":{"base_uri":"https://localhost:8080/"},"outputId":"7728b86c-fa5c-4ea4-cc86-f8c1a847b332"},"source":["# Load MFCC Features\n","saved_vars = sio.loadmat('AIML_DS_AUDIO_STD/mfcc_feats/tf_speech_mfcc_31st_jan18.mat')\n","# print(saved_vars.keys())\n","\n","mfcc_features_train = saved_vars['mfcc_features_train']\n","mfcc_labels_train = saved_vars['mfcc_labels_train']\n","\n","mfcc_features_val = saved_vars['mfcc_features_val']\n","mfcc_labels_val = saved_vars['mfcc_labels_val']\n","print(mfcc_features_train.shape, mfcc_features_val.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(57923, 416) (6798, 416)\n"]}]},{"cell_type":"code","metadata":{"id":"maabN9W7iGKg"},"source":["# Check for the no of unique labels in the trainset\n","print(np.unique(mfcc_labels_train))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vbm-8HxIqvxR"},"source":["### Initializing CUDA\n","\n","CUDA is used as an interface between our code and the GPU.\n","\n","Normally, we run the code in the CPU. To run it in the GPU, we need CUDA. Check if CUDA is available:"]},{"cell_type":"code","metadata":{"id":"tYjglAImvY25","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c257e1dd-997a-4a5f-90a3-e767f8f773b0"},"source":["# To test whether GPU instance is present in the system of not.\n","use_cuda = torch.cuda.is_available()\n","print('Using PyTorch version:', torch.__version__, 'CUDA:', use_cuda)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using PyTorch version: 1.9.0+cu102 CUDA: True\n"]}]},{"cell_type":"code","metadata":{"id":"HXVWujrnvutQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5cf4a5ea-82b3-4758-88f4-101c84df9961"},"source":["device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"qsi-0nRnlnp8"},"source":["### Defining the Neural Network"]},{"cell_type":"code","metadata":{"id":"GKAD3OrQwC0n"},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","    \n","        self.fc1 = nn.Linear(416, 208)  # First fully connected layer\n","        self.fc2 = nn.Linear(208, 104)  # Second fully connected layer\n","        self.fc3 = nn.Linear(104, 30)   # Third fully connected layer which outputs the no of labels\n","\n","    def forward(self, x):  \n","\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        return x "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qxfH-PwAlzpb"},"source":["### Creating Instance for the Model"]},{"cell_type":"code","metadata":{"id":"ySwVubgtl58N"},"source":["model = Net()   \n","model = model.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"paTSU7Azl7oc"},"source":["### Defining Loss Function and Optimizer"]},{"cell_type":"code","metadata":{"id":"NMeHbSblx9mN"},"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AAvoYOySmBwo"},"source":["### Training the Model"]},{"cell_type":"code","metadata":{"id":"QrL6dDtmhoey"},"source":["epochs = 5\n","accuracy = []\n","train_loss = 0\n","\n","for epoch in range(epochs):\n","    correct = 0\n","    for i, feature in enumerate(mfcc_features_train):\n","        # Convert the features to pytorch tensor\n","        feature = torch.Tensor(feature).to(device)\n","        \n","        # Zero out the gradients from the preivous step \n","        optimizer.zero_grad()\n","        \n","        # Do forward pass\n","        outputs = model(feature)\n","        \n","        labels = torch.Tensor(mfcc_labels_train[i]).to(device)\n","        outputs = outputs.unsqueeze(0)\n","        \n","        # Calculating the loss\n","        loss = criterion(outputs, labels.long())\n","        train_loss += loss.item()\n","        \n","        # Do backward pass\n","        loss.backward()\n","        \n","        # optimizer.step() updates the weights accordingly\n","        optimizer.step()\n","\n","        # Accuracy calculation\n","        _, predicted = torch.max(outputs, 1)\n","        correct += (predicted == labels).sum()\n","\n","    accuracy.append(correct/len(mfcc_features_train))   \n","    print(accuracy[-1].item())"],"execution_count":null,"outputs":[]}]}