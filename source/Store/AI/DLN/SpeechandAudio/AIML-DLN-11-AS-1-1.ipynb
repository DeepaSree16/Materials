{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"AIML-DLN-11-AS-2-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hH3UvBtnW755"},"source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"KXubZhEt6g3u"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"0shlrdB36iZs"},"source":["At the end of the experiment you will be able to :\n","\n","-  extract and visualize features for one audio sample.\n","-  use extracted features to classify them into 30 different classes\n"]},{"cell_type":"markdown","metadata":{"id":"8euDABu_Bqja"},"source":["**NOTE:** The number of unique classes chosen here is 30. Further, you are encouraged to use different types of classifiers (within SkLearn if you like). Also, try to use sklearn's joblib to save your model (and observe the size of your various ML models). Saving a model is an important step, especially if you wish to export it to another machine/device where you'd like to run your ML algorithm."]},{"cell_type":"code","metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":501},"id":"ftfEjck9eqp4","executionInfo":{"elapsed":23,"status":"ok","timestamp":1625824991948,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"},"user_tz":-330},"outputId":"97782baf-afb3-4d47-ba56-8ed7dc94ab43"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Feb10/mfcc_new.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"854\" height=\"480\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/Feb10/mfcc_new.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"OgP2LVgh625u"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"AVSiGXI67ARp"},"source":["### Description\n","\n","In this experiment we will use TensorFlowâ€™s Speech Commands Datasets which includes 1lakh+ samples in which each sample is a one-second-long utterance of 30 short commands. This dataset has been curated using thousands of people and is opensource under a Creative Commons BY 4.0 license.\n","\n","Example commands:  'bed', 'bird', 'cat', 'dog', 'down', 'eight', 'five', 'four', 'go', 'happy', 'house', 'left', 'marvin', 'nine', 'no', 'off', 'on', 'one', 'right', 'seven', 'sheila', 'six', 'stop', 'three', 'tree', 'two', 'up', 'wow', 'yes', 'zero'\n"]},{"cell_type":"markdown","metadata":{"id":"Zj2Xw8qA7Syd"},"source":["## Domain Information\n","\n","When we listen to an audio sample it changes constantly. This means that speech is non-stationary signal. Therefore, normal signal processing techniques cannot be applied to get features from audio. However, if the speech signal is observed using a very small duration window, the speech content in that small duration appears to be  stationary. That brought in the concept of short-time processing of speech. \n","\n","MFCC is a technique for short-time processing of speech. \n","\n","**Note: While it is a bonus to understand 'how' a speech signal is converted to a 'representation' by MFCC (or other similar speech processing techniques), it is not mandatory to understand the same. It is sufficient to realize that you're generating a unique numeric representation (a set of numbers) of a speech signal and that this unique representation helps you run your machine learning algorithms over the samples.**"]},{"cell_type":"markdown","metadata":{"id":"94LwGp0o7WoK"},"source":["## AIML Technique\n","\n","In this short-time processing technique MFCC, a small duration window (say 25 milli sec) is considered for processing of the audio samples at a time. This small duration is called a frame. Now, for each of the frames, MFCC features are computed which gives a compact representation of the audio samples. A spectrogram is computed for audio samples which gives a heat map of frequencies vs time from the series of spectral vectors generated from audio samples.\n","\n","This representation is obtained by keeping in mind that humans are much better at grasping small changes in the audio at low frequencies than audio at high frequencies. So mel-scale converts high frequencies to human graspable frequencies. "]},{"cell_type":"code","source":["! wget -qq https://cdn.talentsprint.com/aiml/Experiment_related_data/week3/Exp1/AIML_DS_5E1B34A6_NOHASH_0_STD.wav\n","! mv AIML_DS_5E1B34A6_NOHASH_0_STD.wav?dl=1 AIML_DS_5E1B34A6_NOHASH_0_STD.wav\n","! wget  https://cdn.talentsprint.com/aiml/Experiment_related_data/week3/Exp1/AIML_DS_AUDIO_STD.zip\n","! unzip AIML_DS_AUDIO_STD.zip"],"metadata":{"id":"mu4kULHpyIGZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nmbSrNi6zQ81"},"source":["The above setup cell takes some time to complete the execution, the deep features zip file is around 1.1GB so it takes time to run."]},{"cell_type":"markdown","metadata":{"id":"zROXmFsgX10F"},"source":["### Importing required packages\n"]},{"cell_type":"code","metadata":{"id":"CDGLPWSfW76D"},"source":["import os\n","\n","import numpy as np\n","from scipy import signal\n","from scipy.io import wavfile\n","import librosa\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import librosa.display"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uhF2YnieW76X"},"source":["## 1. MFCC features"]},{"cell_type":"markdown","metadata":{"id":"5_tdjdHYW76M"},"source":["Let us read a single sample audio file (.wav) from the dataset:\n","\n","Note: Refer [wavfile.read](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html) from scipy"]},{"cell_type":"code","metadata":{"id":"reVAHgWqW76N"},"source":["filename = 'AIML_DS_5E1B34A6_NOHASH_0_STD.wav'\n","sample_rate, samples = wavfile.read(str(filename))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GFd2w1iVW76S"},"source":["samples = samples.astype('float16')\n","print(samples)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxUeoP4KW76Z"},"source":["###  1.1. Amplitude\n","\n","Speech is a temporal signal, where the amplitude of the signal varies with time.\n","\n","The amplitude v/s time graph of the audio file we read is:"]},{"cell_type":"code","metadata":{"id":"zqfKmB2nW76a"},"source":["fig = plt.figure(figsize=(14, 8))\n","plt.plot(np.linspace(0, sample_rate/len(samples), sample_rate), samples)\n","plt.gca().set_title('Raw wave of ' + filename)\n","plt.gca().set_xlabel('Time')\n","plt.gca().set_ylabel('Amplitude')\n","plt.grid(\"on\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ql9nmBW1W76g"},"source":["## 1.2 Log Spectrogram\n","\n","The same speech signal could be interpreted as made up of several frequencies of waves. A visualization of the power, i.e. energy per unit time in each frequency v/s time is called the Spectrogram.\n","\n","Usually, the log of this energy is considered a better parameter. This is because the power in sound is synonymous with volume (loudness) of the sound, and the human ears are more sensitive to smaller volumes than larger volumes. So it is more convenient to observe the log of the volume rather than the volume itself. The log of sound power is measured in deciBels (dB). (You might be familiar with dB as a unit of sound volume). Hence, we shall consider the Log Spectrogram instead of just the spectrogram.\n","\n","Let us compute the Log Spectrogram of the audio file we read:"]},{"cell_type":"markdown","metadata":{"id":"GuTW0MeYmvST"},"source":["Note: Refer [signal.spectogram](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.spectrogram.html) from scipy"]},{"cell_type":"code","metadata":{"id":"tfu5HnG-W76i"},"source":["def log_specgram(audio, sample_rate, window_size=20, step_size=10):\n","    \n","    '''\n","    Divide the signal into equal-length segments. \n","    The segments must be short enough that the frequency of the signal does not change appreciably within a segment.\n","    The segments may or may not overlap.\n","    Window each segment and compute its spectrum\n","    '''\n","    # Number of samples per window/segment\n","    # window_size represents a number of samples, and a duration\n","    nperseg = int(round(window_size * sample_rate / 1e3))\n","    \n","    # Number of overlapping samples\n","    # Number of points to overlap between segments\n","    noverlap = int(round(step_size * sample_rate / 1e3))\n","    \n","    # Compute the spectrogram\n","    freqs, times, spec = signal.spectrogram(audio,\n","                                            fs=sample_rate,\n","                                            window='hann',\n","                                            nperseg=nperseg,\n","                                            noverlap=noverlap)\n","    \n","    # Return log of the spectrogram values, with time axis in columns and frequencies in rows\n","    return freqs, times, np.log(spec.T)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"90Gcx_sIW76n"},"source":["# Call the log specgram function\n","freqs, times, spectrogram = log_specgram(samples, sample_rate)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_3lg53XcW76r"},"source":["Let us plot the log spectrogram:"]},{"cell_type":"code","metadata":{"id":"VDiCEkzRW76s"},"source":["fig = plt.figure(figsize=(14, 4))\n","plt.imshow(spectrogram.T, aspect='auto', origin='lower', cmap = plt.cm.RdYlBu,\n","           extent=[times.min(), times.max(), freqs.min(), freqs.max()])\n","plt.gca().set_yticks(freqs[::16])\n","plt.gca().set_xticks(times[9::10])\n","plt.gca().set_title('Spectrogram of ' + filename)\n","plt.gca().set_ylabel('Frequency in Hz')\n","plt.gca().set_xlabel('Time')\n","plt.colorbar()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eGF52L0GW76y"},"source":["As can be seen from the frequency v/s time graph, the command is being uttered between 0.4 seconds and 0.6 seconds.\n","\n","As can be seen from the spectrogram, the command is composed more of lower frequencies than higher frequencies. The rest of the time when there is no speech, each frequency has an equal contribution to the sound. This is called \"White Noise\".\n","\n","Notice that the range of frequencies we are observing in the spectrogram is the linear scale between 0 Hz and 8000 Hz."]},{"cell_type":"markdown","metadata":{"id":"YtEBGU5fW76z"},"source":["### 1.3. Mel Spectrogram\n","\n","Human ears tend to listen to sounds on the log scale. That means, at lower frequencies we can detect small changes, but at higher frequencies our ears become less sensitive to small changes. For example, the difference between 10 Hz and 20 Hz would sound almost the same to us as that between 1000 Hz and 2000 Hz. To observe this logarithmic change, the frequency scale is modified into the [\"mel frequency\" scale](https://en.wikipedia.org/wiki/Mel_scale).\n","\n","Let us compute the Mel Spectrogram using a convenient function in the `librosa` library in Python:"]},{"cell_type":"markdown","metadata":{"id":"RoZ7qcqvnxXr"},"source":["**Note:** Refer to [librosa.feature.melspectogram](https://librosa.org/doc/latest/generated/librosa.feature.melspectrogram.html) here"]},{"cell_type":"code","metadata":{"id":"or3bV7FvW761"},"source":["# From this tutorial\n","# https://github.com/librosa/librosa/blob/master/examples/LibROSA%20demo.ipynb\n","\n","# hop_length is the number of samples between successive frames\n","S = librosa.feature.melspectrogram(samples, sr=sample_rate, hop_length=int(0.020*sample_rate), n_mels=128)\n","print(S.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zTEFfDs8W764"},"source":["As mentioned before, the log of the spectrogram is a better parameter to observe rather than the spectrogram itself. Let us compute this using another convenient function in the `librosa` library:"]},{"cell_type":"code","metadata":{"id":"doa6JjcHW765"},"source":["# Convert to log scale (dB). We'll use the peak power (max) as reference.\n","log_S = librosa.power_to_db(S, ref=np.max)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eEKISI0CW768"},"source":["Let's plot the log Mel spectrogam with the y-axis having frequenies in the mel scale instead of the linear scale: "]},{"cell_type":"code","metadata":{"id":"XipRSnuAW769"},"source":["plt.figure(figsize=(12, 4))\n","librosa.display.specshow(log_S, sr=sample_rate, y_axis='mel')\n","plt.xlabel('Time')\n","plt.title('Log-power Mel spectrogram ')\n","plt.colorbar(format='%+02.0f dB')\n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mpd3m6WsW77A"},"source":["Observe that the frequencies in the y-axis are not linear in scale. "]},{"cell_type":"markdown","metadata":{"id":"fd5mLClPW77B"},"source":["### 1.4 Mel Frequency Cepstral Coefficients (MFCCs)\n","\n","Next, \"Cepstral Coefficients\" are important numbers that describe speech information in audio. By computing these Cepstral Coefficients in the mel scale, we shall obtain Mel Frequency Cepstral Coefficients.\n","\n","For technical details, the procedure to compute MFCCs is:\n","\n","- Take the Discrete Fourier Transform on every sliding window over the audio with some overlap.\n","- Apply `n_mels` triangular Mel-scale filters onto the Fourier power spectrum, and apply logarithm to the outputs.\n","- Apply the Discrete Cosine Transform, and reduce dimensionality to `n_mfcc` dimensions.\n","\n","Let's use a convenient library function called `librosa.feature.mfcc` to compute MFCCs from the spectrogram:"]},{"cell_type":"markdown","metadata":{"id":"TckIUbakosQ_"},"source":["Note: Refere to [librosa.feature.mfcc](https://librosa.org/doc/latest/generated/librosa.feature.mfcc.html) here"]},{"cell_type":"code","metadata":{"id":"hLtpljeUW77C"},"source":["mfcc = librosa.feature.mfcc(S=log_S, n_mfcc=13)\n","print(mfcc.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gU0VRCohW77K"},"source":["### 1.5 Delta MFCCs\n","\n","MFCCs as such are quite powerful features, but even better features are their first-order and second-order derivatives.\n","The mfcc features are then passed to delta which perform 1st and 2nd order differentiation.\n","\n","Let's use a convenient library function called `librosa.feature.mfcc` to compute the second-order delta MFCCs:"]},{"cell_type":"code","metadata":{"id":"5oaeBModW77L"},"source":["# Find 1st order delta_mfcc\n","delta1_mfcc = librosa.feature.delta(mfcc, order=1)\n","\n","# Find 2nd order delta_mfcc\n","delta2_mfcc = librosa.feature.delta(mfcc, order=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMrkUfaTiZ1n"},"source":["delta1_mfcc.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s840U7KCW77P"},"source":["Let's plot the 1st and 2nd order delta MFCCs:"]},{"cell_type":"code","metadata":{"id":"z6mSz_TGW77R"},"source":["plt.figure(figsize=(12, 6))\n","\n","plt.subplot(211)\n","librosa.display.specshow(delta1_mfcc)\n","plt.ylabel('1st order Delta MFCC coeffs')\n","plt.xlabel('Time')\n","plt.title('1st order Delta MFCC')\n","plt.colorbar()\n","plt.tight_layout()\n","\n","plt.subplot(212)\n","librosa.display.specshow(delta2_mfcc)\n","plt.ylabel('2nd order Delta MFCC coeffs')\n","plt.xlabel('Time')\n","plt.title('2nd order Delta MFCC')\n","plt.colorbar()\n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJXbJhvkPfVn"},"source":["## 2. Load the Dataset\n","\n","The dataset is of ~10GB in size and operating directly on it will take a lot of time.\n","Our team has instead precomputed the features which can be loaded directly and computed on.\n","\n","Dataset is available to download using the below link: <br>[http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz ](http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz)\n"]},{"cell_type":"markdown","metadata":{"id":"2q2QUvXFFwNR"},"source":["## 3. Load MFCC features\n","\n","NOTE: For the rest of this experiment assume that the term Validation (short name: val) is the same as 'Test' dataset.\n","Later on in the course you will appreciate the significance of a three way split, i.e. train, val and test. But in this \n"," experiment we have two-way Train/Val(same as test) split\n","\n","**Note:** Refer to [sio.loadmat](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.loadmat.html)"]},{"cell_type":"code","metadata":{"id":"UqKF01iTW77W"},"source":["# Load MFCC Features\n","import scipy.io as sio\n","saved_vars = sio.loadmat('AIML_DS_AUDIO_STD/mfcc_feats/tf_speech_mfcc_31st_jan18.mat')\n","# print(saved_vars.keys())\n","\n","mfcc_features_train = saved_vars['mfcc_features_train']\n","mfcc_labels_train = saved_vars['mfcc_labels_train']\n","\n","mfcc_features_val = saved_vars['mfcc_features_val']\n","mfcc_labels_val = saved_vars['mfcc_labels_val']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z60J-rJJW77d"},"source":["## 4. Load Deep Features\n","\n","Deep features were trained on the same MFCC Features above using a Deeper Neural network, thus helping us to understand in extracting better features for representing the data. This helps in improving the accuracy"]},{"cell_type":"code","metadata":{"id":"qIW_FX4PW77f"},"source":["deep_features_train = np.squeeze(np.load('AIML_DS_AUDIO_STD/deep_feats/train_set.npz'))\n","deep_labels_train = np.load('AIML_DS_AUDIO_STD/deep_feats/train_labs.npz')\n","deep_features_val = np.squeeze(np.load('AIML_DS_AUDIO_STD/deep_feats/validation_set.npz'))\n","deep_labels_val = np.load('AIML_DS_AUDIO_STD/deep_feats/validation_labs.npz')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ri89OVGWXE6K"},"source":["deep_labels_train = deep_labels_train.reshape(-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-MSxXVHXHJI"},"source":["deep_labels_val = deep_labels_val.reshape(-1,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tp3g6eIfW77l"},"source":["## 5. Validation (in this case, synonymous with Test)\n","\n","Use the ML model for speech classification:\n","\n","- kNN classifier with MFCC features\n","\n","- kNN classifier with Deep features\n","\n","\n","To do that, let's find the validation accuracies using MFCC features and deep features.\n","\n","We shall use our familiar convenient function to choose an algorithm, train on training features and labels as inputs, and obtain accuracy on given features and labels."]},{"cell_type":"markdown","metadata":{"id":"8UhUw2HiW77o"},"source":["### 5.1. kNN Classifier"]},{"cell_type":"code","metadata":{"id":"at_MVmtwwqvA"},"source":["from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T78QHHriW77s"},"source":["### 5.1.1. kNN with MFCC features"]},{"cell_type":"markdown","metadata":{"id":"NZx4AKWwW77t"},"source":["**Ungraded Exercise 1: Find the validation (in this case, synonymous with Test) accuracy on MFCC features using k=3**\n","\n","**Note : For quick execution, using only 5000 samples of mfcc_features_train for training  and 1000 samples of mfcc_features_val for validation**"]},{"cell_type":"code","metadata":{"id":"zCvph_0BxCY0"},"source":["X_train = mfcc_features_train[:5000, :]\n","y_train = mfcc_labels_train[:5000]\n","X_val = mfcc_features_val[:1000, :]\n","y_val = mfcc_labels_val[:1000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dFWWbNeWIGsb"},"source":["X_train.shape, X_val.shape, y_train.shape, y_val.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVCXxLKNIPW3"},"source":["# YOUR CODE HERE: To create an object for KNN classifier with 'n_neighbors=3'\n","\n","# YOUR CODE HERE: To fit the model with the train data\n","\n","# YOUR CODE HERE: To calculate the accuracy on the validation data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZWS6NxqqW778"},"source":["### 5.1.2. kNN with Deep features"]},{"cell_type":"markdown","metadata":{"id":"kA4a6kHfW779"},"source":["\n","**Ungraded Exercise 2: Find the validation (in this case, synonymous with Test) accuracy on Deep features using k=3**\n","\n","\n","**Note : For quick execution, using only 5000 samples of deep_features_train for training  and 1000 samples of deep_features_val for validation**"]},{"cell_type":"code","metadata":{"id":"VqAwC5WFPEzP"},"source":["X_train_Deep = deep_features_train[:5000, :]\n","y_train_Deep = deep_labels_train[:5000]\n","X_val_Deep = deep_features_val[:1000, :]\n","y_val_Deep = deep_labels_val[:1000]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bxfm4-u0CFPM"},"source":["# YOUR CODE HERE: To create an object for KNN classifier with 'n_neighbors=3'\n","\n","# YOUR CODE HERE: To fit the model with the train data\n","\n","# YOUR CODE HERE: To calculate the accuracy on the validation data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_xEVSJgrN9Zz"},"source":["**Ungraded Exercise 3:** Try various other ML models that you know of to compare the accuracies "]},{"cell_type":"code","metadata":{"id":"eG253R_NWNMi"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a65EmdAqNmtP"},"source":["**Ungraded Exercise 4:** Use the following [link](https://scikit-learn.org/stable/modules/model_persistence.html) to see how to use joblib, and try saving and loading your different ML models using the example provided there. Also try observing the size of the ML models that you saved. Here is another good article on saving and loading models using sklearn [link](https://machinelearningmastery.com/save-load-machine-learning-models-python-scikit-learn/)"]},{"cell_type":"code","metadata":{"id":"JeK3C87bb6-p"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]}]}