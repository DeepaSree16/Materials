{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-09-AS-1-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ys_OuKdn_ij5"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"B6YRo1_NFkb7"},"source":["## Learning Objectives\n","\n","At the end of the experiment, you will be able to :\n","\n","* obtain the mfcc features from audio samples \n","* train the classifier with these mfcc features\n","* classify the audio samples into 'yes' and 'no'"]},{"cell_type":"code","metadata":{"id":"T6TIgra3pPfA","cellView":"form"},"source":["#@title Experiment Walkthrough Video\n","\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"854\" height=\"480\" controls>\n","<source src=\"https://cdn.exec.talentsprint.com/content/yes_no_classifier.mp4\" type=\"video/mp4\">\n","</video>\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tbAmXhdzFoX5"},"source":["## Dataset "]},{"cell_type":"markdown","metadata":{"id":"chgrAe1GFrYb"},"source":["### Description\n","The dataset used in the experiment contains 2661 audio samples with the extension .wav. \n","\n","Each audio sample has utterances 'yes' or 'no'\n","\n","The naming convention of the sample is of the format 'yes_no/' followed by '10' (representing 'yes') or '11' (representing 'no')\n","\n","Ex:   \n","\n","        'yes_no/10_g38_46.wav'    ---> yes audio sample\n","        'yes_no/11_G6_11.wav'     ---> no audio sample\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ISxllNzfFtwa"},"source":["## Domain Information\n","\n","When we listen to an audio sample it changes constantly. This means that speech is non-stationary signal. Therefore, normal signal processing techniques cannot be applied to get features from audio. However, if the speech signal is observed using a very small duration window, the speech content in that small duration appears to be  stationary. That brought in the concept of short-time processing of speech. \n","\n","MFCC is a techique for short-time processing of speech. \n","\n","**Note: While it is a bonus to understand 'how' a speech signal is converted to a 'representation' by MFCC (or other similar speech processing techniques), it is not mandatory to understand the same. It is sufficient to realize that you're generating a unique numeric representation (a set of numbers) of a speech signal and that this unique representation helps you run your machine learning algorithms over the samples.**"]},{"cell_type":"markdown","metadata":{"id":"mZUtIsOUFv_a"},"source":["## AIML Technique\n","\n","In this short-time processing technique MFCC, a small duration window (say 25 milli sec) is considered for processing of the audio samples at a time. This small duration is called a frame. Now, for each of the frames, MFCC features are computed which give a compact representation of the audio samples. A spectogram is computed for audio samples which gives a heat map of frequencies vs time from the series of spectral vectors generated from audio samples.\n","\n","This representation is obtained by keeping in mind that humans are much better at grasping small changes in the audio at low frequencies than audio at high frequencies. So mel-scale converts high frequencies to human graspable frequencies. "]},{"cell_type":"code","source":["! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/yes_no.zip\n","! unzip yes_no.zip"],"metadata":{"id":"y6MRGeObx9r0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8L2zGiq9foa2"},"source":["### Importing Required Packages"]},{"cell_type":"code","metadata":{"id":"D_wB9zdv_b9y"},"source":["import numpy as np\n","import librosa\n","\n","# Visualization\n","import matplotlib.pyplot as plt\n","import librosa.display\n","\n","# Import glob to perform pattern matching to find files\n","import glob\n","\n","# Import train_test_split to split the data into train and test sets\n","from sklearn.model_selection import train_test_split\n","\n","# Import MLPClassifier to be trained on features of wav files\n","from sklearn.neural_network import MLPClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ogx1P_LqHUxj"},"source":["#### About glob.iglob:\n","\n","The glob library  provides methods for traversing the file system and returning files that matched a defined set of glob patterns. Here it helps traverse file name matching the \".wav\" pattern\n","\n","**Note:** Refer to  [glob.iglob](https://docs.python.org/3/library/glob.html)"]},{"cell_type":"code","metadata":{"id":"3aQBPBaX3W8S"},"source":["labels = []\n","wave_files = []\n","for filename in glob.iglob('yes_no/*.wav', recursive=True):\n","  # Store the labels of each wav file in a list\n","  labels.append(int((filename.split(\"/\")[-1]).split(\"_\")[0])) # Split the file name to get the labels of the audio files\n","  # Store the wav files in a list\n","  wave_files.append(filename)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmKykvSxsgd4"},"source":["# Check the length of wav files\n","len(wave_files)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YtEBGU5fW76z"},"source":["### Mel Spectrogram\n","\n","Human ears tend to listen to sounds on the log scale. That means, at lower frequencies we can detect small changes, but at higher frequencies our ears become less sensitive to small changes. For example, the difference between 10 Hz and 20 Hz would sound almost the same to us as that between 1000 Hz and 2000 Hz. \n","\n","Let us compute the Mel Spectrogram features using a convenient function in the `librosa` library in Python:"]},{"cell_type":"markdown","metadata":{"id":"Y10Vz0VDYZnC"},"source":["Below is the function for loading the wavefile and extracting the melspectogram features at a sample rate\n","\n","* Refer to [librosa.load](http://man.hubwiz.com/docset/LibROSA.docset/Contents/Resources/Documents/generated/librosa.core.load.html)\n","\n","* Refer to [librosa.feature.melspectrogram](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html)"]},{"cell_type":"code","metadata":{"id":"ZUuLLyddYZnE"},"source":["def get_melspectrogram_features(filename):\n","    y, sr = librosa.load(filename)\n","    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n","    # The log of the spectrogram is a better parameter to observe rather than the spectrogram itself.\n","    # Convert to log scale (dB). We'll use the peak power (max) as reference.\n","    log_S = librosa.power_to_db(S, ref=np.max)\n","    return log_S,sr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qbXV7RceYZnL"},"source":["Let us try to extract the features for one of the wave file and plot the melspectrogram \n","\n","**Note:** Refer to [librosa.display.specshow](https://librosa.org/doc/latest/generated/librosa.display.specshow.html)\n"]},{"cell_type":"code","metadata":{"id":"XtUy-DaP_b9-"},"source":["features, sr = get_melspectrogram_features(wave_files[0])\n","plt.figure(figsize=(12, 4))\n","librosa.display.specshow(features, sr=sr, x_axis='time', y_axis='mel')\n","plt.title('Mel power spectrogram ')\n","plt.colorbar(format='%+02.0f dB')\n","plt.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qin-hjCShaoU"},"source":["# Play the audio from wave file\n","import IPython.display as ipd\n","ipd.Audio(wave_files[0]) "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yiZMR8ZxInfO"},"source":["### Extract the MFCC features for all Audio files\n","\n","\n","* Refer to [librosa.load](http://man.hubwiz.com/docset/LibROSA.docset/Contents/Resources/Documents/generated/librosa.core.load.html)\n","\n","\n","*   Refer to [librosa.feature.mfcc](https://librosa.org/doc/0.7.2/generated/librosa.feature.mfcc.html)  "]},{"cell_type":"code","metadata":{"id":"DP-3oye8TZxb"},"source":["n_mfcc = 30  # mfcc features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WVJqijHmK3CX"},"source":["Since our ear cannot response to very fast change of speech data, we normally cut the speech data into frames before analysis.\n"]},{"cell_type":"code","metadata":{"id":"h_9CS41S_b96"},"source":["# Extract MFCC features \n","\n","def get_mfcc_features(filename, sr=7000):\n","    frames = 15          # Get the MFCC features with frame size 15  \n","    # Load the audio files with the sampling rate 7000.\n","    y, sr = librosa.load(filename, sr=sr)\n","    features = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=30)\n","    # We need to split the signal into short-time frames as the frequencies in a signal change over time.\n","    # So in most cases we don't do the Fourier transform across the entire signal.\n","    if features.shape[1] < frames :\n","        features = np.hstack((features, np.zeros((n_mfcc, frames - features.shape[1]))))\n","    elif features.shape[1] > frames:\n","        features = features[:, :frames]\n","    return features"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mVCKIjMfgCST"},"source":["### Extracting mfcc features for all the samples"]},{"cell_type":"code","metadata":{"id":"ZZniCuHD_b-R"},"source":["features = []                  # It takes some time to run this cell\n","for filename in wave_files:\n","    fea = get_mfcc_features(filename)\n","    features.append(fea)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_QdLHH3Om2a"},"source":["# Check the shape of the features\n","features[0].shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ukfzjKvigEAa"},"source":["### Split the mfcc features into train and test sets with 80-20 split "]},{"cell_type":"code","metadata":{"id":"_FUreImQ_b-W"},"source":["X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Htjjq8CbgHmk"},"source":["Get the length of the train and test data"]},{"cell_type":"code","metadata":{"id":"04PKEJDJ_b-a"},"source":["len(X_train), len(X_test), len(y_train), len(y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5vsTt1A_b-g"},"source":["# Convert the X_train to an array and reshape from 2128*30*15 to 2128*450\n","\n","X_train = np.array(X_train)\n","X_train = X_train.reshape(X_train.shape[0], X_train.shape[1] *  X_train.shape[2])\n","\n","# Convert the X_test to an array and reshape from 533*30*15 to 533*450\n","\n","X_test = np.array(X_test)\n","X_test = X_test.reshape(X_test.shape[0], X_test.shape[1] *  X_test.shape[2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smcjlD8h-sIw"},"source":["# Get the shape of X_train, X_test\n","X_train.shape, X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hAyyo1mQPtMC"},"source":["### Apply MLP classifier on the MFCC features\n","\n","**Note:** Refer to [MLP Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) from sklearn"]},{"cell_type":"code","metadata":{"id":"VdPIHTV0_b-j"},"source":["# Create a object for MLPClassifier \n","clf = MLPClassifier(activation='logistic', hidden_layer_sizes=(100,50), random_state=1, solver='adam')\n","\n","# Fit the train fetaures and labels to the classifier\n","clf.fit(np.array(X_train), np.array(y_train))  \n","\n","# Predict the labels of the test features\n","predicted_values = clf.predict(X_test) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2pGxbZeN_b-5"},"source":["# Get the accuracy score by comparing predicted labels and test labels\n","from sklearn.metrics import accuracy_score\n","accuracy_score(y_test,predicted_values)"],"execution_count":null,"outputs":[]}]}