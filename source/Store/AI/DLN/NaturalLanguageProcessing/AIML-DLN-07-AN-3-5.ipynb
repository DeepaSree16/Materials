{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-07-AN-2-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"y9NNhMb5wbUK"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","\n","### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"qu26Vq9jDTpj"},"source":["### Learning Objectives:\n","\n","At the end of the experiment, you will be able to:\n"," \n","*  generate word embeddings using pre-trained models\n","*  visualize the similar words"]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/glove.6B.zip\n","! unzip glove.6B.zip\n","! wget https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/experiment_related_data/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\n","! unrar e /content/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\n","    "],"metadata":{"id":"h4mfgZMxvvA3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mmXvUFOt9fJv"},"source":["## PART I\n","### Find the similarity between words using GloVe"]},{"cell_type":"code","metadata":{"id":"H9sC9--9wbUq"},"source":["# Import required Packages\n","import pandas as pd\n","import numpy as np\n","\n","# pprint is a native Python library that allows to customize the formatting of output\n","import pprint"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hDwNJHcfPaKd"},"source":["* **Load the GloVe pretrained model**\n","\n","  GloVe stands for “Global Vectors” for word representation. It is developed by Stanford for generating word embeddings. GloVe captures both global statistics and local statistics of a corpus, in order to come up with word vectors.\n"]},{"cell_type":"code","metadata":{"id":"fZt0USXAZVnk"},"source":["GloVe_Dict = {}\n","# Loading the 50-dimensional vector of the model\n","with open(\"glove.6B.50d.txt\", 'r') as f:\n","  for line in f:\n","      values = line.split()\n","      word = values[0]\n","      vector = np.asarray(values[1:], \"float32\")\n","      GloVe_Dict[word] = vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SEswIOf9xeAg"},"source":["# Length of the word vocabulary\n","print(len(GloVe_Dict))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"isE0P3iIPaKf"},"source":["* Develop GloVe Embeddings for the given list of words"]},{"cell_type":"code","metadata":{"id":"UMzp4I3L9c4b"},"source":["words = ['king', 'queen', 'river', 'water', 'ocean', 'tree', 'leaf', 'happy', 'glad', 'mother', 'daughter']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cChTS3_BZsnh"},"source":["# Creating a PrettyPrinter() object\n","pp = pprint.PrettyPrinter()\n","\n","# Vector representation of a specific word \n","print(\"Size of the vector is\", len(GloVe_Dict[\"king\"]))\n","pp.pprint(GloVe_Dict[\"king\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1eBRiHNB-WJ1"},"source":["# Vector representation of each word using GloVe\n","vectors = []\n","for word in words:\n","  vector = GloVe_Dict[word]\n","  vectors.append(vector)\n","print(\"There are %d words and the vector size of each word is %d\" %((len(vectors),len(vectors[0]))))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1J8Kjgo_PaKh"},"source":["* Measure the similarity between the words using cosine_similarity\n"]},{"cell_type":"code","metadata":{"id":"FO1Vc6ZTMavW"},"source":["# Importing the cosine similarity\n","from sklearn.metrics.pairwise import cosine_similarity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WuRZ_OZo0Z-y"},"source":["word_similarity = []\n","for i, word_1 in enumerate(words):\n","  row_wise_simiarity = []\n","  for j, word_2 in enumerate(words):\n","    # Get the vectors of the word using GloVe\n","    vec_1, vec_2 = GloVe_Dict[word_1], GloVe_Dict[word_2]\n","\n","    # As the vectors are in one dimensional, convert it to 2D by reshaping\n","    vec_1, vec_2 = np.array(vec_1).reshape(1,-1), np.array(vec_2).reshape(1,-1) \n","\n","    # Measure the cosine similarity between the vectors.\n","    similarity = cosine_similarity(vec_1, vec_2)\n","    row_wise_simiarity.append(np.array(similarity).item())\n","\n","  # Store the cosine similarity values in a list  \n","  word_similarity.append(row_wise_simiarity)\n","\n","# Create a DataFrame to view the similarity between words\n","pd.DataFrame(word_similarity, columns=words, index=words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7D3ZvdVjMqIH"},"source":[" *GloVe derives the semantic relationship between the words. Higher the cosine similarity, the words are relatively closer*\n","\n","*For eg:* *The word 'King' is more closer to word 'Queen'*"]},{"cell_type":"markdown","metadata":{"id":"SzDW1vT2PaKj"},"source":["* Visualize the words in 2D-plane by reducing the dimensions using PCA"]},{"cell_type":"code","metadata":{"id":"xVPYkoF5S9rm"},"source":["# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n","from sklearn.decomposition import PCA\n","\n","# n_components in PCA specifies the no.of dimensions\n","pca = PCA(n_components=2)\n","\n","# Fit and transform the vectors using PCA model\n","reduced_vectors = pca.fit_transform(vectors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QnczXr2BLNj"},"source":["from matplotlib import pyplot as plt\n","plt.figure(figsize=(7,5))\n","plt.scatter(reduced_vectors[:,0],reduced_vectors[:,1], s = 12, color = 'red')\n","plt.xlim([-3.5,4.5])\n","plt.ylim([-3.5,3.5])\n","x, y = reduced_vectors[:,0] , reduced_vectors[:,1]\n","for i in range(len(x)):\n","  plt.annotate(words[i],xy=(x[i], y[i]),xytext=(x[i]+0.05,y[i]+0.05))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YkQDxw7m9k-j"},"source":["## PART II\n","### Find the similarity between words using Word2Vec\n","\n"]},{"cell_type":"markdown","metadata":{"id":"jWJai9jPO3RX"},"source":["* Load Gensim pretrained model\n","\n","  * Gensim is an open source Python library for natural language processing. It is developed and is maintained by the Czech natural language processing researcher Radim Řehůřek and his company RaRe Technologies. \n","\n","  * Use gensim to load a word2vec model, pretrained on google news, covering approximately 3 million words and phrases. The vector size is 300 features.\n","\n","  * Download the google news bin file with the limit 500000 words and save in a binary word2vec format. If **binary = True**, then the data will be saved in binary word2vec format, else it will be saved in plain text."]},{"cell_type":"code","metadata":{"id":"2mfXzP6WCXOB"},"source":["import gensim\n","\n","# Load Google news 300 vectors file\n","model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_PgoxX5VO9dd"},"source":["* Develop Word2Vec Embeddings for the list of words"]},{"cell_type":"code","metadata":{"id":"hg3R90IqMwQG"},"source":["# Vector representation of a specific word \n","print(\"Size of the vector is\", len(model[\"king\"]))\n","pp.pprint(model[\"king\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"euxldHh-M4BL"},"source":["# Vector representation of each word using Word2Vec\n","word2vec = []\n","\n","for word in words:\n","  try:\n","    word2vec.append(model[word])\n","  except:\n","    pass\n","print(\"There are %d words and the vector size of each word is %d\" %(len(word2vec),len(word2vec[0])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w23olxol8BIf"},"source":["* Measure the similarity between the words using cosine_similarity\n"]},{"cell_type":"code","metadata":{"id":"KvdSBvgP30h-"},"source":["w2v_similarity = []\n","for i, word_1 in enumerate(words):\n","  w2v_row_wise_simiarity = []\n","  for j, word_2 in enumerate(words):\n","    # Get the vectors of the word using Word2Vec\n","    vec_1, vec_2 = model[word_1], model[word_2]\n","\n","    # As the vectors are in one dimensional, convert it to 2D by reshaping\n","    vec_1, vec_2 = np.array(vec_1).reshape(1,-1), np.array(vec_2).reshape(1,-1) \n","\n","    # Measure the cosine similarity between two vectors\n","    similarity = cosine_similarity(vec_1,vec_2)\n","    w2v_row_wise_simiarity.append(np.array(similarity).item())\n","\n","  # Store the cosine similarity values in a list    \n","  w2v_similarity.append(w2v_row_wise_simiarity)\n","\n","pd.DataFrame(w2v_similarity, columns = words, index = words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8JKH7o3lYlbm"},"source":["*Higher the cosine similarity, the words are more closer*\n","\n","*For eg: The word 'King' is more similar to the word 'Queen'*"]},{"cell_type":"markdown","metadata":{"id":"xkD6RUWK8GUG"},"source":["* Visualize the words in 2D-plane by reducing the dimensions using PCA."]},{"cell_type":"code","metadata":{"id":"qd1TTmcSSy2w"},"source":["# Create a 2-dimensional PCA model of the word vectors using the scikit-learn PCA class\n","from sklearn.decomposition import PCA\n","\n","# n_components in PCA specifies the no.of dimensions\n","pca = PCA(n_components=2)\n","\n","# Fit and transform the vectors using PCA model\n","reduced_w2v = pca.fit_transform(word2vec)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"27VShOpkNKay"},"source":["plt.figure(figsize=(8,5))\n","plt.scatter(reduced_w2v[:,0],reduced_w2v[:,1], s = 12, color = 'red')\n","plt.xlim([-2.5,2.5])\n","plt.ylim([-2.5,2.5])\n","x, y = reduced_w2v[:,0] , reduced_w2v[:,1]\n","for i in range(len(x)):\n","  plt.annotate(words[i],xy=(x[i], y[i]),xytext=(x[i]+0.05,y[i]+0.05))"],"execution_count":null,"outputs":[]}]}