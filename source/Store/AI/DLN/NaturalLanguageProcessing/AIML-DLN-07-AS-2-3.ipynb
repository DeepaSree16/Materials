{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-07-AS-1-2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UH62CWxTDTpf"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"qu26Vq9jDTpj"},"source":["### Learning Objectives:\n","\n","At the end of the experiment, you will be able to:\n","\n","*  generate vectors using Word2Vec model"]},{"cell_type":"code","metadata":{"id":"AdDBMT1gCRgM","cellView":"form"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"520\" height=\"440\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Walkthrough/Aptitude_classification.mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FL0Ve1abn6YJ"},"source":["## Dataset\n","\n","Being able to classify the questions will be difficult in natural language processing. The dataset is taken from the TalentSprint aptitude questions.\n","\n","## Description\n","This dataset has the following columns:\n","1. **Category:** Gives the high-level categorization of the question\n","2. **Sub-Category:** Determines the type of questions\n","3. **Article:** Gives the article name of the question\n","4. **Questions:** Questions are listed\n","5. **Answers:** Contains answers\n","\n","\n","\n","The dataset, which is considered in the experiment is partially pre-processed using BeautifulSoup and removed punctuations, HTML tags.\n"]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Cleaned_Aptitude_Classification.csv\n","! wget https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/experiment_related_data/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\n","! unrar e /content/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar \n","    "],"metadata":{"id":"CbCID4ktuSCf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pP9bMLVgLRTp"},"source":["### Importing required packages\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"1uTIg1Jtaj_w"},"source":["import nltk\n","import gensim\n","import pandas as pd\n","from nltk.stem import WordNetLemmatizer \n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","nltk.download('punkt')\n","nltk.download(\"stopwords\")\n","nltk.download('wordnet')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KeTxW7Uf8xYn"},"source":["### Data loading and preparation\n","\n","Load the aptitude classification dataset containing all the aptitude questions of various sub-categories\n"]},{"cell_type":"code","metadata":{"id":"LbrgXcFcBtgW"},"source":["data = pd.read_csv(\"/content/Cleaned_Aptitude_Classification.csv\")\n","data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4sEP70HSTRlw"},"source":["data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0T5jFli81J0"},"source":["Out of 15 sub-categories from the loaded data, choosing two sub-categories (Misspell words, Finding Errors) for this experiment"]},{"cell_type":"code","metadata":{"id":"zglkFtj-LAIf"},"source":["# Extracting two sub-categories questions \n","category1_Que = data[data['Sub-Category']== 'Misspell words']['Questions'].values\n","category2_Que = data[data['Sub-Category']== 'Finding Errors']['Questions'].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dGxvheKY-lmv"},"source":["# Printing the sample question from the first chosen Sub-Category\n","category1_Que[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T7WYeZTQLlu6"},"source":["### Pre-processing and tokenization\n","\n","Pre-processing the text and applying tokenization to get vocabulary words of both chosen sub-categories"]},{"cell_type":"code","metadata":{"id":"fopV3S-OuImU"},"source":["# Intializing nltk requirements for pre-processing\n","lemmatizer = WordNetLemmatizer()\n","stoplist = set(stopwords.words('english')) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oWzYzn50uoLr"},"source":["# Tokenize the sentence and get vocab words\n","def Tokenize(AllQuestions):\n","  pre_processed_words = []\n","  for each in AllQuestions:\n","    words = word_tokenize(each)\n","    words = [lemmatizer.lemmatize(w) for w in words]\n","    pre_processed_words.extend(words)\n","\n","  pre_processed_words = set(pre_processed_words)\n","\n","  pre_processed_words = [word for word in pre_processed_words if word not in stoplist]\n","  return pre_processed_words"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h6yQqOLuvHbj"},"source":["# Calling the above Tokenize function to get vocab words of both sub-categories\n","category1_words = Tokenize(category1_Que)\n","category2_words = Tokenize(category2_Que)\n","\n","# Combining the words of two sub-categories\n","all_words = category1_words + category2_words\n","print(\"Number of valid words after pre-processing:\", len(all_words))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QTNmN01RHUf-"},"source":["### Load the word2vec model"]},{"cell_type":"markdown","metadata":{"id":"csm5a0hrDTqI"},"source":["Load Gensim pretrained model\n","\n","  * Gensim is an open source Python library for natural language processing. It is developed and is maintained by the Czech natural language processing researcher Radim Rehurek and his company RaRe Technologies. \n","\n","  * Use gensim to load a word2vec model, pretrained on google news, covering approximately 3 million words and phrases. The vector length is 300 features.\n","\n","  * Download the google news bin file with the limit 500000 words and save in a binary word2vec format. If **binary = True**, then the data will be saved in binary word2vec format, else it will be saved in plain text.\n"]},{"cell_type":"code","metadata":{"id":"2mfXzP6WCXOB"},"source":["# Load 300 vectors directly from the file. As the model is in .bin extension, we need to enable default parameter, binary = True\n","model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ksKrewQRDLKe"},"source":["# Pre-trained model gives representation of 300 size vector\n","print(\"Dimension of the word 'tree': \", len(model['tree']))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W3Y6UlpLKAeT"},"source":["### Generate vectors for each word\n","\n","Words that appear in both the sub-categories will have the same representation but different label, which may lead to less accuracy in classification, Ignoring the words that are intersecting both the chosen sub-categories"]},{"cell_type":"code","metadata":{"id":"fxCopZzgEuwW"},"source":["# Get vector representation using model for the all the extraced words of two sub-categories\n","vectors, labels = [], []\n","for word in all_words:\n","  try:\n","    # Ignoring the words that appear in both sub-categories\n","    if ~(word in category1_words and word in category2_words):\n","      vectors.append(model[word])\n","      if word in category1_words:\n","        labels.append(0)\n","      else:\n","        labels.append(1)\n","  except:\n","    pass\n","print(\"Number of words:\", len(labels))\n","print(\"Number of dimensions in each vector:\", len(vectors[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lwye2kelJLy2"},"source":["### Split the Data into train and test"]},{"cell_type":"code","metadata":{"id":"FjlN6HSMQkqm"},"source":["from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(vectors, labels, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PSGk6xiGJPMt"},"source":["### Fit the model and calculate the accuracy"]},{"cell_type":"code","metadata":{"id":"Ig9_o2V1FcOC"},"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","clf = DecisionTreeClassifier()\n","clf.fit(X_train, y_train)\n","\n","print(\"Accuracy of the model:\",clf.score(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jfj1RGVTIEPM"},"source":["### Ungraded Exercise: \n","\n","Take any other two sub-categories and get vector representation using word2vec"]},{"cell_type":"code","metadata":{"id":"_z5_5cRKIBWd"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]}]}