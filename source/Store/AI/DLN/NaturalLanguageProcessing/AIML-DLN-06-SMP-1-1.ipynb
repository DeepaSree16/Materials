{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-06-SMP-1-1.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PsKRUcQh27Dp"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"C3BQbYtN2-5L"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"DP5Nf6UZ3Ej1"},"source":["The problem is to identify the subcategory and classify the question based on the group it belongs to.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WeHg23di3oHF"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"27fwuBVS3tF3"},"source":["At the end of the experiment, you will be able to understand:\n","\n","*   Beautiful Soup\n","*   Use NLTK package\n","*   Text Representation\n","*   Classification"]},{"cell_type":"markdown","metadata":{"id":"FL0Ve1abn6YJ"},"source":["## Dataset\n","Being able to classify the questions will be difficult in natural language processing. The dataset is taken from the TalentSprint aptitude questions which contains more than 20K questions.\n","\n","## Description\n","This dataset has the following columns:\n","1. **Category:** Gives the high-level categorization of the question\n","2. **Sub-Category:** Determines the type of questions\n","3. **Article:** Gives the article name of the question\n","4. **Questions:** Questions are listed\n","5. **Answers:** Contains answers\n"]},{"cell_type":"markdown","metadata":{"id":"ndQNKsjS7c04"},"source":["### Grading = 20 Marks"]},{"cell_type":"code","source":["! wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Aptitude_Classification_data.csv\n","! wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Mentors_Test_Data.csv\n","    "],"metadata":{"id":"X6U5LqaWQWUJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYyfIzohaMMC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622637233238,"user_tz":-330,"elapsed":2619,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"1a316aa1-e6ce-4001-c089-8adfdbf61237"},"source":["# Import Python Libraries\n","from bs4 import BeautifulSoup\n","import nltk\n","import re\n","import string\n","import warnings\n","import numpy as np\n","import pandas as pd\n","from collections import Counter\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score\n","warnings.filterwarnings('ignore')\n","nltk.download('punkt')\n","nltk.download(\"stopwords\")\n","nltk.download('averaged_perceptron_tagger')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"2DKmejLMh2YH"},"source":["##   **Stage 1**:  Dataset Preparation\n","\n","### 1 Mark -> Load the data set and prepare the data based on group allocation. \n","\n","Each group should consider their respective sub-categories as mentioned below:\n","\n","> Team A = Groups 1, 4, 7, 10;   &nbsp; &nbsp; Sub-Category = Misspell words, Algebra, Percentages, Mathematical operations, Probability\n","\n","> Team B = Groups 2, 5, 8, 11; &nbsp; &nbsp;   Sub-Category = Finding Errors, Ratio and Proportion, Logarithms, Time and Distance, Simple and Compound Interest\n","\n","> Team C = Groups 3, 6, 9, 12;  &nbsp; &nbsp;  Sub-Category =  Synonyms and Antonyms, Time and Work, Permutations and Combinations, LCM and HCF, Profit and Loss\n","\n","\n","**Hint:** &nbsp; To access Sub-Categories from given Data, refer [link](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.isin.html)"]},{"cell_type":"code","metadata":{"id":"rZ1vto-DAAvZ","colab":{"base_uri":"https://localhost:8080/","height":225},"executionInfo":{"status":"ok","timestamp":1622637286737,"user_tz":-330,"elapsed":484,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"ecc1f741-5ad9-4cc6-8951-6f7f3a4af88f"},"source":["# YOUR CODE HERE TO LOAD THE APTITUDE CLASSIFICATION DATASET & EXTRACT THE DATA BASED ON YOUR SUB-CATEGORIES\n","data = pd.read_csv(\"Aptitude_Classification_data.csv\")\n","print(data.shape)\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(4631, 5)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Category</th>\n","      <th>Sub-Category</th>\n","      <th>Article</th>\n","      <th>Questions</th>\n","      <th>Answers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Verbal</td>\n","      <td>Misspell words</td>\n","      <td>chapter 1</td>\n","      <td>Which of the following is correct?\\n\\n\\n\\n\\n</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Quantitative</td>\n","      <td>Time and Distance</td>\n","      <td>Time and Distance - Model 05</td>\n","      <td>Rohan leaves point A and reaches point B in 6 ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Verbal</td>\n","      <td>Finding Errors</td>\n","      <td>44054</td>\n","      <td>Read the sentence to find out whether there is...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Quantitative</td>\n","      <td>Time and Work</td>\n","      <td>tech mahindra_5th August</td>\n","      <td>4 men can check exam papers in 8 days working ...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Quantitative</td>\n","      <td>Permutations and Combinations</td>\n","      <td>AX10DEPT01</td>\n","      <td>From 13 persons, how many ways of selection of...</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Category  ... Answers\n","0        Verbal  ...       2\n","1  Quantitative  ...       2\n","2        Verbal  ...       2\n","3  Quantitative  ...       2\n","4  Quantitative  ...       2\n","\n","[5 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0frsEEPrtzvK","executionInfo":{"status":"ok","timestamp":1622637289134,"user_tz":-330,"elapsed":330,"user":{"displayName":"tanuja b","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg9Q6sUC5CBS6NIWyhPbrCA-02jrcvBDs3WpQT_sQ=s64","userId":"04437579799669360422"}},"outputId":"3b1767c5-4313-41d7-bfee-dca22cac1dfb"},"source":["data.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(4631, 5)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"aAjhycS2RFX0"},"source":["categories = [['Misspell words','Algebra', 'Percentages', 'Mathematical operations','Probability'],\n","            ['Finding Errors', 'Ratio and Proportion', 'Logarithms', 'Time and Distance', 'Simple and Compound Interest'],\n","            ['Synonyms and Antonyms','Time and Work', 'Permutations and Combinations', 'LCM and HCF', 'Profit and Loss']]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5RzclUxhkn67"},"source":["## **Stage 2:** Data Pre-Processing\n","\n","### 3 Marks -> Clean and Transform the data into a specified format\n","\n","*   Remove the rows of the Questions column which contains blank / NaN.\n","\n","\n","*   Few set of questions have HTML tags within the question.\n","  - You can use Beautiful Soup library to convert HTML into text (Refer **\"Dealing with HTML\"** section from this [link](https://www.nltk.org/book/ch03.html).)\n","\n","\n","*  Consider Question column as feature and Sub-category as target variable. Convert Sub-category into numerical.\n","\n","*  Drop the unwanted columns\n","\n","\n","  **Hint:** Use Label Encoder for obtaining a numeric representation, refer to the [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html). "]},{"cell_type":"code","metadata":{"id":"oM0DzfyZSkBv"},"source":["print(data.isnull().sum())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EZ_211jTSmrU"},"source":["print(\"Shape of the data before dropping the na values\",data.shape)\n","data.dropna(inplace=True)\n","\n","Team_data = data[data['Sub-Category'].isin(category)]\n","print(\"Shape of the team data is:\",Team_data.shape)\n","Team_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Whj-50DlTwyZ"},"source":["# YOUR CODE HERE for BeatifulSoup\n","\n","from bs4 import BeautifulSoup                                                                                                                                                                   \n","\n","def cleanandTransform(data1):\n","  data1 = data1.drop(list(data1[data1[\"Questions\"].isnull()].index)) \n","  data1[\"Questions\"] = [BeautifulSoup(text,\"html.parser\").get_text() for text in data1[\"Questions\"]]\n","  return data1.drop(['Category','Article','Answers','Sub-Category'],axis=1)\n","\n","features = cleanandTransform(Team_data)\n","print(features.shape)\n","features.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VaMiP9gextc5"},"source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder() # DO NOT CHANGE THIS LINE as we will be using for the Test evaluation.\n","\n","# YOUR CODE HERE for Fit label encoder and return encoded labels\n","\n","targets = le.fit(Team_data['Sub-Category'])\n","targets = le.transform(Team_data['Sub-Category'])\n","print(\"Shape of the Targets is: \",targets.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7AOtxyuBtdKv"},"source":["## **Stage 3:** Text representation using Bag of Words (BOW)\n","\n","### 3 Marks -> a) Get valid words from all questions & add them to a list.\n","\n","\n","Treat each question as a separate document and get the list of words using the following:\n","1.   Split the sentence into words\n","\n","2.   Remove Stop words. Use NLTK packages for getting the Stop words.\n","\n","3.   Replace proper names with \"name\" \n","  - Example: \"Rahul\" -> \"name\"\n","       \n","4.   Remove the single white space character (\\n, \\t, \\f, \\r), refer [link](https://developers.google.com/edu/python/regular-expressions)\n","\n","5.   Ignore words whose length is less than 3 (Eg: 'is', 'a').\n","\n","6.   Remove punctuation and non-alphabetic words\n","\n","7.   Convert the text to lowercase\n","\n","8.   Use the Porter Stemmer to normalize the words\n","\n","\n","Refer [link](https://www.nltk.org/book/ch03.html) for extracting the words.\n","\n","Refer [link](https://medium.com/free-code-camp/an-introduction-to-bag-of-words-and-how-to-code-it-in-python-for-nlp-282e87a9da04) for more information."]},{"cell_type":"code","metadata":{"id":"DzT2VGv_TgZ5"},"source":["stoplist = set(stopwords.words('english'))\n","porter = nltk.PorterStemmer()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jLyMy2q9rGK_"},"source":["def extract_words(question):\n","    # YOUR CODE HERE\n","    # Hint: Extract words for each question using the above 8 instructions.\n","    words = word_tokenize(question)\n","      \n","    preprocessed_text = [w for w in words if w not in stoplist] \n","    preprocessed_text = [w for w in preprocessed_text if len(w) > 3]\n","    preprocessed_text = ['name' if w[1]=='NNP' else w[0] for w in  nltk.pos_tag(preprocessed_text)]\n","    preprocessed_text = [re.sub('\\s', \" \", w) for w in preprocessed_text]\n","    remove_punctuation = str.maketrans('', '', string.punctuation)\n","    preprocessed_text = [w.translate(remove_punctuation).lower() for w in preprocessed_text]\n","    preprocessed_text = [porter.stem(w) for w in preprocessed_text]\n","    return preprocessed_text "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C_eiynShd7bK"},"source":["def tokenize(allquestions):\n","  valid_words = []\n","  for question in allquestions:\n","    words = extract_words(question)\n","    valid_words.extend(words)\n","  return set(valid_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_ty4DphvV7oM"},"source":["# Use the function to extract words for all questions\n","# YOUR CODE HERE\n","\n","allquestions = features[\"Questions\"].values\n","vocab = tokenize(allquestions)\n","len(vocab)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9csJ86Bivg1H"},"source":["### 4 Marks -> b) Generate vectors that can be used by the machine learning algorithm\n","\n","\n","1.   The length of the vector for each question will be the length of the valid words. Initialize each vector with all Zeros\n","\n","2.   Compare each valid word with the words in question and generate the vectors based on the counter frequency of the word in that question.\n","\n"]},{"cell_type":"code","metadata":{"id":"gbV4VzDOLQ4X"},"source":["def generate_vectors(question):\n","    # YOUR CODE HERE\n","    # Hint: Initialize each vector with all zeros. \n","    bag_vector = np.zeros(len(vocab))\n","    \n","    # Extracting words for each question and count the words\n","    words = extract_words(question)\n","    word_dict = Counter(words)\n","    \n","    # YOUR CODE HERE \n","    # Hint: If the word is in valid words then generate the vectors based on the counter frequency of the word in that question.\n","    for word in words:\n","      for i,w in enumerate(vocab):\n","          if w == word:\n","              bag_vector[i] = word_dict[word]\n","    return bag_vector"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mnQEkbj--Zof"},"source":["# Use the above function for collecting the vectors of all questions into a list.\n","# YOUR CODE HERE\n","\n","Sentence_vector = []\n","for question in allquestions:\n","  Sentence_vector.append(generate_vectors(question))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I-Og4IX5ZYRm"},"source":["## **Stage 4:** Classification\n","\n","### 3 Marks -> Perform a Classification \n","\n","1.   Identify the features and labels\n","\n","2.   Use train_test_split for splitting the train and test data\n","\n","3.   Fit your model on the train set using fit() and perform prediction on the test set using predict()\n","\n","4. Get the accuracy of the model\n","\n","## Expected Accuracy above 90%\n"]},{"cell_type":"code","metadata":{"id":"FjlN6HSMQkqm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622311707310,"user_tz":-330,"elapsed":621,"user":{"displayName":"AIML Support","photoUrl":"","userId":"10944637975474083227"}},"outputId":"7b44dee1-7e62-4379-b342-0c510ea5ecb6"},"source":["from sklearn.model_selection import train_test_split\n","\n","# YOUR CODE HERE\n","# X is the vectors and Y is the labels\n","X_train,X_test, y_train, y_test = train_test_split(Sentence_vector,targets,test_size=0.2,random_state=40)\n","\n","#DecisionTreeClassifier\n","clf = DecisionTreeClassifier()\n","clf.fit(X_train, y_train)\n","predicted_labels = clf.predict(X_test)\n","\n","print(accuracy_score(y_test,predicted_labels))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0.9108910891089109\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BHxULg4OggQe"},"source":["## **Stage 5:** Evaluation (This is for Mentors)\n","\n","### 6 Marks -> Evaluate with the given test data \n","\n","1.  Loading the Test data\n","\n","2.  Converting the Test data into vectors\n","\n","3.  Pass through the model and verify the accuracy\n","\n","## Expected Accuracy above 90%\n"]},{"cell_type":"code","metadata":{"id":"BWM4Boa4zXAs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1622311673914,"user_tz":-330,"elapsed":651,"user":{"displayName":"AIML Support","photoUrl":"","userId":"10944637975474083227"}},"outputId":"d53ee36a-dd19-418e-b47c-bae53597ba37"},"source":["# YOUR CODE HERE for selecting the trained classifier model, eg: MODEL = decision_tree\n","MODEL = clf #ENTER YOUR MODEL\n","\n","Test_data = pd.read_csv(\"Mentors_Test_Data.csv\")\n","Test_data = Test_data[Test_data['Sub-Category'].isin(le.classes_)]\n","labels = le.transform(Test_data['Sub-Category'])\n","Test_questions= Test_data['Questions']\n","\n","Test_BOW=[]\n","for TQ in Test_questions: \n","  Test_vectors = generate_vectors(TQ) \n","  Test_BOW.append(Test_vectors)\n","\n","predict = MODEL.predict(Test_BOW) \n","accuracy_score(labels, predict)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9113924050632911"]},"metadata":{"tags":[]},"execution_count":34}]}]}