{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AIML-DLN-07-AN-2-3.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"71ADWiP3OTnA"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint\n","### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"ujLy0fEXdGDF"},"source":["#  Word Arithmetic"]},{"cell_type":"code","metadata":{"id":"PdfXQfcN8K0Z","cellView":"form"},"source":["#@title Case Study Walkthrough\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"520\" height=\"440\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/queen_king_man_woman.mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YM4WuZYudQEr"},"source":["\n","#### Man = Queen - King + Women"]},{"cell_type":"markdown","metadata":{"id":"58ASB6F4O7Rm"},"source":["We saw a naive way to represent words as dense vectors which can leverage the semantics of the words in the class.\n","\n","The problem with count-based word representations is that they are costly in terms of memory to compute large co-occurrence matrices. Let us see another method to find representations of words without explicitly counting words.\n","\n","Here, we aim to predict the next word given the context in which the word appears. (For example, given the last $n$ words, predict the next word). A very smart way to do this is by using a feature representation called \"Word2Vec\" with transforms each word into 300-dimensional vectors."]},{"cell_type":"code","source":["! wget -qq https://www.dropbox.com/s/fm7nvhyvekhaka4/AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1\")\n","! mv AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1 AIML_DS_WORD2VEC2D_STD.pkl.zip\")\n","! unzip AIML_DS_WORD2VEC2D_STD.pkl.zip\")"],"metadata":{"id":"rz5BDbFzurt6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YJaskp1-dWTU"},"source":["## Visualization\n","\n","Before we go to the actual 300 dimensional vectors, let's try to explore some of the more intriguing properties of word2vec.\n","\n","You have been provided with a sample of word vectors. **We have reduced the dimensionality of the 300-dimensional vectors to 2 dimensions, so that we can plot them in matplotlib.**"]},{"cell_type":"code","metadata":{"id":"vs330Rmr3AzQ"},"source":["from matplotlib import pyplot as plt\n","import pickle"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbbYGvArFnSV"},"source":["def plot_values(values, labels, figsize = (8,4), c = []):\n","    x = []\n","    y = []\n","    for value in values:\n","        x.append(value[0])\n","        y.append(value[1])\n","        \n","    plt.figure(figsize=figsize) \n","    for i in range(len(labels)):\n","        plt.scatter(x[i],y[i], color=c[i])\n","        plt.annotate(labels[i],\n","                     xy=(x[i], y[i]),\n","                     xytext=(5, 2),\n","                     textcoords='offset points',\n","                     ha='right',\n","                     va='bottom')\n","    plt.show()\n","\n","\n","\n","#.pkl file which is already trainied file which contain two dimentional represenatation of a word\n","two_dim_model = pickle.load(open('AIML_DS_WORD2VEC2D_STD.pkl', 'rb'))\n","\n","wv_labels = {}\n","for vec, word in two_dim_model:\n","    wv_labels[word] = vec"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2Yxw_7FDTqP"},"source":["We have given you the 2D representation of different word vectors. Plot the word vectors for the words 'King', 'Queen', 'man', 'women', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest' in green color:"]},{"cell_type":"code","metadata":{"id":"WN6ImoSNFxCP"},"source":["wv_list = ['king', 'queen', 'man', 'woman', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest']\n","wv_new_labels = {}\n","for word in wv_list:\n","    wv_new_labels[word] = wv_labels[word]\n","\n","colors = ['green' for i in range(len(wv_new_labels))]\n","plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ERU80PamZizJ"},"source":["Consider the word analogy question: \"**Queen is to King, as Woman is to what?**\"\n","\n","To answer this question, we aim to find what the difference between a \"King\" and \"Queen\" is, and apply that difference to a \"Woman\". If we try to put this mathematically, we can write:- \n","$$\n"," Answer = Queen - King + Woman\n","$$\n","\n","Compute the value of the vector on the right hand side of the above equation and plot the resulting vector in red in the same plot as before. "]},{"cell_type":"code","metadata":{"id":"IsCBG1moGnag"},"source":["answer = wv_new_labels['queen']  - wv_new_labels['king'] + wv_new_labels['woman']\n","\n","wv_new_labels['answer'] = answer\n","\n","colors = ['green' if word not in ['answer'] else 'red' for word in wv_new_labels]\n","\n","plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LQHQIBB3ZqZg"},"source":["Notice how the answer vector is very close to the vector of the word \"Man\"? Incidentally, \"Man\" is the right answer to the word analogy question! This is the power of Word2Vec representations."]}]}