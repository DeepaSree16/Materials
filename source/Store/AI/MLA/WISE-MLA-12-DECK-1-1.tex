\documentclass{beamer}
\usetheme{Madrid}
\usepackage{amsmath}
\usepackage{ragged2e}


\title{ML Algorithms by Similarity}
\author{by Talentsprint Pvt. Ltd.}
\centering
\date{July 2020}

\begin{document}
\maketitle
\begin{frame}{Content}
	\begin{itemize}
		\item Regression
		\item Instance-Based
		\item Regularization
		\item Decision Trees
		\item Bayesian
		\item Clustering
		\item Association Rules
		\item Artificial Neural Networks
		\item Deep Learning
		\item Dimensionality Reduction
		\item Ensemble
	\end{itemize}
\end{frame}

\begin{frame}{Regression}
\begin{flushleft}
	Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model. 
\\
\vspace{10pt}
	The most popular regression algorithms are:
\begin{itemize}
	\item Ordinary Least Squares Regression (OLSR)
	\item Linear Regression
	\item Logistic Regression
	\item Stepwise Regression
	\item Multivariate Adaptive Regression Splines (MARS)
	\item Locally Estimated Scatterplot Smoothing (LOESS)
\end{itemize}
\end{flushleft}
\end{frame}

\begin{frame}{Instance-Based}
	\begin{flushleft}
		Instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model.

Such methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on the representation of the stored instances and similarity measures used between instances.
\\
\vspace{10pt}
	The most popular instance-based algorithms are:
\begin{itemize}
	\item k-Nearest Neighbor (kNN)
	\item Learning Vector Quantization (LVQ)
	\item Self-Organizing Map (SOM)
	\item Locally Weighted Learning (LWL)
	\item Support Vector Machines (SVM)
\end{itemize}
	\end{flushleft}
\end{frame}

\begin{frame}{Regularization}
\begin{flushleft}
		An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing.

		They are popular, powerful and generally simple modifications made to other methods.
\\
\vspace{10pt}
	The most popular regularization algorithms are:
\begin{itemize}
	\item Ridge regression
	\item Least Absolute Shrinkage and Selection Operator (LASSO)
	\item Elastic Net
	\item Least-Angle Regression (LARS)
\end{itemize}
	\end{flushleft}
\end{frame}

\begin{frame}{Decision Trees}
\begin{flushleft}
		Decision tree methods construct a model of decisions made based on actual values of attributes in the data.

Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems.
\\
\vspace{10pt}
	The most popular decision tree algorithms are:
\begin{itemize}
	\item Classification and Regression Tree (CART)
\item Iterative Dichotomiser 3 (ID3)
\item C4.5 and C5.0 (different versions of a powerful approach)
\item Chi-squared Automatic Interaction Detection (CHAID)
\item Decision Stump
\item M5
\item Conditional Decision Trees
\end{itemize}
	\end{flushleft}
\end{frame}

\begin{frame}{Bayesian}
\begin{flushleft}
		Bayesian methods are those that explicitly apply Bayesâ€™ Theorem for problems such as classification and regression.
\\
\vspace{10pt}
	The most popular bayesian algorithms are:
\begin{itemize}
	\item Naive Bayes
\item Gaussian Naive Bayes
\item Multinomial Naive Bayes
\item Averaged One-Dependence Estimators (AODE)
\item Bayesian Belief Network (BBN)
\item Bayesian Network (BN)
\end{itemize}
	\end{flushleft}
\end{frame}

\begin{frame}{Clustering}
\begin{flushleft}
	Clustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.
\\
\vspace{10pt}
The most popular clustering algorithms are:
\begin{itemize}
	\item k-Means
	\item k-Medians
	\item Expectation Maximisation (EM)
	\item Hierarchical Clustering
\end{itemize}

	\end{flushleft}
\end{frame}

\begin{frame}{Association Rule Learning}
\begin{flushleft}
	Association rule learning methods extract rules that best explain observed relationships between variables in data.

These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.
\\
\vspace{10pt}
The most popular association rule learning algorithms are:

\begin{itemize}
	\item Apriori algorithm
	\item Eclat algorithm
\end{itemize}

	\end{flushleft}
\end{frame}

\begin{frame}{Artificial Neural Networks}
\begin{flushleft}
	Artificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks.

They are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.
\\
\vspace{10pt}
The most popular artificial neural networks algorithms are:

\begin{itemize}
	\item Perceptron
\item Multilayer Perceptrons (MLP)
\item Back-Propagation
\item Stochastic Gradient Descent
\item Hopfield Network
\item Radial Basis Function Network (RBFN)
\end{itemize}

	\end{flushleft}
\end{frame}

\begin{frame}{Deep Learning}
\begin{flushleft}
	They are concerned with building much larger and more complex neural networks and, as commented on above, many methods are concerned with very large datasets of labelled analog data, such as image, text. audio, and video.
\\
\vspace{10pt}
The most popular deep learning algorithms are:

\begin{itemize}
	\item Convolutional Neural Network (CNN)
\item Recurrent Neural Networks (RNNs)
\item Long Short-Term Memory Networks (LSTMs)
\item Stacked Auto-Encoders
\item Deep Boltzmann Machine (DBM)
\item Deep Belief Networks (DBN)
\end{itemize}

	\end{flushleft}
\end{frame}
\begin{frame}{Dimensionality Reduction}
\begin{flushleft}
	Like clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner.

This can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method.
\\
\vspace{10pt}
The most popular dimensionality reduction algorithms are:

\begin{itemize}
	\item Principal Component Analysis (PCA)
\item Principal Component Regression (PCR)
\item Partial Least Squares Regression (PLSR)
\item Sammon Mapping
\item Multidimensional Scaling (MDS)
\item Projection Pursuit
\item Linear Discriminant Analysis (LDA)
\item Mixture Discriminant Analysis (MDA)
\item Quadratic Discriminant Analysis (QDA)
\item Flexible Discriminant Analysis (FDA)
\end{itemize}

	\end{flushleft}
\end{frame}
\begin{frame}{Ensemble}
	\begin{flushleft}
	Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction.

Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.
\\
\vspace{10pt}
The most popular ensemble algorithms are:

\begin{itemize}
	\item Boosting
\item Bootstrapped Aggregation (Bagging)
\item AdaBoost
\item Weighted Average (Blending)
\item Stacked Generalization (Stacking)
\item Gradient Boosting Machines (GBM)
\item Gradient Boosted Regression Trees (GBRT)
\item Random Forest
\end{itemize}

	\end{flushleft}
\end{frame}
\begin{frame}
\huge{\centerline{The End}}
\end{frame}
\end{document}