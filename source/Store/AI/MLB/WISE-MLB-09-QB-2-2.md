# CFU

## Question 1

In a real problem, you should check to see if the SVM is separable and then include slack variables if it is not separable. 

1. True
2. False

> Second option is correct

## Question 2

A support vector machine computes P(y|x)

1. True
2. False

> Second option is correct

## Question 3

Using the kernel  trick, one can get non-linear decision boundaries  using  algorithms designed originally for linear models

1. True
2. False

> First option is correct

## Question 4

Which of the following is not a good habit to avoid overfitting?

1. Using a two part cost function which includes a regularizer to penalize model complexity.
2. Discarding 50% of randomly chosen samples
3. Building a structure of nested subsets of models and train learning machines in each subset, starting from the inner subset, and stopping when the cross-validation error starts increasing.

> Second option is correct

## Question 5

What are support vectors:

1. The only examples necessary to compute f(x) in  an SVM.
2. The class centroids
3. The examples far from the decision boundary

> First option is correct

## Question 6

Which of the following can only be used when training data are linearlyseparable?

1. Linear hard-margin SVM
2. Linear Logistic Regression
3. Linear Soft margin SVM

> First option is correct

## Question 7

Which of the following might be valid reasons for preferring an SVM over a neural network?

1. An SVM can automatically learn to apply a non-linear transformation on the input space; a neural net cannot.
2. An SVM can effectively map the data to an infinite-dimensional space; a neural net cannot
3. The transformed (basis function) representation constructed by an SVM is usually easier to visualise/interpret than for a neural net.

> Second option is correct

## Question 8

Which of the following statement is incorrect about SVM?

1. A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane
2. A support vector machine (SVM) is a supervised machine learning model that uses classification algorithms for two-group classification problems
3. SVM produces a tree-based structure

> Third option is correct

## Question 9

Linear SVMs have no hyperparameters that need to be set by cross-validation.

1. True
2. False

> Second option correct

## Question 10

When the C parameter is set to infinite, which of the following holds true?

1. The optimal hyperplane if exists, will be the one that completely separates the data
2. The soft-margin classifier will separate the data 

> First option is correct

## Question 11

The SVMâ€™s are less effective when:

1. data is noisy 
2. data is linearly separable
3. daat is cleaned

> First option is correct

## Question 12

The main use of "kernel trick" is/are:

1. Used to perform Multi-class classification
2. Used to perform linear classification
3. Implicitly mapping the inputs into high-dimensional feature spaces

> Third option is correct

## Question 13

Which of the following statements is correct about SVM

1. SVM can be used for binary classifier
2. SVM doesn't support learning to rank
3. SVM can be used for multi-class classifier

> First option is correct

## Question 14

Which of the following statement is incorrect about Bagging?

1. Bagging reduces variance of the classifier
2. Bagging increases the variance of the classifier
3. Bagging can help make robust classifiers from unstable classifiers
4. Majority is one way of combining outputs from various classifiers which are being bagged

> Second option is correct

## Question 15

Which among the following prevents overfitting when we perform bagging?

1. The use of sampling with replacement as the sampling technique
2. The use of weak classifiers
3. The use of classification algorithms which are not prone to overfitting
4. The practice of validation performed on every classifier trained

> Second option is correct

## Question 16

Consider an alternative way of learning a Random Forest where instead of randomly sampling the attributes at each node, we sample a subset of attributes for each tree and build the tree on these features. Would you prefer this method over the original or not, and why?

1. Yes, because it reduces the correlation between the resultant trees
2. Yes, because it reduces the time taken to build the trees due to the decrease in the attributes considered
3. No, because many of the trees will be bad classifiers due to the absence of critical features considered in the construction of some of the trees

> Third option is correct

## Question 17

When the C parameter is set to infinite, which of the following holds true?

1. The optimal hyperplane if exists, will be the one that completely separates the data
2. The soft-margin classifier will separate the data

> First option is correct

## Question 18

Support vectors are the data points that lie closest to the decision surface.

1. True
2. False

> First option is correct

## Question 19

 Suppose you are dealing with 4 class classification problem and you want to train a SVM model on the data for that you are using One-vs-all method. How many times we need to train our SVM model in such case?

1. 2
2. 4
3. 6

> Second  option is correct

## Question 20

Data was collected on the weight of a male laboratory rat for the first 25 weeks after its birth. The linear regression equation is y= 40x+100, where x is number of weeks and y is weight in grams. What does the y-intercept mean in context of the problem?

1. The predicted weight of the rat in year 0.
2. The predicted weight of the rat at birth.
3. The current weight of the rat.
4. The average increase in the rat's weight.

> Second option is correct


