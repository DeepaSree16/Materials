# CFU

## Question 1

A decision tree is a __ used to help managers make decisions.

1. super
2. mathematical
3. scientific

> First option is correct

## Question 2

Decision tree usually underfit the data. Is this statement True or False?

1. True
2. False

> Second option is correct

## Question 3

In a decision tree, If the number of levels is too low i.e. the depth of the tree is too low, the model tends to overfit. Is this statement True or False?

1. True
2. False

> Second option is correct

## Question 4

When we remove sub-nodes of a decision node,You can say opposite process of splitting.

1. Pruning
2. Unsplitting
3. Removing

> First option is correct

## Question 5

In decision tree, first node is called __ node

1. root
2. terminal
3. leaf

> First option is correct

## Question 6

__ is a two-layer neural net that processes text by “vectorizing” words.

1. Bag of words
2. Word2vec
3. Glove

> Second option is correct

## Question 7

Vector representations of a particular word is called __

1. Word embedding
2. Feature vector
3. Numeric vector

> First option is correct

## Question 8

Word2vec uses __ neural network to learn word embeddings.

1. deep
2. shallow
3. one dimenssional

> Second option is correct

## Question 9

Word2vec is developed at __

1. Google
2. Facebook
3. Microsoft

> First option is correct

## Question 10

Which of the following word2vec method takes the context of each word as the input and tries to predict the word corresponding to the context?

1. Skip Gram
2. Common Bag of words
3. Visual Bag of words

> Second option is correct

## Question 11

Which of the following word2vec method uses the current word to predict the surrounding window of context words?

1. Skip Gram
2. Common Bag of words
3. Visual Bag of words

> First option is correct

## Question 12

A __ is a way to convert a piece of text in a numerical format that our machines can read.

1. Word Embedding
2. Count Vectorizer
3. Feature vector

> Second option is correct

## Question 13

Which of the following statement is incorrect about word2vec?

1. Predicts a word by using its neighbors, by learning dense vectors called embedding.
2. It is inefficient in computation.
3. It is a two-layer neural network model

> Second option is correct

## Question 14

Word embedding is also called as __

1. distributed semantic model
2. feature space
3. vector

>  First option is correct

## Question 15

Which of the following statements is incorrect about word2vec?

1. It ignores the context of words.
2. It is a shallow two-layered neural network. 
3. It represents words in vector space representation. 

> First option is correct

## Question 16

Which of the following method learns word by predicting its surrounding context?

1. Bag of words
2. Word2vec
3. Word Embeddings

> Second option is correct

## Question 17

Suppose you learn a word embedding for a vocabulary of 10000 words. Then the embedding vectors should be 10000 dimensional, so as to capture the full range of variation and meaning in those words.

1. True
2. False

> Second option is correct

## Question 18

Word2vec comes with __ flavours.

1. one
2. two
3. three

> Second option is correct

## Question 19

Which of the following statement is incorrect?

1. In CBOW model,  the distributed representations of surrounding words are combined to predict the word in the middle.
2. In  Skip-gram model, the distributed representation of the input word is used to predict the context.
3. CBOW method uses the current word to predict the surrounding window of context words

> Third option is correct

## Question 20

Which of the following statement is incorrect about Skip-gram method?

1. works well with a small amount of the training data, represents well even rare words or phrases.
2. uses the current word to predict the surrounding window of context words
3. Faster than CBOW model

> Third option is correct

## Question 21

__  is a hybrid of count based and window based model.

1. word2vec
2. bag of words
3. glove

> Third option is correct

## Question 22

__ is a technique  to improve the learning without compromising the quality of embedding

1. Negative Sampling
2. Positive Sampling
3. Sampling

> First option is correct

## Question 23

Which of the following statement is incorrect about CBOW word2vec architecture?

1. CBOW is trained to predict a single word from a fixed window size of context words, whereas Skip-gram does the opposite, and tries to predict several context words from a single input word.
2. CBOW tends to find the probability of a word occurring in a context. 
3. CBOW needs more data to be trained contains more knowledge about the context.

> Third option is correct

## Question 24

Which of the following statements is incorrect?

1. CBOW learns better syntactic relationships between words while Skip-gram is better in capturing better semantic relationships.
2. CBOW is trained to predict a single word from a fixed window size of context words, whereas Skip-gram does the opposite, and tries to predict several context words from a single input word. 
3. CBOW observes only the target word and a single word of the context in a single input/output tuple

> Third option is correct

## Question 25

Which of the following word2vec method is prone to overfit frequent words?

1. CBOW
2. Skip-gram

> First option is correct

## Question 26

When learning word embeddings, we create an artificial task of estimating P(target \mid context)P(target∣context). It is okay if we do poorly on this artificial prediction task; the more important by-product of this task is that we learn a useful set of word embeddings.

1. True
2. False

> Second option is correct

## Question 27

Can K-NN be used for regression?

1. True
2. False

## Question 28

Which of the following statement is incorrect about K-NN?

1. If the dataset is large, there will be a lot of processing which may adversely impact the performance of the algorithm.
2. If the dataset is small, there are chances of noise in the dataset which adversely affect the performance of KNN algorithm.
3. KNN works well with smaller dataset because it is a lazy learner. It needs to store all the data and then makes decision only at run time.

> Second option is correct

## Question 29

Which of the following is disadvantage of K-NN?

1. No Training period
2. Does not work well with high dimenssions
3. Easy to implement 

> Second option is correct

##  Question 30

Is K-nn sensitive to noisy data?

1. Yes
2. No
