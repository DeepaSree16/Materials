# CFP

## Question 1

The procedure of training each individual learner on different subsets of the data and then averaging the predictions is known as __________

1. Overfitting
2. Bagging
3. Boosting

> Second option is correct

## Question 2

Suppose we have our training data as [1, 2, 3, 4, 5, 6], then which of the following is not output after Bagging?

1. [1, 1, 1, 1, 1, 1]
2. [1, 2, 3, 4, 5, 6]
3. [1, 2, 0, 5, 6, 3]

> Third option is correct

## Question 3

Which of the following statements are true regarding Decision Trees?

1. Decision trees can only perform the classification tasks
2. Decision trees can only perform the regression tasks
3. Decision trees can perform both classification and regression tasks

> Third option is correct

## Question 4

---------------- combine several trees base algorithms to construct better predictive performance than a single tree base algorithm

1. Ensemble methods
2. Bagging
3. Decision Trees

> First option is correct

## Question 5

The ------------- approach is a bagging method where deep trees, fitted on bootstrap samples, are combined to produce an output with lower variance.

1. Boosting
2. Random forest
3. Stacking

> Second option  is correct

## Question 6

Bootstrap Aggregating is an ensemble method

1. TRUE
2. FALSE

> First option is correct

## Question 7

Ensemble methods helps improve machine learning results by combining multiple models

1. TRUE
2. FALSE

> First option is correct

## Question 8

--------------- is an ensemble method that creates a strong classifier from a number of weak classifiers

1. Bagging
2. Boosting
3. Stacking

> Second option is correct

## Question 9

--------------- is usually used to average the predictions of different models to get a better prediction

1. Boosting
2. Random Forest
3. Ensemble learning

> Third option is correct

## Question 10

------------ module is that it can be used for your part-of-speech tagging.

1. Scipy
2. NLTK 
3. Sklearn

> Second option is correct

## Question 11

Removing affixes from words and returning the root word is called ---------------

1. Stemming
2. Tolenisation
3. Lemmatization

>First option is correct

## Question 12

What makes random forests ‘ensemble’ classifiers?

1. The model’s prediction is the mean of the outputs of its constituent individual decision trees
2. The model’s prediction is the median of the outputs of its constituent individual decision trees
3. The model’s prediction is the mode of the outputs of its constituent individual decision trees

>Third option is correct

## Question 13

What is the idea behind ‘Bootstrapping’?

1. Randomly sample from the dataset with replacement
2. Randomly sample from the dataset without replacement

> First option is correct

## Question 14

Besides bagging, how can we ensure that the predictions made by the individual trees have low correlations with each other without compromising the input feature representation?

1. Adding extra features
2. Randomizing the features
3. Minimizing the features

> Second option is correct

## Question 15

Which of the following statements is true regarding Decision tree algorithms?

1. CART uses Gini Index and Entropy function as metrics
2. D3 uses Entropy function and Information gain as metrics
3. None of the above

>Second option is correct

## Question 16

What type of variables does the parameter max_depth in RandomForestClassifier() take? 

1. String
2. Float
3. Integer

> Third option is correct

## Question 17

The purpose of the inverse document frequency is to increase the weight of terms with high collection frequency.

1. TRUE
2. FALSE

> Second option is correct

## Question 18

What if we take all independent variables in each tree of random forest

1. It increase the accuracy but reduce testing accuracy
2. We may end up building dependent models
3. It increases the execution time, there will be no other change

> Second option is correct

## Question 19

In boosting algorithm, what happens when we increase the number of trees to really high value.

1. We may end up with an under fitted model
2. Boosting algorithm doesn't converge
3. We may end up with an overfitted model

> Third option is correct

## Question 20

Can random forest give us important variable as an output?

1. Yes
2. No, It is a black box method
3. Yes only for small datasets

> First option is correct

## Question 21

Word lemmatizing is similar to stemming, but the difference lies in the output.

1. True
2. False

> First option is correct

## Question 22

Ensemble methods can be used for supervised learning but not unsupervised learning.

1. True
2. False

> Second option is correct

## Question 23

What is bagging?

1. Take multiple boot strap samples combine them using union. Build one consolidated model
2. Take single boot strap sample and build multiple classifiers
3. Take multiple boot strap samples and build classifiers on each of them

> Third option is correct

## Question 24

What type of sampling is used in boosting models?

1. Bootstrapped samples in miss classified records
2. Weighted Samples for misclassified records
3. Simple random samples for misclassified records

> Second option is correct

## Question 25

Mark the correct statement according to Random Forest:

1. It outputs the class that is the mode of the class's output by individual Decision Trees
2. It outputs the class that is the mean of the class's output by individual Decision Trees
3. It outputs the class that is the median of the class's output by individual Decision Trees

> First option is correct

## Question 26

What are the random factors in Random Forest models?

1. The Samples & Features considered for building individual trees
2. The random initialization of estimates and random finalization of trees
3. The technique used in building multiple trees and variables used in each tree

> First option is correct

## Question 27

In boosting algorithm why do we do sampling again and again.

1. Re-sampling increases randomness in the variables
2. To find the right data set for building the final mode
3. We do re-sampling to give more weight to previously misclassified records

> Third option is correct

## Question 28

Random Forest models are special cases of:

1. K-Fold cross validation
2. Bagging
3. Decision Trees

> Second option is correct

## Question 29

While building ensemble models, we need to take care of two points:

1. All models are independent and every model should be moderately good
2. Models can be dependence and every model should be highly accurate
3. All models should have same accuracy and every model should have at least one independent variable

> First option is correct

## Question 30

The tf-idf weight is highest when a term t occurs many times within a small number of documents.

1. True
2. False

> First option is correct

