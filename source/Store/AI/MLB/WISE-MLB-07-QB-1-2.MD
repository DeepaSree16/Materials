CFP

## Question 1

Which of the are correct about False Negative?

1. The total number of inaccurate predictions that were “positive”
2. The total number of accurate predictions that were “negative”
3. The total number of inaccurate predictions that were “negative”

> Third option is correct

## Question 2

---------------  can be used when the data is imbalance

1. F1 score
2. ROC-AUC
3. Accuracy

> Second option is correct

## Question 3

True Positive Rate is defined as *TP / (FN+TP)*

1. TRUE
2. FALSE

> First option is correct

## Question 4

F1 Score is used to measure a test's accuracy

1. True
2. False

> First option is correct

## Question 5

-------------- is the average of the difference between the Original Values and the Predicted Values

1. Mean Squared Error
2. Mean Absolute Error
3. None of the abovee

> Second option is correct

## Question 6

An --------------- is a graph showing the performance of a classification model at all classification thresholds.

1. ROC Curve
2. Classification Threshold
3. AUC Curve

> First option is correct

## Question 7

Given one positive & one negative label data point, the probability that our model will classify those two points correctly is------------------

1. 100%
2. 20%
3. 80%

> Third option is correct

##  Question 8

F1 Score is the Harmonic Mean between precision and recall

1. True
2. False

> First option is correct

## Question 9

Which of the following are the examples of Performance Metrics? 

1. Productivity
2. Quality
3. Both A and B

> Third option is correct

## Question 10

Which of the following is correct about True Positives?

1. True positives are the cases when the actual class of the data point was 1(True) and the predicted is also 1(True)
2. True positives are the cases when the predicted class of the data point was 1(True) and the actual is also 1(True)
3. None of the above

> First option is correct

## Question 11

False Negative Rate is defined as number of items wrongly identified as negative out of total true positives- FN/(FN+TP)

1. True
2. First

> First option is correct

## Question 12

-------------- is defined as number of items correctly identified as positive out of total items identified as positive- TP/(TP+FP)

1. Accuracy
2. Precision
3. Recall

> Second option is correct

## Question 13

How many times do we validate a dataset with an assumed K value of 4?

1. 3
2. 4
3. 2

> First option is correct

## Question 14

------------- is a performance metric, based on varying threshold values, for classification problems.

1. F1-Score
2. AUC-ROC
3. Confusion Matrix

> Second option is correct

## Question 15

What is the formula for Accuracy?

1. (TP + TN) / (TP + FP + TN + FN)
2. (TN + TP) / (TP + FP + TP + FN)
3. (TP + TN)/(TP + TN + TN + FP)

> First option is correct

## Question 16

The ---------------- may be considered as a set of features which characterize the global variation

among face images

1. Word embedding
2. Feature vector
3. Eigen faces

> Third option is correct

## Question 17

------------ is a collective measure of Precision & Recall

1. Confusion Matrix
2. ROC-AUC
3. F1-Score

> Third option is correct

## Question 18

------------- is defined as the proportion of actual negative cases which are correctly identified.

1. Confusion Matrix
2. ROC-AUC
3. Specificity

> Third option is correct

## Question 19

RMSE is the most popular evaluation metric used in regression problems

1. True
2. False

> First option is correct

## Question 20

The most widely used metrics and tools to assess a classification model are:

1. Confusion matrix
2. Area under the ROC curve
3. Both 1 and 2

> Third option is correct

## Question 21

Which of the following measures of central tendency will always change if a single value in the data changes?

1. Mean
2. Mode
3. Median

> First option is correct

## Question 22

In ROC-AUC there is a probability threshold of 0.5 for a problem i.e if P(y)>0.5 it is marked positive else negative

1. True
2. False

> First option is correct

## Question 23

AUC is maximized when the True negatives(TN) are maximum for a given threshold.

1. True
2. False

> First option is correct

## Question 24

 --------------- is defined as number of items correctly identified as positive out of total true positives- TP/(TP+FN)

1. Specificity
2. Precision
3. Recall or Sensitivity

> Third option is correct

## Question 25

We know that there will be some error associated with every model that we use for predicting the true class of the target variable. This will result in False Positives and False Negatives

1. True
2. False

> First option is correct

## Question 26

For classification problems which of the following metrics is used:

1. Log-Loss
2. Accuracy
3. AUC
4. All the above

> Fourth option is correct

## Question 27

The advantage of MSE is:

1. Predictions were from actual output
2. the effect of larger errors become more
3. easier to compute the gradient

> Third option is correct

## Question 28

R Squared metric is generally used for explanatory purpose and provides an indication of the goodness or fit of a set of predicted output values to the actual output values.

1. True
2. False

> First option is correct

## Question 29

---------------- is the weighted average of the precision and recall

1. AUC
2. F1-Score
3. Accuracy

> Second option is correct

## Question 30

Which of the following value of k in k-NN would minimize the leave one out cross validation accuracy?

1. 3
2. 5
3. None of the above

> Second option is correct

