# CFP

## Question 1

With respect to KNN, an outlier sample is:

### Options

A. the one with no nearest neighbour
B. the one with very distant nearest neighbours
C. the one with few nearest neighbours

Answer: B

## Question 2

In a two class classification (0/1) problem with a 60:40 split of classes (0:1) among test samples, what is the expected accuracy in percent if we Predict 0 for 60% of randomly selected test samples and 1 for the rest:

### Options

A. 60
B. 50
C. 52 

Answer: C

## Question 3

Word2Vec is best described as:

### Options

A. An algorithm to figure out missing parts of a sentence
B. A representation of words and their meaning and context 
C. A data structure that represents a group of words

Answer: B

## Question 4

kNN is a poor choice if there are severe memory constraints.

### Options

A. TRUE
B. FALSE

Answer: A

## Question 5

 When creating a Decision Tree, can we reuse a feature to split a node?

### Options

A. TRUE
B. FALSE

**Answer:** A 

## Question 6

Overfitting can be avoided by:

### Options

A. Building a less complicated and simpler model
B. By removing some less impacting variables
C. By validating and finetuning the model
D. All the above can be used for avoiding overfitting 

**Answer:** D

## Question 7

------------ is a very popular method for word embedding

### Options

A. Word2Vec
B. Bag of Words
C. Feature Selection

Answer: A

## Question 8

TF-IDF helps to establish?

### Options

A. most important word in the document
B. most frequently occurring word in the document
C. None of the above

Answer: A

## Question 9

In Word2Vec how to use number vector to represent a word?

### Options

A. N-grams
B. Word Embedding
C. One-Hot Encoding

Answer: C

## Question 10

-------------------- method takes the context of each word as the input and tries to predict the word corresponding to the context.

### Options

A. Skip-gram model
B. Continuous Bag of Words model
C. Word2Vec

Answer: B

## Question 11

Which of the following may be a poor choice for the kNN algorithm?

### Options

A. 3
B. 4
C. 5

Answer: B

## Question 12

The objective of ----------------- is to predict the context of central words.

### Options

A. Skip-gram model
B. Continuous Bag of Words model
C. Bag of Words

Answer: A

## Question 13

Word embeddings capture multiple dimensions of data and are represented as vectors.

### Options

A. TRUE
B. FALSE

Answer: A

## Question 14

Identify the sentence that best describes Validation dataset:

### Options

A. It is part of the testing dataset
B. It is used to tune the hyper-parameters
C. It is used to verify the hyper-parameters

Answer: B

## Question 15

Why do we do pruning in decision trees?

### Options

A. Pruning is done when the input data sample size is really large
B. To increase the size of the tree
C. To reduce the size of the tree and to avoid overfitting

**Answer:** C

## Question 16

---------------- is an algorithm that accepts text corpus as an input and outputs a vector representation for each word

### Options

A. Bag of Words
B. Gradient Descent
C. Word2Vec

Answer: C

## Question 17

Which of the following is correct for Word2Vec model?

### Options

A. The architecture of word2vec consists of only two layers – continuous bag of words and skip-gram model
B. Both CBOW and Skip-gram are shallow neural network models
C. Both A and B

Answer: B

## Question 18

------------------ is a measure of similarity, which makes it possible to relate not only words, but also sentences and whole texts

### Options

A. Word Embedding
B. Term Frequency
C. Skip-gram model

Answer: A

## Question 19

In NLP, the algorithm decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents

### Options

A. Inverse Document Frequency (IDF)
B. Term Frequency (TF)
C. Word2Vec

Answer: A

## Question 20

Dissimilarity between words expressed using cosine similarity will have values significantly higher than 0.5

### Options

A. TRUE
B. FALSE

Answer: A

## Question 21

Which of the following are applications of Word Embedding?

### Options

A. Document Clustering
B. Natural Language Processing
C. Sentiment Analysis
D. All the above

Answer: D

## Question 22

 Mark the Correct Statement:

### Options

A. K Nearest Neighbours – Samples that have distances smaller than K, while Epsilon Neighbours – Epsilon number of samples that are closest to you.
B. K Nearest Neighbours – Samples that have distances smaller than K, while Epsilon Neighbours – Samples that have distances smaller than epsilon.
C. K Nearest Neighbours – K number of samples that are closest to you, while Epsilon Neighbours – Samples that have distances smaller than epsilon.

Answer: C

## Question 23

 What is the computational complexity of computing Euclidean distance between two vectors of d dimensions?

### Options

A. O(sqrt(d))
B. O(d) 
C. O(d * (log d))

Answer: B

## Question 24

----------------- is a language modeling technique used for mapping words to vectors of real numbers

### Options

A. Word Embedding
B. Count Vectorizer
C. Term Frequency

Answer: A

## Question 25

Which of the following are disadvantages of KNN?

### Options

A. Does not work well with high dimensions
B. No Training Period
C. New data can be added seamlessly

Answer: A

## Question 26

As the model complexity increases, models tend to overfit; what happens to training and testing error:

### Options

A. Training error reduces and testing error reduces
B. Both errors increase gradually
C. Training error always reduces testing error reduces and increases after a point of complexity

Answer: C

## Question 27

Which of the following is correct about continuous bag of words?

### Options

A. It is faster and has better representations for more frequent words.
B. It works well with small amount of data
C. None of the above

Answer: A

## Question 28

Which of the following is correct about Skip Gram model?

### Options

A. It is faster and has better representations for more frequent words.
B. It works well with small amount of data and is found to represent rare words well
C. It takes the context of each word as the input and tries to predict the word corresponding to the context

Answer: B

## Question 29

The cosine similarity of two documents will range from:

### Options

A. 0 to 1
B. -1 to +1
C. 1 to Infinity

Answer: A

## Question 30

What metric is used in splitting the nodes in decision tree algorithm?

### Options

A. Euclidean distance
B. P-Value split metric
C. Information gain

**Answer:** C