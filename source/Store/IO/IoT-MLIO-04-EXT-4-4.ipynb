{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZbO1TP3GWLl"
   },
   "source": [
    "# IoT & Smart Analytics\n",
    "## A Program by IIIT-H and TalentSprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssvaS0BoGWLp"
   },
   "source": [
    "## Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zC3yBYSkGWLp"
   },
   "source": [
    "At the end of the experiment, you will be able to\n",
    "\n",
    "* distinguish between Feed Forward (fully connected) neural networks and Convolutional neural networks\n",
    "* understand terms like filtering, convolution, pooling\n",
    "* build CNN model to tackle fashion mnist dataset\n",
    "* build CNN model to tackle digit mnist dataset\n",
    "* know various CNN architectures: AlexNet \n",
    "* build AlexNet architecture using Keras Sequential API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyJONJ0LrSwD"
   },
   "source": [
    "### Setup Steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PVJ6ovMrYiM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2YzfoPvJDiTX"
   },
   "outputs": [],
   "source": [
    "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
    "Id = \"P22I01E_test\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEzlYL4CDrmE"
   },
   "outputs": [],
   "source": [
    "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
    "password = \"9876543456\" #@param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WBPPuGmBlDIN",
    "outputId": "5944d4b8-d7c8-41ff-b188-e6cebf8633d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=P22I01E_test&recordId=113\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup completed successfully\n"
     ]
    }
   ],
   "source": [
    "#@title Run this cell to complete the setup for this Notebook\n",
    "from IPython import get_ipython\n",
    "\n",
    "ipython = get_ipython()\n",
    "  \n",
    "notebook= \"CNN\" #name of the notebook\n",
    "\n",
    "def setup():\n",
    "    from IPython.display import HTML, display\n",
    "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
    "    print(\"Setup completed successfully\")\n",
    "    return\n",
    "\n",
    "def submit_notebook():\n",
    "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
    "    \n",
    "    import requests, json, base64, datetime\n",
    "\n",
    "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
    "    if not submission_id:\n",
    "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
    "      r = requests.post(url, data = data)\n",
    "      r = json.loads(r.text)\n",
    "\n",
    "      if r[\"status\"] == \"Success\":\n",
    "          return r[\"record_id\"]\n",
    "      elif \"err\" in r:        \n",
    "        print(r[\"err\"])\n",
    "        return None        \n",
    "      else:\n",
    "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
    "        return None\n",
    "    \n",
    "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
    "      f = open(notebook + \".ipynb\", \"rb\")\n",
    "      file_hash = base64.b64encode(f.read())\n",
    "\n",
    "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n",
    "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n",
    "              \"answer1\" : Answer1, \"id\" : Id, \"file_hash\" : file_hash,\n",
    "              \"notebook\" : notebook, \"answer2\" : Answer2,\n",
    "              \"feedback_experiments_input\" : Comments,\n",
    "              \"feedback_mentor_support\": Mentor_support}\n",
    "\n",
    "      r = requests.post(url, data = data)\n",
    "      r = json.loads(r.text)\n",
    "      if \"err\" in r:        \n",
    "        print(r[\"err\"])\n",
    "        return None   \n",
    "      else:\n",
    "        print(\"Your submission is successful.\")\n",
    "        print(\"Ref Id:\", submission_id)\n",
    "        print(\"Date of submission: \", r[\"date\"])\n",
    "        print(\"Time of submission: \", r[\"time\"])\n",
    "        print(\"View your submissions: https://iot.iiith.talentsprint.com/notebook_submissions\")\n",
    "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
    "        return submission_id\n",
    "    else: submission_id\n",
    "    \n",
    "\n",
    "def getAdditional():\n",
    "  try:\n",
    "    if not Additional: \n",
    "      raise NameError\n",
    "    else:\n",
    "      return Additional  \n",
    "  except NameError:\n",
    "    print (\"Please answer Additional Question\")\n",
    "    return None\n",
    "\n",
    "def getComplexity():\n",
    "  try:\n",
    "    if not Complexity:\n",
    "      raise NameError\n",
    "    else:\n",
    "      return Complexity\n",
    "  except NameError:\n",
    "    print (\"Please answer Complexity Question\")\n",
    "    return None\n",
    "  \n",
    "def getConcepts():\n",
    "  try:\n",
    "    if not Concepts:\n",
    "      raise NameError\n",
    "    else:\n",
    "      return Concepts\n",
    "  except NameError:\n",
    "    print (\"Please answer Concepts Question\")\n",
    "    return None\n",
    "  \n",
    "  \n",
    "def getComments():\n",
    "  try:\n",
    "    if not Comments:\n",
    "      raise NameError\n",
    "    else:\n",
    "      return Comments\n",
    "  except NameError:\n",
    "    print (\"Please answer Comments Question\")\n",
    "    return None\n",
    "  \n",
    "\n",
    "def getMentorSupport():\n",
    "  try:\n",
    "    if not Mentor_support:\n",
    "      raise NameError\n",
    "    else:\n",
    "      return Mentor_support\n",
    "  except NameError:\n",
    "    print (\"Please answer Mentor support Question\")\n",
    "    return None\n",
    "\n",
    "def getAnswer1():\n",
    "  try:\n",
    "    if not Answer1:\n",
    "      raise NameError \n",
    "    else: \n",
    "      return Answer1\n",
    "  except NameError:\n",
    "    print (\"Please answer1 Question\")\n",
    "    return None\n",
    "\n",
    "def getAnswer2():\n",
    "  try:\n",
    "    if not Answer2:\n",
    "      raise NameError \n",
    "    else: \n",
    "      return Answer2\n",
    "  except NameError:\n",
    "    print (\"Please answer2 Question\")\n",
    "    return None\n",
    "  \n",
    "\n",
    "def getId():\n",
    "  try: \n",
    "    return Id if Id else None\n",
    "  except NameError:\n",
    "    return None\n",
    "\n",
    "def getPassword():\n",
    "  try:\n",
    "    return password if password else None\n",
    "  except NameError:\n",
    "    return None\n",
    "\n",
    "submission_id = None\n",
    "### Setup \n",
    "if getPassword() and getId():\n",
    "  submission_id = submit_notebook()\n",
    "  if submission_id:\n",
    "    setup() \n",
    "else:\n",
    "  print (\"Please complete Id and Password cells before running setup\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wDGpB4AGWLq"
   },
   "source": [
    "## Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWVZa0E6GWLq"
   },
   "source": [
    "The idea of the CNN was derived from the biological process of how the visual cortex is structured and works. \n",
    "\n",
    "As per the study of David H. and Torsten Wheel,\n",
    " \n",
    "* neurons in the visual cortex have a local **receptive field** that means neurons will respond to stimuli only in the restricted region, and \n",
    "\n",
    "* receptive fields of all neurons combine to make the whole visual image. \n",
    "\n",
    "The above study inspired the paper Neocognitron in 1980 and which later evolved into Convolutional Neural networks (CNN).\n",
    "\n",
    "The most important building block of a CNN is the convolutional layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7Iax1yyGWLq"
   },
   "source": [
    "### Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_z3LXJ26GWLr"
   },
   "source": [
    "Neurons in the first convolutional layer are not connected to every single pixel in the input image (like they were in the layers of DNN), but only to pixels in their receptive fields as shown in the figure below. In turn, each neuron in the second convolutional layer is connected only to neurons located within a receptive field in the first layer.\n",
    "\n",
    "This architecture allows the network to concentrate on small low-level features in the first hidden layer, then assemble them into larger higher-level features in the next hidden layer, and so on.\n",
    "<center>\n",
    "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/conv_layers_multiple_fmap.png\" width=600px/>\n",
    "</center>\n",
    "$\\hspace{6cm} \\text {Convolutional layers with multiple feature maps, and images with three color channels}$\n",
    "<br><br>\n",
    "\n",
    "A neuron’s weights can be represented as a small image with the size of the receptive field. This is termed as **filter**. And, a layer full of neurons using the same filter outputs a **feature map**, which highlights the areas in an image that activate the filter the most.\n",
    "\n",
    "**A convolutional layer has multiple filters and outputs one feature map per filter**, also:\n",
    "\n",
    "* it has one neuron per pixel in each feature map, and all neurons within a given feature map share the same parameters (i.e., the same weights and bias term). \n",
    "\n",
    "* neurons in different feature maps use different parameters. \n",
    "\n",
    "* a neuron’s receptive field extends across all the previous layers’ feature maps i.e, its shape will be $f_h$ x $f_w$ x depth of previous layer, where $f_h$ and $f_w$ are the height and width of the receptive field. \n",
    "\n",
    "In short, a convolutional layer simultaneously applies multiple trainable filters to its inputs, making it capable of detecting multiple features anywhere in its inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FFyAL26MGWLr"
   },
   "source": [
    "For instance, if the input image and the filter look like the following:\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/CNN_Initial.png\" width=400px/>\n",
    "</center>\n",
    "\n",
    "$\\hspace{10.5cm} \\text {Input}   \\hspace{4cm} \\text {Kernel/Filter}$                               \n",
    "\n",
    "The filter (green) slides over the input image (blue) one pixel at a time starting from the top left. The filter multiplies its own values with the overlapping values of the image while sliding over it and adds all of them up to output a single value for each overlap until the entire image is traversed:\n",
    "<br><br>\n",
    "<center>\n",
    "\n",
    "![](https://cdn.extras.talentsprint.com/IOT/Images/CNN_Ani.gif)\n",
    "\n",
    "\n",
    "\n",
    "</center>\n",
    "\n",
    "In the case of images with multiple channels (e.g. RGB), the Kernel has the same depth as that of the input image. Convolution is performed between $K_n$ and $I_n$ stack ([$K_1,I_1$], [$K_2,I_2$], [$K_3,I_3$]) and all the results are summed with the bias to give us a squashed one-depth channel Convoluted Feature Output:\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/3Channel_CNN_Ani.gif\" />\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hdv-4oBe29PV"
   },
   "source": [
    "### **Single filter/kernel having two channel**\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/conv-multi-in.png\" />\n",
    "</center>\n",
    "\n",
    "### **Two filter/kernel having three channel each**\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/conv-1x1.png\" />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj4ArdG0GWLs"
   },
   "source": [
    "**Note that:** \n",
    "\n",
    "* all neurons located in the same row $i$ and column $j$ but in different feature maps are connected to the outputs of the exact same neurons in the previous layer.\n",
    "\n",
    "* we do not have to define the filters manually: instead, during training, the convolutional layer will automatically learn the most useful filters for its task, and the layers above will learn to combine them into more complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zV9JjuNrGWLs"
   },
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gL4hjwyJGWLt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten, BatchNormalization, InputLayer, Conv1D, ReLU, GlobalAveragePooling1D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feZFBVDmGWLu"
   },
   "source": [
    "#### Using a TensorFlow implementation, we will now understand convolutional operations on images\n",
    "\n",
    "* Applying a convolutional filter\n",
    "* Feature Map generation\n",
    "* Stride and Padding\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ysxxzh6RGWLv"
   },
   "source": [
    "In TensorFlow, \n",
    "\n",
    "* each input image is typically represented as a 3D tensor of shape\n",
    "[height, width, channels]. \n",
    "\n",
    "* A mini-batch is represented as a 4D tensor of shape [minibatch size, height, width, channels]. \n",
    "\n",
    "* The weights of a convolutional layer are represented as a 4D tensor of shape [$f_h$, $f_w$, $f_{n′}$, $f_n$]. \n",
    "\n",
    "* The bias terms of a convolutional layer are simply represented as a 1D tensor of shape [$f_n$].\n",
    "\n",
    "where \n",
    "\n",
    "$f_h$ is filter (or receptive field) height, \n",
    "\n",
    "$f_w$ is filter width, \n",
    "\n",
    "$f_{n′}$ is number of feature maps in the previous layer (layer $l – 1$), and \n",
    "\n",
    "$f_n$ is number of feature maps in the current layer (layer $l$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGi23P1BGWLv"
   },
   "source": [
    "Let's creates two filters and apply them to any two MNIST digit  images.\n",
    "\n",
    "In the following code we use Scikit-Learn’s `fetch_openml()` (which loads digit MNIST dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXCYE8bXGy8_"
   },
   "source": [
    "#### Loading Data Digit MNIST Dataset\n",
    "Using SciKit-Learns ```fetch_openml``` to load MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8hvfwKNGy9F"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9Yk2RcaHMsH"
   },
   "source": [
    "Understanding the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jZkaS7-2HMsM",
    "outputId": "5c1a77c3-3e6d-4250-993e-6b3eb464453d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape : (70000, 784) \n",
      "\n",
      "Keys :  dict_keys(['data', 'target', 'frame', 'categories', 'feature_names', 'target_names', 'DESCR', 'details', 'url']) \n",
      "\n",
      "Description : **Author**: Yann LeCun, Corinna Cortes, Christopher J.C. Burges  \n",
      "**Source**: [MNIST Website](http://yann.lecun.com/exdb/mnist/) - Date unknown  \n",
      "**Please cite**:  \n",
      "\n",
      "The MNIST database of handwritten digits with 784 features, raw data available at: http://yann.lecun.com/exdb/mnist/. It can be split in a training set of the first 60,000 examples, and a test set of 10,000 examples  \n",
      "\n",
      "It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image. It is a good database for people who want to try learning techniques and pattern recognition methods on real-world data while spending minimal efforts on preprocessing and formatting. The original black and white (bilevel) images from NIST were size normalized to fit in a 20x20 pixel box while preserving their aspect ratio. The resulting images contain grey levels as a result of the anti-aliasing technique used by the normalization algorithm. the images were centered in a 28x28 image by computing the center of mass of the pixels, and translating the image so as to position this point at the center of the 28x28 field.  \n",
      "\n",
      "With some classification methods (particularly template-based methods, such as SVM and K-nearest neighbors), the error rate improves when the digits are centered by bounding box rather than center of mass. If you do this kind of pre-processing, you should report it in your publications. The MNIST database was constructed from NIST's NIST originally designated SD-3 as their training set and SD-1 as their test set. However, SD-3 is much cleaner and easier to recognize than SD-1. The reason for this can be found on the fact that SD-3 was collected among Census Bureau employees, while SD-1 was collected among high-school students. Drawing sensible conclusions from learning experiments requires that the result be independent of the choice of training set and test among the complete set of samples. Therefore it was necessary to build a new database by mixing NIST's datasets.  \n",
      "\n",
      "The MNIST training set is composed of 30,000 patterns from SD-3 and 30,000 patterns from SD-1. Our test set was composed of 5,000 patterns from SD-3 and 5,000 patterns from SD-1. The 60,000 pattern training set contained examples from approximately 250 writers. We made sure that the sets of writers of the training set and test set were disjoint. SD-1 contains 58,527 digit images written by 500 different writers. In contrast to SD-3, where blocks of data from each writer appeared in sequence, the data in SD-1 is scrambled. Writer identities for SD-1 is available and we used this information to unscramble the writers. We then split SD-1 in two: characters written by the first 250 writers went into our new training set. The remaining 250 writers were placed in our test set. Thus we had two sets with nearly 30,000 examples each. The new training set was completed with enough examples from SD-3, starting at pattern # 0, to make a full set of 60,000 training patterns. Similarly, the new test set was completed with SD-3 examples starting at pattern # 35,000 to make a full set with 60,000 test patterns. Only a subset of 10,000 test images (5,000 from SD-1 and 5,000 from SD-3) is available on this site. The full 60,000 sample training set is available.\n",
      "\n",
      "Downloaded from openml.org.\n"
     ]
    }
   ],
   "source": [
    "print('Data Shape :',mnist.data.shape,'\\n')\n",
    "print('Keys : ',mnist.keys(),'\\n')\n",
    "print('Description :', mnist.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8fR9ePvOHMsQ"
   },
   "source": [
    "Separating features and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "abULopVUHMsW",
    "outputId": "8669ab66-dcc7-4b25-b2eb-1dc905444bbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((70000, 784), dtype('float64'))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X, y) = mnist[\"data\"].values, mnist[\"target\"].values\n",
    "# Shape and datatype of X_train_full\n",
    "X.shape, X.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQUmV9rmHMsZ"
   },
   "source": [
    "Converting label from string to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YRsjBCtRHMsc",
    "outputId": "0d413850-19c7-4a5c-bd07-e6a0445036a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "print(type(y[90]))\n",
    "y = y.astype(np.uint8)\n",
    "print(type(y[90]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmLk3cuhCzKa"
   },
   "outputs": [],
   "source": [
    "sample_digit = X[12] # Check the result by putting different values in square bracket \n",
    "sample_digit_image = sample_digit.reshape(28, 28)\n",
    "plt.imshow(sample_digit_image, cmap=mpl.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6fGbNxbCC_bt"
   },
   "outputs": [],
   "source": [
    "sample_digit1 = X[2]\n",
    "sample_digit2 = X[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kpGoOeI2DyeR"
   },
   "outputs": [],
   "source": [
    "sample_digit1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uexJKiHoGWLw"
   },
   "outputs": [],
   "source": [
    "# Scale image features\n",
    "sample_digit1 =sample_digit1.reshape( 28, 28) / 255\n",
    "sample_digit2 = sample_digit2.reshape(28, 28) / 255\n",
    "\n",
    "# Visualize images\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].imshow(sample_digit1, cmap=mpl.cm.binary)\n",
    "ax[1].imshow(sample_digit2, cmap=mpl.cm.binary)\n",
    "plt.show()\n",
    "sample_digit1.shape, sample_digit2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fTNPRQfQGWLw"
   },
   "outputs": [],
   "source": [
    "# Combine images as single 4D array\n",
    "images = np.array([sample_digit1, sample_digit2])[:,:,:,np.newaxis]\n",
    "batch_size, height, width, channels = images.shape\n",
    "batch_size, height, width, channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LY15X3qFxZDt"
   },
   "source": [
    "#### **Creating filters for horizontal and vertical edges**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmuVHfc4GWLx"
   },
   "source": [
    "Create two 7 × 7 filters (one with a vertical white line in the middle, and the other with a horizontal white line in the middle).\n",
    "\n",
    "Note that shape should be [$f_h$, $f_w$, $f_{n′}$, $f_n$]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UQNcmmIyGWLx",
    "outputId": "413aef9a-24b9-4ae0-caa9-bd4c9bc02b31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7, 1, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create 2 filters\n",
    "filters = np.zeros(shape=(7, 7, channels, 2), dtype=np.float32)\n",
    "# vertical line\n",
    "filters[:, 3, :, 0] = 1 \n",
    "# horizontal line\n",
    "filters[3, :, :, 1] = 1\n",
    "filters.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bndqX8B_yRYL"
   },
   "outputs": [],
   "source": [
    "filters[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oapZRHEszS0f"
   },
   "outputs": [],
   "source": [
    "filters[:,:,:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGl6HLzBGWLx"
   },
   "source": [
    "Apply the filters to both images using the `tf.nn.conv2d()` function, which is part of TensorFlow’s low-level Deep Learning API. \n",
    "\n",
    "Here, we use zero padding (`padding=\"SAME\"`) and a stride of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FDtI9fdjGWLx"
   },
   "outputs": [],
   "source": [
    "# Convolutional layer\n",
    "outputs = tf.nn.conv2d(images, filters, strides=1, padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PBnurx7czhdg",
    "outputId": "fc009611-23d8-44ad-8826-f25acd16f5ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 28, 28, 2])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b5MgZldBGWLx"
   },
   "outputs": [],
   "source": [
    "# Images' 1st feature map using vertical filter\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].imshow(outputs[0, :, :, 0], cmap=\"gray\")\n",
    "ax[1].imshow(outputs[1, :, :, 0], cmap=\"gray\")\n",
    "print(\"1st feature map through verticle filter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBTXOBz7GWLx"
   },
   "outputs": [],
   "source": [
    "# Images' 2nd feature map using horizontal filter\n",
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "ax[0].imshow(outputs[0, :, :, 1], cmap=\"gray\")\n",
    "ax[1].imshow(outputs[1, :, :, 1], cmap=\"gray\")\n",
    "print(\"2nd feature map through horizontal filter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TsXxF2G2GWLy"
   },
   "source": [
    "In `tf.nn.conv2d()`:\n",
    "\n",
    "* **images** is the input mini-batch (a 4D tensor).\n",
    "\n",
    "* **filters** is the set of filters to apply (also a 4D tensor).\n",
    "\n",
    "* **strides** is equal to 1, but it could also be a 1D array with four elements, where the two central elements are the vertical and horizontal strides ($s_h$ and $s_w$). The first and last elements must currently be equal to 1. They may one day be used to specify a batch stride (to skip some instances) and a channel stride (to skip some of the previous layer’s feature maps or channels).\n",
    "\n",
    "* **padding** must be either \"SAME\" or \"VALID\":\n",
    "\n",
    "  * If set to \"SAME\", the convolutional layer uses zero padding if necessary. The output size is set to the number of input neurons divided by the stride, rounded up. For example, if the input size is 13 and the stride is 5 as shown in the figure below, then the output size is 3 (i.e., 13 / 5 = 2.6, rounded up to 3). Then zeros are added as evenly as possible around the inputs, as needed.\n",
    "\n",
    "  * If set to \"VALID\", the convolutional layer does not use zero padding and may\n",
    "ignore some rows and columns at the bottom and right of the input image,\n",
    "depending on the stride. This means that every neuron’s receptive field lies strictly within valid positions inside the input, hence the name valid.\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://wizardforcel.gitbooks.io/scikit-and-tensorflow-workbooks-bjpcjp/content/pics/padding-options.png\" width=450px/>\n",
    "</center>\n",
    "$\\hspace{9.5cm} \\text {Different padding options}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSqkxdVAFI2N"
   },
   "source": [
    " #### **Zero Padding**\n",
    " <br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/zero_padding.png\" />\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZh1eecQGWLy"
   },
   "source": [
    "**In the above example, we manually defined the filters, but in a real CNN we would normally define filters as trainable variables so the neural net can learn which filters work best. Instead of manually creating the variables,** use the `keras.layers.Conv2D` layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yk5qBb4uGWLy"
   },
   "outputs": [],
   "source": [
    "conv = keras.layers.Conv2D(filters=32, kernel_size=3, strides=1, padding=\"same\", activation=\"relu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amu40MjkFvW5"
   },
   "source": [
    "### Deepaer look into One Layer of a CNN\n",
    "\n",
    "\n",
    " <br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/One_Layer_CNN.png\" width=750\n",
    "px/>\n",
    "</center>\n",
    "\n",
    "<center>\n",
    "\n",
    "**Equations** \n",
    "\n",
    "~--------------------------------------~\n",
    "\n",
    "$Z^{[1]}=W^{[1]} \\circledast A^{[0]} + b^{[1]}$\n",
    "\n",
    "$A^{[1]}=ReLu(Z^{[1]})$\n",
    "\n",
    "~--------------------------------------~\n",
    "\n",
    " **Shape of output :**\n",
    "\n",
    "$$\\frac{n+2p-f}{s} +1$$\n",
    "\n",
    "~-------------------------------------~\n",
    "\n",
    "**Example :** In this case: n=6, f=3, p=0, s=1\n",
    "\n",
    "$$\\frac{6+2*0-3}{1} +1=4$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXZD3hEhGWLy"
   },
   "source": [
    "As we can see, convolutional layers have quite a few hyperparameters and we can use cross-validation to find the right values, but this is very time-consuming.\n",
    "\n",
    "Also, CNNs' convolutional layers require a huge amount of RAM. This problem is taken care of by the second common building block of CNNs: the pooling layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fcAQ-t5pGWLz"
   },
   "source": [
    "### **Pooling Layers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIp2wjP6GWLz"
   },
   "source": [
    "The goal of the pooling layer is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters. Each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a receptive field. \n",
    "\n",
    "However, **a pooling neuron has no weights**; all it does is aggregate the inputs using an aggregation function such as the **max** or **mean/average**. \n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/Max_Pool.png\" width=500px/>\n",
    "</center>\n",
    "<br><br>\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/Average_Pool.png\" width=500px/>\n",
    "</center>\n",
    "<br><br>\n",
    "\n",
    "The below figure shows a max pooling layer, with a 2 × 2 pooling kernel, stride of 2 and no padding. Only the max input value in each receptive field makes it to the next layer. Because of the stride of 2, the output image has half the size of the input image.\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/Pooling_layer.png\" width=600px/>\n",
    "</center>\n",
    "$\\hspace{7cm} \\text {Max pooling layer (2x2 pooling kernel, stride 2, no padding)}$\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5NgBVoNGWLz"
   },
   "source": [
    "#### Using a TensorFlow implementation, we will now understand pooling operation on images\n",
    "\n",
    "* Applying a pooling layer\n",
    "* Image compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hstKAFXtGWLz"
   },
   "source": [
    "The following code creates a max pooling layer using a 2 × 2 kernel and applies it to `images`. The strides default to the kernel size, so this layer will use a stride of 2 (both horizontally and vertically):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LNx9eeWMGWLz"
   },
   "outputs": [],
   "source": [
    "# Pooling layer\n",
    "max_pool = keras.layers.MaxPool2D(pool_size=2, padding=\"valid\", dtype='float64')(images)\n",
    "max_pool.shape, max_pool[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3rO_s79qGWL0"
   },
   "source": [
    "To create an average pooling layer, just use AvgPool2D instead of MaxPool2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AiO1GfIbGWL0"
   },
   "outputs": [],
   "source": [
    "# Images before and after pooling operation, showing only single channel\n",
    "fig, ax = plt.subplots(2,2, figsize=(10,6))\n",
    "\n",
    "ax[0][0].imshow(images[0, :, :, :].reshape(28, 28), cmap=\"gray\")\n",
    "ax[0][1].imshow(images[1, :, :, :].reshape(28, 28), cmap=\"gray\")\n",
    "\n",
    "ax[1][0].imshow(max_pool[0].numpy().reshape(14,14), cmap=\"gray\")\n",
    "ax[1][1].imshow(max_pool[1].numpy().reshape(14,14), cmap=\"gray\")\n",
    "\n",
    "print(\"Images before and after pooling operation:\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVFraiWNGWL1",
    "outputId": "09623521-fe73-48e6-fe93-f7574e0e26c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The compression ratio between the original images size and the total size after pooling is: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Compression ratio\n",
    "original_bytes = images.nbytes\n",
    "pooling_bytes = np.array(max_pool).nbytes\n",
    "ratio = pooling_bytes / original_bytes\n",
    "print(\"The compression ratio between the original images size and the total size after pooling is:\", ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAbHLiAzGWL1"
   },
   "source": [
    "From the above results, we can see that the images are shrunk.\n",
    "\n",
    "Now we know all the building blocks to create convolutional neural networks.\n",
    "Let's  understand a typical CNN architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wACRVHSnGWL1"
   },
   "source": [
    "### **CNN Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOk9Z4MdGWL1"
   },
   "source": [
    "Typical CNN architectures stack a few convolutional layers (generally followed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also gets deeper and deeper (i.e., with more feature maps), as shown in the figure below. At the top of the stack, a regular feedforward neural network is added, composed of a few fully connected layers (+ReLUs), and the final layer outputs the prediction.\n",
    "\n",
    "<br><br>\n",
    "<center>\n",
    "<img src=\"https://i1.wp.com/thecleverprogrammer.com/wp-content/uploads/2020/11/1-cnnlayer.png?resize=1024%2C259&ssl=1\" width=600px/>\n",
    "</center>\n",
    "\n",
    "$\\hspace{11cm} \\text {CNN architecture}$\n",
    "<br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtaM_hgA6-n3"
   },
   "source": [
    "### **CNN Example**\n",
    "\n",
    "Have a look into the detailing of each step involved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JMEIf-m1W8HP"
   },
   "source": [
    " <br><br>\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/CNN_Ex1.png\" width=600 px/>\n",
    "\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/CNN_Ex2.png\" width=600 px/>\n",
    "\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/CNN_Ex3.png\" \n",
    "width=600 px/>\n",
    "\n",
    "**Parameters Calculation of each layer**\n",
    "\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/CNN_Parameters.png\" width=800 px/>\n",
    "\n",
    "\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umwjk70LW8a9"
   },
   "source": [
    "#### **CNN on Fashion MNIST dataset**\n",
    "Here is how we can implement a simple CNN to tackle the Fashion MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-s7j23vGWL1"
   },
   "outputs": [],
   "source": [
    "# Using Keras to load the dataset\n",
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eoG8ubBGWL2"
   },
   "outputs": [],
   "source": [
    "# Visualize an image from data\n",
    "print(\"Label: \", y_train_full[0])\n",
    "plt.imshow(X_train_full[0], cmap='Greys')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ATde942GWL2"
   },
   "outputs": [],
   "source": [
    "# Shape and datatype of X_train\n",
    "X_train_full.shape, X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxHft9mdGWL2"
   },
   "outputs": [],
   "source": [
    "# Split into training and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=123)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P0v9OOixGWL2"
   },
   "outputs": [],
   "source": [
    "# Reshape train, test, and validation data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5tHXN4rGWL3"
   },
   "source": [
    "<center>\n",
    "<img src=\"https://cdn.iisc.talentsprint.com/CDS/Images/CNN_architecture_fmnist.png\" width=900px/>\n",
    "</center>\n",
    "$\\hspace{7.5cm} \\text {CNN architecture used for Fashion MNIST dataset}$\n",
    "<br><br>\n",
    "\n",
    "The CNN architecture used for FMNIST dataset is shown in the figure above and its corresponding sequential model is created using the code below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AufjBh6_vNVD"
   },
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VDS4NtKeGWL3"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential([\n",
    "                    Conv2D(64, 7, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
    "                    MaxPooling2D(2),\n",
    "\n",
    "                    Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "\n",
    "                    Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "                    MaxPooling2D(2),\n",
    "\n",
    "                    Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "                    \n",
    "                    Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "                    MaxPooling2D(2),\n",
    "\n",
    "                    Flatten(),\n",
    "                    Dense(128, activation=\"relu\"),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(64, activation=\"relu\"),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(10, activation=\"softmax\")\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36rNsPuLGWL3"
   },
   "source": [
    "Let’s go through this model:\n",
    "\n",
    "* The first layer uses 64 fairly large filters (7 × 7) but no stride because the input images are not very large. It also sets input_shape=[28, 28, 1], because the images are 28 × 28 pixels, with a single color channel (i.e., grayscale).\n",
    "\n",
    "* Next, we have a max pooling layer which uses a pool size of 2, so it divides each spatial dimension by a factor of 2.\n",
    "\n",
    "* Then we repeat the same structure twice: two convolutional layers followed by a max pooling layer. For larger images, we could repeat this structure several more times (the number of repetitions is a hyperparameter we can tune).\n",
    "\n",
    "* Note that the number of filters grows as we climb up the CNN toward the output layer (it is initially 64, then 128, then 256): since the number of low-level features is often fairly low (e.g., small circles, horizontal lines), but there are many different ways to combine them into higher-level features.\n",
    "\n",
    "* Next is the fully connected network, composed of two hidden dense layers and a dense output layer. Note that we must flatten its inputs since a dense network expects a 1D array of features for each instance. We also add two dropout layers, to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aLiFiuyeGWL3"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyL3Lab4veQO"
   },
   "source": [
    "**Augentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yyYKZOdiGWL3"
   },
   "source": [
    "Here we use `ImageDataGenerator` that generates batches of tensor image data with real-time data augmentation. The images output by the generator will have the same dimensions as the input images. It lets us augment the images in real-time while the model is still training. \n",
    "\n",
    "We can apply any random transformations on the training image as it is passed to the model. Few parameters are:\n",
    "\n",
    "* **rescale**: scale the image\n",
    "* **horizontal_flip**: randomly flip inputs horizontally\n",
    "* **width_shift_range**: shift the image to the left or right (horizontal shifts)\n",
    "* **height_shift_range**: shift the image vertically (up or down)\n",
    "* **rotation_range**: degree range for random rotations of the image\n",
    "\n",
    "To know more about ImageDataGenerator, click [here](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Di7PG3XGGWL3"
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "batch_size = 256\n",
    "\n",
    "# Instantiate ImageDataGenerator\n",
    "gen = ImageDataGenerator(rescale = 1.0/255, \n",
    "                         width_shift_range = 0.005,\n",
    "                         height_shift_range = 0.005,\n",
    "                         rotation_range = 0,\n",
    "                         horizontal_flip = True)\n",
    "\n",
    "# Generate batches of tensor image data\n",
    "train_batches = gen.flow(X_train, y_train, batch_size = batch_size)\n",
    "val_batches = gen.flow(X_val, y_val, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z9y4B3LQmF8M"
   },
   "source": [
    "**Note:** Use [`flow_from_directory()`](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator#flow_from_directory) to load images from the directory. It takes the path to a directory & generates batches of augmented data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1AzgbfIKvics"
   },
   "source": [
    "**Training the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaIvMTWrmEXL"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_batches,\n",
    "                    steps_per_epoch = X_train.shape[0]//batch_size,\n",
    "                    epochs = 1,\n",
    "                    validation_data = val_batches,\n",
    "                    validation_steps = X_val.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3R93T5IGWL4"
   },
   "outputs": [],
   "source": [
    "# Instantiate ImageDataGenerator\n",
    "test_gen = ImageDataGenerator(rescale=1.0/255)\n",
    "# Generate batches of tensor image data\n",
    "test_batches = test_gen.flow(X_test, y_test, batch_size= 50)\n",
    "\n",
    "# Evaluate Model against test data and get the score\n",
    "score = model.evaluate(test_batches)\n",
    "\n",
    "# Print Metrics\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p48GcdeKGWL4"
   },
   "source": [
    "By increasing the number of epochs, the accuracy can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FPirMziwvppo"
   },
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lmWA8_3AGWL4"
   },
   "outputs": [],
   "source": [
    "# Predict class probabilities for first three instances of X_test\n",
    "X_new = X_test[-3:]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXaz6rgeGWL4"
   },
   "outputs": [],
   "source": [
    "# List of labels\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o2Z_sUv8GWL4"
   },
   "outputs": [],
   "source": [
    "# Predict class labels for first three instances of X_test\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "print(\"Predicted labels: \\n\", y_pred)\n",
    "\n",
    "print(np.array(class_names)[y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BsrtUE5DGWL4"
   },
   "outputs": [],
   "source": [
    "# Actual labels\n",
    "fig, ax = plt.subplots(1,3)\n",
    "for axi, i in zip(ax.ravel(), np.arange(len(X_new))):\n",
    "    axi.imshow(X_new[i].reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4WjysELFx1D9"
   },
   "source": [
    "From the above results, we can see the performance of network on three images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN4dAL3cvzVA"
   },
   "source": [
    "#### **CNN on Digit MNIST Dataset**\n",
    "#### Loading Data:\n",
    "Using SciKit-Learns ```fetch_openml``` to load MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7mOfLhVoc-T"
   },
   "outputs": [],
   "source": [
    "mnist = fetch_openml('mnist_784', cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G7A6-dk8oapg"
   },
   "source": [
    "**Understanding the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0hpchr3oZYI"
   },
   "outputs": [],
   "source": [
    "print('Data Shape :',mnist.data.shape,'\\n')\n",
    "print('Keys : ',mnist.keys(),'\\n')\n",
    "print('Description :', mnist.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7Etqe75oA0_"
   },
   "source": [
    "**Separating features and targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1Xmd1RioWNS"
   },
   "outputs": [],
   "source": [
    "(X, y) = mnist[\"data\"].values, mnist[\"target\"].values\n",
    "# Shape and datatype of X_train_full\n",
    "X.shape, X.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHm3uPwA09uE"
   },
   "source": [
    "**Converting label from string to int**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5eux-mg1D0a"
   },
   "outputs": [],
   "source": [
    "print(type(y[90]))\n",
    "y = y.astype(np.uint8)\n",
    "print(type(y[90]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-79_dzWtsjn"
   },
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = X[:60000], X[60000:], y[:60000], y[60000:]\n",
    "# Shape and datatype of X_train\n",
    "X_train_full.shape, X_train_full.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHuXREpQr7Rz"
   },
   "outputs": [],
   "source": [
    "# Split into training and validation data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=123)\n",
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hiuoj-60orly"
   },
   "source": [
    "**Visualizing the data set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-HzjA5HowR5"
   },
   "outputs": [],
   "source": [
    "sample_digit = X_train[60] # Check the result by putting different values in square bracket \n",
    "sample_digit_image = sample_digit.reshape(28, 28)\n",
    "plt.imshow(sample_digit_image, cmap=mpl.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxJM7yJv1SoR",
    "outputId": "8d603db6-fa4e-4980-a22d-ac6b75c79d64"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[60] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n2CH7LrEs4Ot",
    "outputId": "a2c8e76e-5bb5-4f9b-95f5-69d5c6428490"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((48000, 28, 28, 1), (12000, 28, 28, 1), (10000, 28, 28, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape train, test, and validation data\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_val = X_val.reshape(X_val.shape[0], 28, 28, 1)\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1e5cqAhJvVgY"
   },
   "source": [
    "**Model Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRf-9l6uvU0W"
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Conv2D(64,3, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D(2),\n",
    "\n",
    "                    Conv2D(64,3, activation=\"relu\", padding=\"same\"),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D(2),\n",
    "\n",
    "                    Conv2D(32,3, activation=\"relu\", padding=\"same\", input_shape=[28, 28, 1]),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D(2),\n",
    "\n",
    "                    Flatten(),\n",
    "                    Dense(64, activation=\"relu\"),\n",
    "                    Dropout(0.25),\n",
    "                    Dense(32, activation=\"relu\"),\n",
    "                    Dropout(0.25),\n",
    "                    Dense(10, activation=\"softmax\")\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4w4r2su0dDS"
   },
   "outputs": [],
   "source": [
    "# Summary of model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daDY6dVhx1Zr"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSlQM8kGyXPq"
   },
   "source": [
    "**Data Augmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wk2fuzgVx9_I"
   },
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "batch_size = 256\n",
    "\n",
    "# Instantiate ImageDataGenerator\n",
    "gen = ImageDataGenerator(rescale = 1.0/255, \n",
    "                         width_shift_range = 0.2,\n",
    "                         height_shift_range = 0.2,\n",
    "                         rotation_range = 0,\n",
    "                         horizontal_flip = True)\n",
    "\n",
    "# Generate batches of tensor image data\n",
    "train_batches = gen.flow(X_train, y_train, batch_size = batch_size)\n",
    "val_batches = gen.flow(X_val, y_val, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-mv4FAPykiy"
   },
   "source": [
    "**Training the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ZHVHsddyk0H"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(train_batches,\n",
    "                    steps_per_epoch = X_train.shape[0]//batch_size,\n",
    "                    epochs = 5,\n",
    "                    validation_data = val_batches,\n",
    "                    validation_steps = X_val.shape[0]//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akCQJi2Ryxcg"
   },
   "outputs": [],
   "source": [
    "# Instantiate ImageDataGenerator\n",
    "test_gen = ImageDataGenerator(rescale=1.0/255)\n",
    "# Generate batches of tensor image data\n",
    "test_batches = test_gen.flow(X_test, y_test, batch_size= 50)\n",
    "\n",
    "# Evaluate Model against test data and get the score\n",
    "score = model.evaluate(test_batches)\n",
    "\n",
    "# Print Metrics\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgN_tXvKAi5L"
   },
   "source": [
    "**Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4XpwLY8tAkbE",
    "outputId": "c79ab3e6-c326-425a-b6c0-c8e9d57460c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict class probabilities for first three instances of X_test\n",
    "X_new = X_test[-3:]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LazQ6pDsAkbK",
    "outputId": "05ce2cab-c7a0-4404-e8e8-418349e895d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: \n",
      " [4 5 6]\n"
     ]
    }
   ],
   "source": [
    "# Predict class labels for first three instances of X_test\n",
    "y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "print(\"Predicted labels: \\n\", y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 149
    },
    "id": "Hnseg9GzAkbM",
    "outputId": "3ed42225-2ee4-444a-beac-82f7ae27a459"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Actual labels\n",
    "fig, ax = plt.subplots(1,3)\n",
    "for axi, i in zip(ax.ravel(), np.arange(len(X_new))):\n",
    "    axi.imshow(X_new[i].reshape(28, 28), cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZCqiG78qAkbN"
   },
   "source": [
    "From the above results, we can see the performance of network on three images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqsoBKABR1wy"
   },
   "source": [
    "### Classic Networks:\n",
    "* LeNet\n",
    "* AlexNet\n",
    "* VGG\n",
    "* ResNet\n",
    "* Inception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJx93dFgYUzS"
   },
   "source": [
    "### AlexNet\n",
    "Now, let's look at the AlexNet architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97_jAo5AYby7"
   },
   "source": [
    "The AlexNet CNN architecture won the 2012 ImageNet ILSVRC challenge by a\n",
    "large margin. It was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. It was the first to stack convolutional layers directly on top of one another, instead of stacking a pooling layer on top of each convolutional layer. The table given below presents this architecture.\n",
    "<br><br>\n",
    "\n",
    "|  Layer  |  Type  | Maps  |  Size  |  Kernel Size | Stride  |  Padding  |  Activation  \n",
    "|:--------------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|:-----------|\n",
    "|  Out  | Fully connected | - |  1,000 | - | - | - | Softmax |\n",
    "| F10 | Fully connected | - | 4,096 | - | - | - | ReLU |\n",
    "| F9 | Fully connected | - | 4,096 | - | - | - | ReLU |\n",
    "| S8 | Max pooling | 256 | 6 x 6 | 3 x 3 | 2 | valid | - |\n",
    "| C7 | Convolution | 256 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
    "| C6 | Convolution | 384 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
    "| C5 | Convolution | 384 | 13 x 13 | 3 x 3 | 1 | same | ReLU |\n",
    "| S4 | Max pooling | 256 | 13 x 13 | 3 x 3 | 2 | valid | - |\n",
    "| C3 | Convolution | 256 | 27 x 27 | 5 x 5 | 1 | same | ReLU |\n",
    "| S2 | Max pooling | 96 | 27 x 27 | 3 x 3 | 2 | valid | - |\n",
    "| C1 | Convolution | 96 | 55 x 55 | 11 x 11 | 4 | valid | ReLU |\n",
    "| In | Input | 3 (RGB) | 227 x 227 | - | - | - | - |\n",
    "\n",
    "$\\hspace{11cm} \\text {AlexNet architecture}$\n",
    "<br><br>\n",
    "\n",
    "To reduce overfitting, the authors used two regularization techniques: \n",
    "\n",
    "* **dropout** with a 50% dropout rate during training to the outputs of layers F9 and F10\n",
    "\n",
    "* **data augmentation** by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions.\n",
    "\n",
    "Let's build the AlexNet architecture and train it on the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) dataset. The dataset contains 60,000 colour images, each with dimensions 32x32px. The content of the images within the dataset is sampled from 10 classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AG8oPbxgfD9M"
   },
   "outputs": [],
   "source": [
    "# Load CIFAR-10 dataset\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hO8nJXENc4UZ"
   },
   "outputs": [],
   "source": [
    "# List to refer labels\n",
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SicSsFDlg0q3"
   },
   "source": [
    "By default, the CIFAR dataset is partitioned into 50,000 training data and 10,000 test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hryFMs6yg5w1"
   },
   "outputs": [],
   "source": [
    "# Shape of train and test images\n",
    "train_images.shape, test_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8lSZGQX2k7i"
   },
   "source": [
    "Instead of taking all the 50000 instances for training, we will only use the first 5000 instances as validation set and next the 5000 instances as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZbeEwoBdY1k"
   },
   "outputs": [],
   "source": [
    "# Validation data\n",
    "val_images, val_labels = train_images[:5000], train_labels[:5000]\n",
    "# Training data\n",
    "train_images, train_labels = train_images[5000:10000], train_labels[5000:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp4JiR0U3E3l"
   },
   "source": [
    "To ease data manipulation and modification we transform the dataset into a TensorFlow dataset using `tf.data.Dataset.from_tensor_slices` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CX3SelvVhlLr"
   },
   "outputs": [],
   "source": [
    "# Convert to TensorFlow dataset\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
    "val_ds = tf.data.Dataset.from_tensor_slices((val_images, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "arkXw5fujoYC"
   },
   "outputs": [],
   "source": [
    "# Visualize few images\n",
    "plt.figure(figsize=(10,10))\n",
    "for i, (image, label) in enumerate(train_ds.take(5)):\n",
    "    ax = plt.subplot(5,5,i+1)\n",
    "    plt.imshow(image)\n",
    "    plt.title(class_names[label.numpy()[0]])\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LeZ0jMuj4i7x"
   },
   "source": [
    "Now as per the network, we need to do few preprocessing steps:\n",
    "\n",
    "* Normalize images to have a mean of 0 and standard deviation of 1\n",
    "\n",
    "* Resizing of the images from 32 x 32 to 227 x 227. The AlexNet network input expects a 227x227 image.\n",
    "\n",
    "We create a function called `process_images` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zQNnlYXnjj7-"
   },
   "outputs": [],
   "source": [
    "def process_images(image, label):\n",
    "    ''' Normalize images to have a mean of 0 and standard deviation of 1, \n",
    "        resize images from 32x32 to 227x227 '''\n",
    "    image = tf.image.per_image_standardization(image)\n",
    "    image = tf.image.resize(image, (227,227))\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-yrMgCW7jhoX"
   },
   "outputs": [],
   "source": [
    "# Size of training, testing, validation set\n",
    "train_ds_size = len(train_ds)\n",
    "test_ds_size = len(test_ds)\n",
    "val_ds_size = len(val_ds)\n",
    "print(\"Training data size:\", train_ds_size)\n",
    "print(\"Test data size:\", test_ds_size)\n",
    "print(\"Validation data size:\", val_ds_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0MLwnvCjdq-"
   },
   "outputs": [],
   "source": [
    "# Perform preprocessing with shuffle and batch data operations\n",
    "train_ds = (train_ds.map(process_images)\n",
    "                    .shuffle(buffer_size=train_ds_size)\n",
    "                    .batch(batch_size=32, drop_remainder=True))\n",
    "\n",
    "test_ds = (test_ds.map(process_images)\n",
    "                  .shuffle(buffer_size=test_ds_size)\n",
    "                  .batch(batch_size=32, drop_remainder=True))\n",
    "\n",
    "val_ds = (val_ds.map(process_images)\n",
    "                .shuffle(buffer_size=val_ds_size)\n",
    "                .batch(batch_size=32, drop_remainder=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvS4htQliMqx"
   },
   "source": [
    "Model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYfX1HGIiOfd"
   },
   "outputs": [],
   "source": [
    "# Create AlexNet CNN architecture\n",
    "model = Sequential([\n",
    "                    Conv2D(filters=96, kernel_size=(11,11), strides=(4,4), activation='relu', input_shape=(227,227,3)),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "                    Conv2D(filters=256, kernel_size=(5,5), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "                    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "                    BatchNormalization(),\n",
    "                    Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "                    BatchNormalization(),\n",
    "                    Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\"),\n",
    "                    BatchNormalization(),\n",
    "                    MaxPooling2D(pool_size=(3,3), strides=(2,2)),\n",
    "                    Flatten(),\n",
    "                    Dense(4096, activation='relu'),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(4096, activation='relu'),\n",
    "                    Dropout(0.5),\n",
    "                    Dense(10, activation='softmax')\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qCGPud2bjFfR"
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer = tf.optimizers.SGD(learning_rate = 0.001), metrics = ['accuracy'])\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbbswBHpjMF3"
   },
   "outputs": [],
   "source": [
    "# Train model on training set\n",
    "history = model.fit(train_ds, epochs = 1, validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpwGIkPwjUiZ"
   },
   "outputs": [],
   "source": [
    "# Model performance on test set\n",
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT6B1L_QR9wz"
   },
   "source": [
    "By increasing the number of training instances and epochs, the model performance can be increased.\n",
    "\n",
    "Other than AlexNet we have few more CNN architectures such as ResNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqEr5Jx-GWL7"
   },
   "source": [
    "### Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dfmVta-GWL7"
   },
   "source": [
    "1. What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    " These are the main advantages of a CNN over a fully connected DNN for image\n",
    "classification:\n",
    "\n",
    "  * Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data.\n",
    "  \n",
    "  * When a CNN has learned a kernel that can detect a particular feature, it can\n",
    "detect that feature anywhere in the image. In contrast, when a DNN learns a\n",
    "feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples.\n",
    "\n",
    "  * Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN’s architecture embeds this prior\n",
    "knowledge. Lower layers typically identify features in small areas of the images, while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start compared to DNNs.\n",
    "\n",
    "2. Why would you want to add a max pooling layer rather than a convolutional\n",
    "layer with the same stride?\n",
    "\n",
    " A max pooling layer has no parameters at all, whereas a convolutional layer has\n",
    "quite a few."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VHfHdGCP_n6Y"
   },
   "source": [
    "## Please answer the questions below to complete the experiment:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmvdJ4aNmGjR"
   },
   "outputs": [],
   "source": [
    "#@title Select the FALSE statement:  { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
    "Answer1 = \"\" #@param [\"\",\"VALID padding will include all the rows and columns whereas SAME padding may ignore some rows and columns of the input image\", \"A higher stride means higher information loss but resultant less computation\", \"A max pooling layer does not have any parameter\", \"All of above\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxjKf9FKXrEZ"
   },
   "outputs": [],
   "source": [
    "#@title Which of the following methods DOES NOT prevent a model from overfitting to the training set?.{ run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
    "Answer2 = \"\" #@param [\"\",\"Early stopping\", \"Data augmentation\",\"Pooling\",\"Dropout\",\"None of the above\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMzKSbLIgFzQ"
   },
   "outputs": [],
   "source": [
    "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
    "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjcH1VWSFI2l"
   },
   "outputs": [],
   "source": [
    "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
    "Additional = \"\" #@param {type:\"string\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4VBk_4VTAxCM"
   },
   "outputs": [],
   "source": [
    "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XH91cL1JWH7m"
   },
   "outputs": [],
   "source": [
    "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8xLqj7VWIKW"
   },
   "outputs": [],
   "source": [
    "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
    "Mentor_support = \"\" #@param [\"\",\"Excellent\", \"Very Good\", \"Good\", \"Average\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "FzAZHt1zw-Y-"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
    "try:\n",
    "  if submission_id:\n",
    "      return_id = submit_notebook()\n",
    "      if return_id : submission_id = return_id\n",
    "  else:\n",
    "      print(\"Please complete the setup first.\")\n",
    "except NameError:\n",
    "  print (\"Please complete the setup first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzXv9SSqTG1D"
   },
   "source": [
    "### **Task**\n",
    "Create a CNN model again for Digit MNIST DataSet using the given architecture below:\n",
    "<center>\n",
    "<img src=\"https://cdn.extras.talentsprint.com/IOT/Images/CNN_Practice.png\" \n",
    "width=700 px/>\n",
    "\n",
    "</center>\n",
    "\n",
    "* Use batch normalization and dropout and play with the layer, neurons, other parameters/hyperparameters and check the accuracy.\n",
    "\n",
    "* Keep epoch less than 5/6 initially.\n",
    "\n",
    "* Complete all the steps till the last predictin and visualization.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
