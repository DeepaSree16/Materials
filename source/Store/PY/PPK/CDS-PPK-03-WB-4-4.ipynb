{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"M0_Practice_04_Data_Wrangling.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xOJ9npnFEi-d"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Practice Notebook: Data Wrangling (Ungraded)"]},{"cell_type":"markdown","metadata":{"id":"CBkjMtSgFBuv"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"FpfeKNyLFEbX"},"source":["At the end of the experiment, you will be able to :\n","\n","* load the dataframe using pandas\n","* perform various operations in the dataframe like:\n","     * renaming the columns,\n","     * indexing and data retrieval (both column wise and row wise),\n","     * drop and add a columns in the dataframe,\n","     * integration of datasets,\n","     * data cleaning,  \n","     * data tansformation operations like standardization, label encoding, feature scaling, etc,\n","     * find the correlation among different features of dataset."]},{"cell_type":"markdown","metadata":{"id":"Km0a903iYVhN"},"source":["#### Exercise 1: Loading the data using Dataframe"]},{"cell_type":"code","metadata":{"id":"M0bLbYlSXrsJ"},"source":["# pandas\n","import pandas as pd\n","# numpy\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2EWdwD6kN_Hz"},"source":["Dataset is chosen from the [UCI repository](http://archive.ics.uci.edu/ml/datasets/Forest+Fires)"]},{"cell_type":"markdown","metadata":{"id":"u20eO4_omaO8"},"source":["#### Dataset Description"]},{"cell_type":"markdown","metadata":{"id":"Kp3Mc8_AmkB6"},"source":["#### **Forest Fires Dataset**\n","1. Number of Instances: 517\n","2. Number of Attributes: 13\n","3. Attribute Information:\n","    \n","      i. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n","\n","      ii. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n","\n","      iii. month - month of the year: 'jan' to 'dec'\n","\n","      iv. day - day of the week: 'mon' to 'sun'\n","      \n","      v. FFMC - FFMC index from the FWI system: 18.7 to 96.20\n","\n","      vi. DMC - DMC index from the FWI system: 1.1 to 291.3\n","      \n","      vii. DC - DC index from the FWI system: 7.9 to 860.6\n","\n","      viii. ISI - ISI index from the FWI system: 0.0 to 56.10\n","     \n","     ix. te mp - temperature in Celsius degrees: 2.2 to 33.30\n","      \n","      x. RH - relative humidity in %: 15.0 to 100\n","      \n","      xi. wind - wind speed in km/h: 0.40 to 9.40\n","      \n","      xii. rain - outside rain in mm/m2 : 0.0 to 6.4\n","      \n","      xiii. area - the burned area of the forest (in ha): 0.00 to 1090.84 "]},{"cell_type":"code","metadata":{"id":"uNNcTjrZ6yHX"},"source":["# download the data\n","!wget -qq https://archive.ics.uci.edu/ml/machine-learning-databases/forest-fires/forestfires.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QwNwWMaKKbMK"},"source":["# setting up headers\n","fire_headers = ['X', 'Y', 'month', 'day', 'FFMC', 'DMC', 'DC', 'ISI', 'temp', 'RH', 'wind', 'rain', 'area']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTn6eVQSK5i5"},"source":["# applying headers and inspecting for accuracy \n","df = pd.read_csv('forestfires.csv', names=fire_headers)\n","# display first 5 rows of the dataset\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ef8UE3eUHNEn"},"source":["In the above dataframe, first row contains the duplicate headers, skip the header using `skiprows`"]},{"cell_type":"code","metadata":{"id":"ZX01564sHKkD"},"source":["# using pandas read the csv file\n","df = pd.read_csv('forestfires.csv',skiprows=1, names=fire_headers)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_TffBhDVYd0W"},"source":["#### Exercise 2: Rename the columns\n","\n","Based on the dataset attributes, we will rename RH column to Relative Humidity and other columns"]},{"cell_type":"code","metadata":{"id":"j8iqwrMlYhPL"},"source":["# rename the columns\n","df = df.rename(columns = {'RH':'Relative Humidity', 'X':'X-axis', 'Y':'Y-axis'})\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jsv9c537Yhmm"},"source":["#### Exercise 3: Indexing and data retrieval"]},{"cell_type":"code","metadata":{"id":"5PuhaPpsp_R0"},"source":["# setting temp as index column\n","df.set_index(\"temp\", inplace = True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jc7Uh0_8sAHW"},"source":["Earlier the index is temp column, it can be removed by resetting the index."]},{"cell_type":"code","metadata":{"id":"-RM0INV0usmB"},"source":["# resetting index\n","df.reset_index(inplace = True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mU6L1HnPQigQ"},"source":["Retrieve column-wise  data from the dataframe"]},{"cell_type":"code","metadata":{"id":"gH9OYk1gYnZs"},"source":["# retrieving columns\n","X1 = df[\"wind\"]\n","print(X1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"smcAA0c2gaES"},"source":["# retrieving multiple columns\n","X2 = df[[\"wind\", \"rain\", \"area\"]]\n","print(X2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h5hVVNqwQ5Ix"},"source":["`loc` is label-based, which means that you have to specify rows and columns based on their row and column labels.\n","\n","syntax: ` loc[row_label, column_label]`"]},{"cell_type":"code","metadata":{"id":"SyZjW5G8hM8r"},"source":["# retrieving rows by loc method\n","rows_1 = df.loc[[4,5,6,7,8,9,10],'temp']\n","print(rows_1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nO6mIqINQ9FT"},"source":["`iloc` is integer position-based, so you have to specify rows and columns by their integer position values (0-based integer position).\n","\n","syntax: `iloc[row_position, column_position]`"]},{"cell_type":"code","metadata":{"id":"4MRLXqV-lWzC"},"source":["# retrieving data using integer location\n","sub_data = df.iloc[0:10,:3]\n","sub_data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZE1uJQ48SfaZ"},"source":["Index of a dataframe"]},{"cell_type":"code","metadata":{"id":"2WECpwJho8Iv"},"source":["# start the index from 1\n","df.index = range(1,len(df)+1)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K13otw6dYp7Y"},"source":["#### Exercise 4: Drop the columns\n","\n","* Identify the irrelevant columns and drop\n","\n","day and month can be added into one single column by dropping"]},{"cell_type":"code","metadata":{"id":"vPzBJsgQLfXx"},"source":["# new column date\n","df['date'] = df['day']+\", \"+df['month']\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oLqELAX6YrZU"},"source":["# drop columns \n","df.drop([\"day\",'month'], axis = 1, inplace=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hV7bCC9ZYs6h"},"source":["#### Exercise 5: Data Integration\n","\n","Create the two dataframes representing different information and combine them as one dataframe"]},{"cell_type":"code","metadata":{"id":"KBJxze4QjQh9"},"source":["# Students dataframe\n","df1 = pd.DataFrame({\"Name\": [\"Aman\", \"Joy\", \"Vinay\", \"Jack\", \"Rita\", \"Robin\", \"Sam\"], \n","                    \"Rollno\": [1, 2, 3, 4, 5, 6, 7]})\n","df1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Kbf7jlyNaGe"},"source":["# Marks dataframe\n","df2 = pd.DataFrame({\"Rollno\": [1, 2, 3, 4, 5, 6, 7],\n","                    \"Maths\": [40, 50, 30, 60, 82, 74, 25],\n","                    \"English\": [90, 84, 48, 64, 45, np.nan, 46],\n","                    \"Science\": [66, 54, 20, np.nan, 90, 48, 28]})\n","df2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FbqwZNUoNfDd"},"source":["Merge both the dataframes side by side, as they are representing different information with common identifier `Rollno`"]},{"cell_type":"code","metadata":{"id":"YcuzAQZ_f9fX"},"source":["# Merge df1 and df2 on the 'Rollno' and 'Roll No' columns\n","df_merge = pd.merge(df1, df2, on=['Rollno'])\n","df_merge"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aqa4NpYAizPQ"},"source":["To know more about other merge operations, click [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html)."]},{"cell_type":"code","metadata":{"id":"mnjAWesEjoyH"},"source":["# Combine two dataframes along columns\n","df_concat = pd.concat([df1, df2],axis=1)\n","df_concat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"diovpnLclRSf"},"source":["To know more about other concat operations, click [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.concat.html)."]},{"cell_type":"markdown","metadata":{"id":"HsjM8O6oYxkR"},"source":["#### Exercise 6: Data Cleaning"]},{"cell_type":"code","metadata":{"id":"qVd29sLnmmUh"},"source":["# Check for missing values\n","df_merge.isna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSauUUl6mW7_"},"source":["# Count missing values\n","df_merge.isna().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7tYrZ44Nnsh0"},"source":["# Show the mean of each column\n","df_merge.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D8sflewSmdas"},"source":["# Filling missing values with the mean value of that column\n","df_filled = df_merge.fillna(df_merge.mean())\n","df_filled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rz7DokQLYz3r"},"source":["#### Exercise 7: Data Transformation\n","\n","Create a Total marks column and identify the Result based on the marks"]},{"cell_type":"code","metadata":{"id":"IbcaBppUp6Za"},"source":["# Add 'Total' marks column \n","df_filled[\"Total\"] = df_filled.iloc[:, -3:].sum(axis=1)\n","df_filled"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CqJFRzTt1nn"},"source":["# Add 'Result' column\n","for i in range(len(df_filled)):\n","    # consider above 35% as pass out of total 300 marks\n","    if df_filled.loc[i, \"Total\"] > (0.35 * 300):\n","        df_filled.loc[i, \"Result\"] = \"Pass\"\n","    else:\n","        df_filled.loc[i, \"Result\"] = \"Fail\"\n","df_filled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rY3vW6x4Ugjj"},"source":["**LabelEncoder:** Sklearn provides a very efficient tool for encoding the levels of categorical features into numeric values. LabelEncoder encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier."]},{"cell_type":"code","metadata":{"id":"rCVFvz-KY2IE"},"source":["# Label encoder\n","from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","le.fit(df_filled[\"Result\"])\n","label = le.transform(df_filled[\"Result\"])\n","df_filled[\"Result label\"] = label\n","df_filled"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JaVxskq-u9oL"},"source":["To know more about LabelEncoder, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html)."]},{"cell_type":"markdown","metadata":{"id":"-pZSOMrWU1Ob"},"source":["**Feature Scaling:** It is a step of Data Pre Processing which is applied to independent variables or features of data. It basically helps to normalise the data within a particular range. Sometimes, it also helps in speeding up the calculations in an algorithm.\n","\n","To know more about StandardScaler, click [here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)"]},{"cell_type":"code","metadata":{"id":"eTUWb9M2dwHg"},"source":["# StandardScaler from sklearn\n","from sklearn.preprocessing import StandardScaler\n","\n","df_scaled_sc = df_filled.copy()\n","\n","#defining the StandardScaler variable\n","sc = StandardScaler()\n","sc.fit(df_scaled_sc[[\"Maths\", \"English\", \"Science\"]])\n","\n","# using .transform to apply StandardScaler operation \n","scaled = sc.transform(df_scaled_sc[[\"Maths\", \"English\", \"Science\"]])\n","df_scaled_sc[[\"Maths_scaled_StandardScaler\", \"English_scaled_StandardScaler\", \"Science_scaled_StandardScaler\"]] = scaled\n","df_scaled_sc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EQnSRWgVWbp9"},"source":["To know more about MinMaxScaler, check this [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)"]},{"cell_type":"code","metadata":{"id":"47NS16bCwx2l"},"source":["# MinMaxScaler from sklearn\n","from sklearn.preprocessing import MinMaxScaler\n","\n","df_scaled_mms = df_filled.copy()\n","\n","mms = MinMaxScaler()\n","# transform features by scaling each feature to a given range e.g. [0,1]\n","mms.fit(df_scaled_mms[[\"Maths\", \"English\", \"Science\"]])\n","\n","scaled = mms.transform(df_scaled_mms[[\"Maths\", \"English\", \"Science\"]])\n","df_scaled_mms[[\"Maths_scaled_MinMaxScaler\", \"English_scaled_MinMaxScaler\", \"Science_scaled_MinMaxScaler\"]] = scaled\n","df_scaled_mms"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rNWW8Sc7Y2u2"},"source":["#### Exercise 8: Features correlation\n","\n","**Data Correlation:** Is a way to understand the relationship between multiple variables and attributes in your dataset. Using Correlation, you can get some insights such as:\n","One or multiple attributes depend on another attribute or a cause for another attribute.\n","One or multiple attributes are associated with other attributes."]},{"cell_type":"code","metadata":{"id":"IvUCv28eZAGz"},"source":["# Correlation between features\n","df_scaled_sc.corr()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N6OIDA4Q1ua7"},"source":["# Heatmap of correlation matrix\n","import seaborn as sns\n","sns.heatmap(df_scaled_sc.corr())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GVdkpywPxted"},"source":["# Change the marks to 35 if it is below 35 for all three subjects\n","df = df_filled[[\"Maths\", \"English\", \"Science\"]]\n","# Marks below 35\n","df.mask(df < 35)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yvJRBRWF0-TW"},"source":["# Assign marks\n","df.mask(df < 35, 35)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nzAIF4lV1YGo"},"source":["To know more about df.mask() operation, click [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mask.html)."]}]}