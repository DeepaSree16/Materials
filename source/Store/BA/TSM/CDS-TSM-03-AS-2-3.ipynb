{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CDS-TSM-03-AS-6-7.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vFGxXJv8tj5d"},"source":["# Advanced Certification Program in Computational Data Science\n","## A program by IISc and TalentSprint\n","### Assignment 6: Timeseries ARIMA"]},{"cell_type":"markdown","metadata":{"id":"nqqRjnCqz1-e"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"2odBo6PtImzj"},"source":["At the end of the experiment, you will be able to:\n","\n","* understand the time series and its stationarity\n","* understand the AR and MA terms\n","* perform Time Series Analysis with ARIMA Forecasting model\n","* understand SARIMA Forecasting model"]},{"cell_type":"markdown","metadata":{"id":"HyXQmXWENr1i"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"B4bMv9WSNuGi"},"source":["The dataset chosen for this experiment is  AirPassengers. This dataset provides monthly totals of US airline passengers from 1949 to 1960. "]},{"cell_type":"markdown","metadata":{"id":"J2VSAz_87W-b"},"source":["## Information"]},{"cell_type":"markdown","metadata":{"id":"t6VIOnrmpihg"},"source":["**Auto Regressive (AR)** model is a specific type of regression model where the dependent variable depends on past values of itself.\n","This necessarily means that the current values are correlated with values in the previous time-steps. And more specifically, the type of correlation here is partial auto-correlation. The equation for the AR model is shown below.\n","\n","$Y_t = \\beta_1 + \\Phi_1 Y_{t-1} + \\Phi_2 Y_{t-2}+...+\\Phi_p Y_{t-p}$\n","\n","The respective weights(Ф1, Ф2 …Фp) of the corresponding lagged observations are decided by the correlation between… that lagged observation and the current observation. If the correlation is more, the weight corresponding to that lagged observation is high (and vice-versa).\n","\n","Notice the (p) in the equation is called the lag order. It represents the number of prior lag observations we include in the model i.e. the number of lags which have a significant correlation with the current observation.\n","\n","**Moving Average (MA)** model works by analyzing how wrong we were in predicting values for the previous time periods to make a better estimate for the current time period. This model factors in errors from the lagged observations. The effects of these previous(lagged) observation errors, on the current observation, depend on the auto-correlation between them. This is similar in the sense to the AR model which considers the partial auto-correlations.\n","\n","$Y_t = \\beta_2 + \\omega_1 \\varepsilon_{t-1} + \\omega_2 \\varepsilon_{t-2} + ...+\\omega_q \\varepsilon_{t-q}+\\varepsilon_t$\n","\n","The ε terms represent the errors observed at respective lags and the weights (ω1,ω2 …ωq) are calculated statistically depending on the correlations. Notice the (q) in the equation represents the size of the moving window i.e. the number of lag observation errors which have a significant impact on the current observation. It is similar to the lag order(p), but it considers errors instead of the observations themselves.\n","\n","MA model supplements the AR model by taking into consideration, the errors from the previous time periods thereby helping to get a better estimate.\n","When we combine the AR and MA equation we get\n","\n","$Y_t = (\\beta_1 + \\beta_2) + (\\Phi_1 Y_{t-1})+...+(\\Phi_p Y_{t-p}) + (\\omega_1 \\varepsilon_{t-1}+...+\\omega_q \\varepsilon_{t-q}+\\varepsilon_t)$\n","\n","Integrating AR and MA, the final ARIMA model will take the following form.\n","\n","**ARIMA(p,d,q)** where, p = lag order, d = order of differencing, q = moving avergae window."]},{"cell_type":"markdown","metadata":{"id":"RCY_cBEcMdig"},"source":["**Task Flow:**\n","\n","Observation of trends, seasonality, and noise\n","\n","Making a Time Series Stationary\n","\n","Forecasting Model -ARIMA"]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Download dataset\n","!wget https://cdn.iisc.talentsprint.com/aiml/Experiment_related_data/AirPassengers.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRIzMfgjwcAp"},"source":["### Import required Packages "]},{"cell_type":"code","metadata":{"id":"xtDkSr_YRgzP"},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import mean_squared_error\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n","from statsmodels.tsa.arima_model import ARIMA\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","# import necessary files for SARIMA model\n","import statsmodels.api as sm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyQN8GeAwOdx"},"source":["### Load the Data"]},{"cell_type":"code","metadata":{"id":"VRN75fCWOiWw"},"source":["# Read the AirPassengers dataset \n","data = pd.read_csv('AirPassengers.csv',  parse_dates = True)\n","data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2EH0tDyNGSt"},"source":["# Convert Month object into datetime\n","data['Month'] = pd.to_datetime(data.Month)\n","data.set_index('Month', inplace=True)\n","df = data.sort_index(axis=0)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FjSWnRw_GyJz"},"source":["### Before applying the model let us identify the trends and seasonality from the given time series data\n"]},{"cell_type":"markdown","metadata":{"id":"XMDzikDC9meB"},"source":["The statsmodels library provides a suite of functions for working with time series data such as `seasonal_decompose()` function."]},{"cell_type":"code","metadata":{"id":"Jeh_m6HIEg_o"},"source":["ts = data['Passengers']\n","# Decomposition time series\n","result = seasonal_decompose(ts) \n","result.plot();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"90j2Ilx3HNXo"},"source":["The seasonal decompose method broke down the data into three parts: trend, \n","seasonality and residual components.\n","\n","**Trend** – varying mean over time. **For eg.,** the number of passengers is seen to be growing over time.\n","\n","**Seasonality** – variations in specific time-frames. **For eg.,** people may travel more in specific months owing to holidays/festivals.\n","\n","**Residual Component (noise)** - We can see from the plot that there appears to be randomness in the data."]},{"cell_type":"markdown","metadata":{"id":"c8FB0aPMR7y5"},"source":["We can also perform Rolling Statistics to ensure whether the time series is stationary or not.\n","\n","**Rolling Statistics:** Plot the rolling mean and rolling standard deviation. The time series is stationary if they remain constant with time"]},{"cell_type":"markdown","metadata":{"id":"oUU0IVhNmvro"},"source":["**Time Series Stationarity**\n","\n","For a time series to be stationary, it should satisfy the following 3 conditions\n","* Mean (μ) is constant\n","* Standard Deviation (σ) is constant\n","* Seasonality doesn’t exist\n","\n","In most cases, these conditions can be visually analyzed by studying the plot against time."]},{"cell_type":"code","metadata":{"id":"U1buicp_R3s0"},"source":["# Let’s create a function to run the test which determines whether a given time series is stationary\n","def get_stationarity(timeseries):\n","    # Rolling statistics\n","    rolling_mean = timeseries.rolling(window=12).mean()\n","    rolling_std = timeseries.rolling(window=12).std()\n","    \n","    # Rolling statistics plot\n","    original = plt.plot(timeseries, color='blue', label='Original')\n","    mean = plt.plot(rolling_mean, color='red', label='Rolling Mean')\n","    std = plt.plot(rolling_std, color='black', label='Rolling Std')\n","    plt.legend(loc='best')\n","    plt.title('Rolling Mean & Standard Deviation')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OxvWI39SfNFY"},"source":["# Show few rows of df\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bAlIAZCYIcU5"},"source":["# Check for stationarity\n","get_stationarity(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oNtCbhSQSCcH"},"source":["As we can see, the rolling mean and rolling standard deviation increase with time. ***Therefore, it can be inferred that the time series is not stationary.***"]},{"cell_type":"markdown","metadata":{"id":"XwV8VfrKjLsb"},"source":["### Making the time series stationary"]},{"cell_type":"markdown","metadata":{"id":"O8RUO0TErhWK"},"source":["Stationarity is important because many useful analytical tools and statistical tests and models rely on it.\n","\n","Transformations such as logarithms can help to stabilize the variance of a time series. Differencing can help stabilize the mean of a time series by removing changes in the level of a time series, and therefore reducing (or eliminating) trend and seasonality."]},{"cell_type":"code","metadata":{"id":"K8vGzKTkR_xc"},"source":["# ESTIMATING THE TREND\n","# Applying a log transformation\n","df_log = np.log(df)\n","plt.plot(df_log)\n","plt.xlabel(\"Time (Months)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yLUg6pZCT_my"},"source":["Eliminating the Trend and Seasonality by **Differencing** (taking the difference with a particular time lag)"]},{"cell_type":"code","metadata":{"id":"cOepLf73KrqR"},"source":["# Differencing\n","shift_df = pd.concat([df_log, df_log.shift(1)],axis=1)\n","shift_df.columns = ['Actual_Passengers','Forecasted_Passengers']\n","shift_df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xILNzEVy-ZRP"},"source":["We create a lagged copy of the data by subtracting every previous data point from the current data point."]},{"cell_type":"code","metadata":{"id":"4Y2OorNHLqgZ"},"source":["# Taking the difference with a particular time lag\n","df_log_shift = shift_df['Actual_Passengers'] - shift_df['Forecasted_Passengers']\n","df_log_shift.dropna(inplace=True)\n","get_stationarity(df_log_shift)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZN4nldzrUJ5Y"},"source":["We can see that the mean and standard deviation are approximately horizontal. We can also take second or third order differences which might get even better results in certain applications. However, it is still more stationary than the original."]},{"cell_type":"code","metadata":{"id":"rPisSPDrXJy5"},"source":["# Splitting the data into train and test sets\n","train_data, test_data = df_log[0:int(len(df_log)*0.8)], df_log[int(len(df_log)*0.8):]\n","train_data.size, test_data.size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lx0pfnIF_F3m"},"source":["### ARIMA Model"]},{"cell_type":"markdown","metadata":{"id":"9dqQvx84syPY"},"source":["Let us now apply the **ARIMA model (Autoregressive {p} Integrated {d} Moving Average {q}).** \n","\n","The model has 3 parameters:\n","\n","* **p** - the parameter associated with the `Auto-Regressive part` of the ARIMA model. We can use the **PACF** (partial autocorrelation function) plot to find the optimal p value.\n","\n","* **d** - the parameter associated with the `Integration part` of the ARIMA model. This is the **order of difference** or the number of times the time series is differenced in order to stationarize the series.\n","\n","* **q** - the parameter associated with the `Moving Average part` of the ARIMA model. We can use the **ACF** (autocorrelation function) plot to find the optimal q value.\n","\n","ACF describes how well the present value of the series is related to its past values and PACF finds correlation of the residuals of the current with the next lag value. These functions plot **lags** on the horizontal axis and the **correlations** on the vertical axis. \n"]},{"cell_type":"code","metadata":{"id":"tiiJQGOEkEZF"},"source":["# Visualize PACF and ACF plots\n","trainData_diff = train_data.diff().dropna()\n","fig, ax = plt.subplots(1,2, figsize =(20, 5)) \n","fig = plot_pacf(trainData_diff, lags=22, ax=ax[0]);ax[0].set_xlabel(\"Lag\");ax[0].set_ylabel(\"PACF\");\n","fig = plot_acf(trainData_diff, lags=22, ax=ax[1]);ax[1].set_xlabel(\"Lag\");ax[1].set_ylabel(\"ACF\"); \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"epIctwYPAeHy"},"source":["From the above PACF and ACF plots, we can see a sharp cut-off at lag-1 in both of the plots. \n","\n","The [rules](https://people.duke.edu/~rnau/arimrule.htm) for identifying ARIMA models suggests:\n","\n","* If the partial autocorrelation function (PACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is positive then consider adding one or more AR terms to the model. The lag beyond which the PACF cuts off is the indicated number of AR terms.\n","\n","* If the autocorrelation function (ACF) of the differenced series displays a sharp cutoff and/or the lag-1 autocorrelation is negative then consider adding an MA term to the model. The lag beyond which the ACF cuts off is the indicated number of MA terms.\n","\n","From the above points, we can select $p=0+1=1$ and $q=0+1=1$. Also, the differencing is done one time to achieve stationarity, so $d=1$."]},{"cell_type":"code","metadata":{"id":"CBYKrNu2YLLq"},"source":["# ARIMA model\n","train_ar = train_data['Passengers'].values\n","test_ar = test_data['Passengers'].values\n","\n","history = [x for x in train_ar]\n","\n","predictions = list()\n","for t in range(len(test_ar)):\n","    # create arima model with p, d, q\n","    model = ARIMA(history, order=(1,1,1))\n","    '''order = (1, 1, 1) indicates a lag value of 1 for autoregression, a difference order of 1 to make the time series stationary and a moving average model of 1'''   \n","    model_fit = model.fit()        \n","    # one-step forecast\n","    output = model_fit.forecast()     \n","    yhat = output[0]\n","    # append forecast to predictions\n","    predictions.append(yhat)\n","    obs = test_ar[t]\n","    # append true observation for forecast to history\n","    history.append(obs)\n","\n","# RMSE value for test data and its predictions\n","error = np.sqrt(mean_squared_error(test_ar, predictions))\n","print('RMSE value: %.3f' % error)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6z4_RyrvcBP1"},"source":["### Plot the predictions"]},{"cell_type":"code","metadata":{"id":"27WeG7T-tb3b"},"source":["# Visualize predictions along with train and test data\n","plt.figure(figsize=(17,7))\n","plt.plot(train_data['Passengers'], 'green', color='blue', label='Training Data')\n","plt.plot(test_data.index, predictions, color='green',marker='o', linestyle='dashed', label='Predicted Data')\n","plt.plot(test_data.index, test_data['Passengers'], color='red', label='Actual Test Data')\n","plt.legend()\n","plt.xlabel(\"Time (Months)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TipVuyQMATf9"},"source":["### SARIMA "]},{"cell_type":"code","metadata":{"id":"MP5nCjJzLKnL"},"source":["# Re-visualize ACF for train data\n","p1 = plot_acf(trainData_diff, lags=25)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uh2aZtmyLWGn"},"source":["From the above ACF plot, we can see there is a positive correlation at lags = 12 and 24. This represents that seasonality is there in time series."]},{"cell_type":"markdown","metadata":{"id":"0qF6811trPsA"},"source":["Seasonal Autoregressive Integrated Moving Average, SARIMA or **Seasonal ARIMA**, is an extension of ARIMA that explicitly supports univariate time series data with a seasonal component. It adds three new hyperparameters to specify the autoregression (AR), differencing (I) and moving average (MA) for the seasonal component of the series, as well as an additional parameter for the period of the seasonality.\n","\n","Configuring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series. There are three trend elements that require configuration.\n","\n","They are the same as the ARIMA model; specifically:\n","\n","* **p:** Trend autoregression order.\n","* **d:** Trend difference order.\n","* **q:** Trend moving average order.\n","\n","There are four seasonal elements that are not part of ARIMA that must be configured; they are:\n","\n","* **P:** Seasonal autoregressive order.\n","* **D:** Seasonal difference order.\n","* **Q:** Seasonal moving average order.\n","* **S:** The number of time steps for a single seasonal period.\n","\n","Before applying SARIMA, we will check the seasonality of given time series data."]},{"cell_type":"code","metadata":{"id":"XQrzSkv8rPsa"},"source":["# Make the index a date time format\n","df.reset_index(inplace=True)\n","df.Month = pd.to_datetime(df.Month)\n","df.set_index('Month',inplace=True)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a7LeblHsrPsa"},"source":["# Split the data into ~80% train and ~20% test\n","train_ar = df['Passengers'][:int(len(df['Passengers'])*0.8)]\n","test_ar = df['Passengers'][int(len(df['Passengers'])*0.8):]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ayglsDHCrPsb"},"source":["# Differencing to oberserve seasonality for 1 year\n","data_diff_seas = df.diff(12)\n","data_diff_seas = data_diff_seas.dropna()\n","dec = seasonal_decompose(data_diff_seas)\n","dec.plot()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GvnYQafIrPsb"},"source":["In the above plot, we can see that seasonality exists for every year (for 12 entries). So the seasonal order value, $S = 12$."]},{"cell_type":"markdown","metadata":{"id":"tCjgyNWzTkpg"},"source":["**Checking for seasonal differenced series stationarity and selecting (P,D,Q) values**\n","\n","The seasonal difference can be computed by shifting the data by the number of rows per season (in our case 12 months per year) and subtracting them from the previous season. This is not the first seasonal difference. If we get that the seasonal difference is stationary then the D value will be 0. If not then we will compute the seasonal first difference. In other words,\n","\n","seasonal difference(T) = observation(T) – observation(T - 12)\n","\n","seasonal first difference(T) = seasonal difference(T) – seasonal difference(T - 1)"]},{"cell_type":"code","metadata":{"id":"HqlOJduEScfX"},"source":["# Visualize seasonally differenced time series\n","plt.plot(df['Passengers'].diff(12).dropna())\n","plt.xlabel(\"Time (Months)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4EqN9fCNTHBU"},"source":["# Check for stationarity\n","get_stationarity(df['Passengers'].diff(12).dropna())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WR1VXa5kUH2X"},"source":["In the above plot, the mean and standard deviation is changing with time hence the series is not stationary.\n","\n","Now considering seasonal first difference to convert it to stationary series."]},{"cell_type":"code","metadata":{"id":"G_Eppa5VTBe4"},"source":["# Visualize first order difference of seasonally differenced time series\n","plt.plot(df['Passengers'].diff(12).diff(1).dropna())\n","plt.xlabel(\"Time (Months)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7GtVtIxhTUSq"},"source":["# Check for stationarity\n","get_stationarity(df['Passengers'].diff(12).diff(1).dropna())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R_9zZCjHVbjI"},"source":["In the above plot, now the mean and standard deviation is more stable than before. Considering this as stationary time series, we have $S= 12$ and $D=1$.\n","\n","Now, for $P$ and $Q$ values we can see PACF and ACF plots of seasonal first difference series."]},{"cell_type":"code","metadata":{"id":"IEcfaXu_PClt"},"source":["# Visualize PACF and ACF plots \n","fig, ax = plt.subplots(1,2, figsize=(20, 5))\n","fig = plot_pacf(df['Passengers'].diff(12).diff(1).dropna(), lags=22, ax=ax[0]);ax[0].set_xlabel(\"Lag\");ax[0].set_ylabel(\"PACF\");\n","fig = plot_acf(df['Passengers'].diff(12).diff(1).dropna(), lags=22, ax=ax[1]);ax[1].set_xlabel(\"Lag\");ax[1].set_ylabel(\"ACF\"); \n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NLmYMyMlX0k0"},"source":["From the above plots, we can select P and Q values as 1 and 1 respectively. Also, as there is a sharp cut-off in both the plots and the autocorrelation is negative for ACF plot we increase the Q value by one. Now, we have $P=1$ and $Q=2$."]},{"cell_type":"markdown","metadata":{"id":"3CH_35rYrPsb"},"source":["SARIMA Model order [(p,d,q)x(P,D,Q,S)]\n","\n","* (p,d,q) = This is order we can choose from above ARIMA model (1, 1, 1).\n","* (P,D,Q) = This order we calculated above using the same techniques as ARIMA (1, 1, 2).\n","* S = Seasonal order = 12.\n","\n","**Metric:** A metric that can be used to compare the relative quality of each model is AIC (Akaike information criterion), which is an estimator of out-of-sample prediction error; a lower AIC score indicates a more predictive model.\n","\n","Let's create a SARIMA model using the above order."]},{"cell_type":"code","metadata":{"id":"l8WgG7BDY58W"},"source":["warnings.filterwarnings(\"ignore\")\n","# Define hyperparameters for ARIMA model\n","p, d, q, P, D, Q, S = 1, 1, 1, 1, 1, 2, 12\n","# Fit the SARIMA model\n","mod = sm.tsa.statespace.SARIMAX(train_ar, order=(p,d,q),seasonal_order=(P,D,Q,S), \n","                                enforce_stationarity=False, enforce_invertibility=False)\n","result = mod.fit()\n","print('ARIMA{}x{} - AIC:{}'.format((p,d,q), (P,D,Q), result.aic))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uoU3G8TJZ5aa"},"source":["We can also fine-tune the model if needed by trying different order values and check for their AIC scores."]},{"cell_type":"code","metadata":{"id":"22dl2-7VrPsb"},"source":["# Define multiple hyperparameters values for ARIMA model\n","p, d, q, P, D, Q, S = 1, (0,1), 1, (1,2), 1, 2, 12\n","# Fit the SARIMA model\n","for each_d in d:\n","    for each_P in P:\n","        mod = sm.tsa.statespace.SARIMAX(train_ar, order=(p,each_d,q),seasonal_order=(each_P,D,Q,S), \n","                                        enforce_stationarity=False, enforce_invertibility=False)\n","        results = mod.fit()\n","        print('ARIMA{}x{} - AIC:{}'.format((p,each_d,q), (each_P,D,Q), results.aic))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vycl7G-rPsb"},"source":["### Predict the test data and plot"]},{"cell_type":"code","metadata":{"id":"hQWpjsqYrPsb"},"source":["# Prediction with SARIMA model\n","sarima_pred = result.predict(start=test_ar.index[0], end=test_ar.index[-1])\n","\n","# Visualize the results\n","plt.figure(figsize=(10, 7))\n","plt.plot(train_ar.index, train_ar.values, label='Train')\n","plt.plot(test_ar.index, test_ar.values, label='Test', color='r')\n","plt.plot(sarima_pred.index, sarima_pred, label='SARIMA', color='k')\n","plt.legend(loc='best', fontsize='xx-large')\n","plt.xlabel(\"Time (Months)\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"81CXcuBiaUvs"},"source":["**References for Further Reading:**\n","\n","[ACF and PACF plots](https://towardsdatascience.com/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8#:~:text=ACF%20is%20an%20(complete)%20auto,related%20with%20its%20past%20values)\n","\n","[ARIMA model](https://towardsdatascience.com/machine-learning-part-19-time-series-and-autoregressive-integrated-moving-average-model-arima-c1005347b0d7)\n","\n","[Interpret ACF and PACF plots](https://towardsdatascience.com/time-series-from-scratch-autocorrelation-and-partial-autocorrelation-explained-1dd641e3076f)\n","\n"]}]}