{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CDS-TSM-03-AS-7-7.ipynb","provenance":[],"collapsed_sections":["-Be2S_PBH6Qj"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"WFBl3DsqB3AE"},"source":["# Advanced Certification Program in Computational Data Science\n","\n","##  A program by IISc and TalentSprint\n","\n","### Assignment 7: Demand Forecasting"]},{"cell_type":"markdown","metadata":{"id":"maritime-miami"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"95F1ym6qB8VU"},"source":["At the end of the experiment, you will be able to :\n","\n","* perform EDA and analyze the feature importance of sales data\n","* prepare the data for forecasting task\n","* solve the problem of Demand Planning Optimization\n","* implement ARIMA model to forecast the demands of a sales"]},{"cell_type":"markdown","metadata":{"id":"-Be2S_PBH6Qj"},"source":["## Information\n","\n","Sales forecasting is an approach retailers use to anticipate future sales by analyzing past sales, identifying trends, and projecting data into the future. The simplest version of a sales forecast will look at sales in Store A during last year, assume a continuation of some multi-year trend for Store A (e.g. some percentage of growth or decline in sales), and project forward to predict sales in Store A this year.\n","\n","Sales forecasting has been the standard approach to retailing from the beginning of the industry itself.\n","\n","Although modern retailers have more data than ever (as well as new tools and business intelligence dashboards), and can easily leverage more advanced and accurate forms of forecasting, sales-based forecasting is still the backbone of most retail organizations.\n","\n","some benefits of having an accurate sales forecast\n","\n","* Improved decision-making about the future\n","* Reduction of sales pipeline and forecast risks\n","* Alignment of sales quotas and revenue expectations\n","* Reduction of time spent planning territory coverage and setting quota assignments\n","* Benchmarks that can be used to assess trends in the future\n","* Ability to focus a sales team on high-revenue, high-profit sales pipeline  opportunities, resulting in improved win rates\n","\n","\n","This Demand optimization can reduce operational costs by:\n","\n","**Inventory Optimization:** matching store inventory with actual needs to reduce storage space needed (Rental Costs)\n","\n","**Replenishment Optimization:** optimizing replenishment quantity per order to minimize the number of replenishments between warehouse and stores (Warehousing & Transportation Costs)\n"]},{"cell_type":"markdown","metadata":{"id":"NVcis_FNIrFY"},"source":["## Dataset\n","\n","Store Item Demand Forecasting dataset consists of 5 years of store-item sales data from the year 2013 to 2017. It contains sales for 50 different items at 10 different stores with 913000 observations and 4 columns Date Store, item, sales.\n","\n","Attributes information:\n","\n","**Date:** Date starting from 2013 to 2017\n","\n","**Store:** Store indicating with a number\n","\n","**Item:** Item  in a store\n","\n","**Sales:** No.of sales "]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Download dataset\n","!pip -qq install pmdarima\n","!wget https://raw.githubusercontent.com/insaid2018/Term-3/master/Projects/Store_Item_demand_forecast.csv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YRIzMfgjwcAp"},"source":["### Import required packages "]},{"cell_type":"code","metadata":{"id":"34ccJLc6amaQ"},"source":["import warnings\n","warnings.simplefilter('ignore')\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","from matplotlib import pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from statsmodels.tsa.stattools import adfuller\n","import statsmodels.api as sm\n","from scipy import stats\n","from scipy.stats import normaltest\n","import pmdarima as pm"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fmyc9n2Kaq1W"},"source":["### Load the data"]},{"cell_type":"code","metadata":{"id":"evVDmG9xClM9"},"source":["dataset  =  pd.read_csv('Store_Item_demand_forecast.csv')\n","dataset.head(), dataset.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CtDfR5rtq1OU"},"source":["### Data Pre-processing\n","\n","* Convert the Date column and separate Date, Month and Year columns to add more features including day of year,  day of week and week of year.\n","\n","* Create a new feature $Day^{year}$ using Day of year and year\n","\n"]},{"cell_type":"code","metadata":{"id":"bKO_VP0iquZO"},"source":["# DATES FEATURES\n","def date_features(df):\n","    # Date Features\n","    dataset = df\n","    df['date'] = pd.to_datetime(dataset['date'])\n","    df['year'] = dataset.date.dt.year\n","    df['month'] = dataset.date.dt.month\n","    df['day'] = dataset.date.dt.day\n","    df['dayofyear'] = dataset.date.dt.dayofyear\n","    df['dayofweek'] = dataset.date.dt.dayofweek\n","    df['weekofyear'] = dataset.date.dt.weekofyear\n","    \n","    # Additionnal Data Features\n","    df['day^year'] = np.log((np.log(dataset['dayofyear'] + 1)) ** (dataset['year'] - 2000))\n","    \n","    # Drop date\n","    df.drop('date', axis=1, inplace=True)\n","    return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXHMKAKjrKO2"},"source":["# Dates Features for Train\n","train = date_features(dataset)\n","train.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1mcTf_DQMeuH"},"source":["### ARIMA Analysis"]},{"cell_type":"markdown","metadata":{"id":"s10cHNT1HamP"},"source":["#### Decomposition\n","\n","Time series data can be broken down into four core components: the average value, a trend (i.e. an increasing mean), seasonality (i.e. a repeating cyclical pattern), and a residual (random noise). Trends and seasonality are not always present in time dependent data. The residual is what’s left over after trends and seasonality are removed. Time series models assume that the data is stationary and only the residual component satisfies the conditions for stationarity.\n","\n","Python’s statsmodels library has a method for time series decomposition called `seasonal_decompose()`.\n","\n","* Apply decomposition and plot the trend, seasonality and residual"]},{"cell_type":"code","metadata":{"id":"VMcbh3Sp82Pz"},"source":["# Decomposition of time series\n","result = seasonal_decompose(train['sales'], model='additive', freq=365)\n","\n","fig = plt.figure()  \n","fig = result.plot()  \n","fig.set_size_inches(15, 12)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9QIGlZhN-2QE"},"source":["# Let's see it for 1000 records\n","result = seasonal_decompose(train['sales'][:1000], model='additive', freq=365)\n","\n","fig = plt.figure()  \n","fig = result.plot()  \n","fig.set_size_inches(15, 12)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3wtOuH7jtqG"},"source":["#### Stationarity\n","\n","If a time series is stationary and has a particular behavior over a given time interval, then it is safe to assume that it will have same behavior at some later point in time. Most statistical modeling methods assume or require the time series to be stationary.\n","\n","**What is variance?**\n","\n","Variance is a measurement of the spread between numbers in a data set. The variance measures how far each number in the set is from the mean. The square root of variance is the standard deviation.\n","\n","**How to check Whether data is Stationary or not?**\n","\n","There are two ways we can check the stationarity of a time series. The first is by looking at the data. By visualizing the data it should be easy to identify a changing mean or variation in the data. For a more accurate assessment, there is the Dickey-Fuller test. If the ‘Test Statistic’ is greater than the ‘Critical Value’ then the time series is stationary.\n","\n","Implement a function which takes a timeseries as input and perform:\n","  * Dicky fuller test and check p-value\n","  * Rolling mean and rolling standard and plot it with a graph"]},{"cell_type":"code","metadata":{"id":"hCl7U_E29tDb"},"source":["# Create function to test stationarity\n","def test_stationarity(timeseries, window = 12):\n","\n","    # Determing rolling statistics\n","    rolmean = timeseries.rolling(window).mean()\n","    rolstd = timeseries.rolling(window).std()\n","\n","    # Plot rolling statistics:\n","    fig = plt.figure(figsize=(12, 8))\n","    orig = plt.plot(timeseries, color='blue',label='Original')\n","    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n","    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n","    plt.legend(loc='best')\n","    plt.title('Rolling Mean & Standard Deviation')\n","    plt.show()\n","test_stationarity(train['sales'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LZI7B6w2gRWi"},"source":["Differencing the data with shift 1 and see the plot"]},{"cell_type":"code","metadata":{"id":"zXdfcp0JdboD"},"source":["# differencing the data\n","diff = train.sales[:10000].diff(1).fillna(0)\n","test_stationarity(diff)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"51Gq2FTekqiV"},"source":["#### Dickey-Fuller test"]},{"cell_type":"code","metadata":{"id":"FG43gB1VkeGN"},"source":["# Perform Dickey-Fuller test:\n","print('Results of Dickey-Fuller Test:')\n","dftest = adfuller(diff, autolag='AIC', maxlag = 20 )\n","dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n","for key,value in dftest[4].items():\n","    dfoutput['Critical Value (%s)'%key] = value\n","pvalue = dftest[1]\n","if pvalue < 0.01:\n","    print('p-value = %.4f. The series is likely stationary.' % pvalue)\n","else:\n","    print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n","\n","print(dfoutput)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGP-Mgd5E1jr"},"source":["### Autoregression Intuition\n","\n","Consider a time series that was generated by an autoregression (AR) process with a lag of k.\n","\n","We know that the ACF describes the autocorrelation between an observation and another observation at a prior time step that includes direct and indirect dependence information.\n","\n","This means we would expect the ACF for the AR(k) time series to be strong to a lag of k and the inertia of that relationship would carry on to subsequent lag values, trailing off at some point as the effect was weakened.\n","\n","We know that the PACF only describes the direct relationship between an observation and its lag. This would suggest that there would be no correlation for lag values beyond k.\n","\n","This is exactly the expectation of the ACF and PACF plots for an AR(k) process.\n","\n","### Moving Average Intuition\n","\n","Consider a time series that was generated by a moving average (MA) process with a lag of k.\n","\n","Remember that the moving average process is an autoregression model of the time series of residual errors from prior predictions. Another way to think about the moving average model is that it corrects future forecasts based on errors made in recent forecasts.\n","\n","We would expect the ACF for the MA(k) process to show a strong correlation with recent values up to the lag of k, then a sharp decline to low or no correlation. By definition, this is how the process was generated.\n","\n","For the PACF, we would expect the plot to show a strong relationship to the lag and a trailing off of correlation from the lag onwards.\n","\n","Again, this is exactly the expectation of the ACF and PACF plots for an MA(k) process.\n","\n","**Summary:** \n","From the autocorrelation plot we can tell whether or not we need to add MA terms. From the partial autocorrelation plot, we know we need to add AR terms.\n","\n","References:\n","https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/"]},{"cell_type":"markdown","metadata":{"id":"_B6OZawO4nG2"},"source":["### How to interpret ACF and PACF plots\n","\n","For time series models such as Auto Regression (AR), Moving Averages (MA), or their combinations (ARMA), we need to specify one or more parameters (eg. p, q). These can be obtained by looking at ACF (to infer q parameter) and PACF (to infer p parameter) plots.\n","\n","In a nutshell:\n","\n","* If the ACF plot declines gradually and the PACF drops instantly, use Auto Regressive model \n","* If the ACF plot drops instantly and the PACF declines gradually, use Moving Average model \n","* If both ACF and PACF decline gradually, combine Auto Regressive and Moving Average models (ARMA).\n","* If both ACF and PACF drop instantly (no significant lags), it’s likely we won’t be able to model the time series."]},{"cell_type":"code","metadata":{"id":"VgcKqpaA-Bc3"},"source":["# Visualize ACF and PACF plots\n","diff = train.sales[:10000].diff(1).fillna(0)\n","fig = plt.figure(figsize=(12,8))\n","ax1 = fig.add_subplot(211)\n","fig = sm.graphics.tsa.plot_acf(diff, lags=40, ax=ax1)\n","ax2 = fig.add_subplot(212)\n","fig = sm.graphics.tsa.plot_pacf(diff, lags=40, ax=ax2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"czCRHwDqHPjs"},"source":["Create and fit the ARIMA model:"]},{"cell_type":"code","metadata":{"id":"clERqC_b-VpG"},"source":["arima_mod6 = sm.tsa.ARIMA(train.sales[:10000], (8,1,1)).fit(disp=False)\n","arima_mod6.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uI5s5cMNmqFN"},"source":["The summary attribute that results from the output of ARIMA returns a significant amount of information, \n","\n","* The coef column shows the weight (i.e. importance) of each feature and how each one impacts the time series. \n","* The P>|z| column informs us of the significance of each feature weight. Here, each weight has a p-value lower or close to 0.05, so it is reasonable to retain all of them in our model."]},{"cell_type":"markdown","metadata":{"id":"gOGm-y-XLmH2"},"source":["Reading ACF and PACF plots is however still challenging, and using grid search to find optimal parameter values is sometimes useful. From grid search, we can obtain an optimal parameter combination that has the lowest error (such as MAPE) or lowest general quality estimator (such as AIC).\n"]},{"cell_type":"markdown","metadata":{"id":"PI697ZVpLkLE"},"source":["### Identifying optimal parameters using `auto_arima`\n","\n","pmdarima‘s `auto_arima` function is extremely useful when building an ARIMA model as it helps us identify the most optimal p,d,q parameters and return a fitted ARIMA model.\n","\n","**Using pmdarima for Auto ARIMA model:**\n","\n","In the previous method, checking for stationarity, making data stationary if necessary, and determining the values of p and q using the ACF/PACF plots can be time-consuming and less efficient. Using pmdarima’s auto_arima() function makes this task easier for us by eliminating steps for implementing an ARIMA model.\n","\n","\n","#### auto_arima\n","\n","The auto-ARIMA process seeks to identify the most optimal parameters for an ARIMA model, settling on a single fitted ARIMA model.\n","\n","Auto-ARIMA works by conducting differencing tests (i.e., Kwiatkowski–Phillips–Schmidt–Shin, Augmented Dickey-Fuller or Phillips–Perron) to determine the order of differencing, d, and then fitting models within ranges of defined start_p, max_p, start_q, max_q ranges. If the seasonal optional is enabled, auto-ARIMA also seeks to identify the optimal P and Q hyper- parameters after conducting the Canova-Hansen to determine the optimal order of seasonal differencing, D.\n","\n","In order to find the best model, auto-ARIMA optimizes for a given information_criterion, one of (‘aic’, ‘aicc’, ‘bic’, ‘hqic’, ‘oob’) (Akaike Information Criterion, Corrected Akaike Information Criterion, Bayesian Information Criterion, Hannan-Quinn Information Criterion, or “out of bag”–for validation scoring–respectively) and returns the ARIMA which minimizes the value.\n","\n","**Note:** pmdarima may take a long time to find the terms for the dataset, subset of datapoints are used to minimize the execution time, however, we can experiment with a subset of the dataset.\n","\n","\n","From the ACF and PACF plot, we can observe that data is exhibiting same pattern for each 7 lags. (seasonal term 'm' = 7)"]},{"cell_type":"code","metadata":{"id":"-3y4FVXEuE7c"},"source":["# Fit the model\n","model = pm.auto_arima(train.sales[:1000], seasonal=True, m=7) # m is seasonal term\n","\n","# Make forecast\n","forecasts = model.predict()  # predict 1 step into the future"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T0jRkqQb0ib5"},"source":["# Model summary\n","model.summary()"],"execution_count":null,"outputs":[]}]}