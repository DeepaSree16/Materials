{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"28_Numerical_Optimization.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"P6ijzT0f74Vn"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"e2f0jt0oTtC-"},"source":[" At the end of the experiment, you will be able to:\n","\n","* understand the steepest descent algorithm with exact line search \n","method\n","\n","* understand the Multivariate Newton Optimization method "]},{"cell_type":"markdown","source":["**Line search** method is an iterative approach to find a local minimum of a multidimensional nonlinear function using the function's gradients. It computes a search direction and then finds an acceptable step length that satisfies certain standard conditions. \n","\n","\n","Line search method can be categorized into exact and inexact methods. The exact method, as in the name, aims to find the exact minimizer at each iteration; while the inexact method computes step lengths to satisfy conditions including Wolfe and Goldstein conditions. Line search and trust-region methods are two fundamental strategies for locating the new iterate given the current point. With the ability to solve the unconstrained optimization problem, line search is widely used in many cases including machine learning, game theory and other fields."],"metadata":{"id":"y_QLZP4B4WOP"}},{"cell_type":"markdown","source":["### Exact Search\n","\n","#### Steepest Descent Method\n","\n","Given the intuition that the negative gradient $ - \\nabla f_{k}$ can be an effective search direction, steepest descent follows the idea and establishes a systematic method for minimizing the objective function. Setting $ - \\nabla f_{k}$ as the direction, steepest descent computes the step-length $\\alpha_{k}$ by minimizing a single-variable objective function. More specifically, the steps of Steepest Descent Method are as follows:\n","\n","**Steepest Descent Algorithm:**\n","\n","\n","![Img](https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/Line_Search.png)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"Kzq3Ozb85cSp"}},{"cell_type":"markdown","metadata":{"id":"7OiFi8nj77AW"},"source":["### Setup Steps:"]},{"cell_type":"code","metadata":{"id":"2YzfoPvJDiTX"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rEzlYL4CDrmE"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","from warnings import filterwarnings\n","filterwarnings('ignore')\n","\n","ipython = get_ipython()\n","  \n","notebook= \"M1_AST_10_Exact_Line_Search_&_Newton_Method_C\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\") \n","\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","    \n","    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://dlfa.iisc.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","  \n","  \n","# def getWalkthrough():\n","#   try:\n","#     if not Walkthrough:\n","#       raise NameError\n","#     else:\n","#       return Walkthrough\n","#   except NameError:\n","#     print (\"Please answer Walkthrough Question\")\n","#     return None\n","  \n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","  \n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer1():\n","  try:\n","    if not Answer1:\n","      raise NameError \n","    else: \n","      return Answer1\n","  except NameError:\n","    print (\"Please answer Question 1\")\n","    return None\n","\n","def getAnswer2():\n","  try:\n","    if not Answer2:\n","      raise NameError \n","    else: \n","      return Answer2\n","  except NameError:\n","    print (\"Please answer Question 2\")\n","    return None\n","  \n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup() \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Importing required packages"],"metadata":{"id":"8nhfPrB-2Fgy"}},{"cell_type":"code","source":["import numpy as np                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n","import matplotlib.pyplot as plt"],"metadata":{"id":"LKuwSZpFNOgU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["1. Given the objective function is $ x_{1} - x_{2} + 2 x_{1}  x_{2} + 2 x_{1}^2 +  x_{2}^2  $"],"metadata":{"id":"Gq-BRAEV2PAo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ycXeuYyP352i"},"outputs":[],"source":["# Define objective function\n","def f(x):\n","    x1 = x[0]\n","    x2 = x[1]\n","    obj = x1 - x2 + 2.0 * x1 * x2 + 2 * x1**2 + x2**2\n","    return obj\n","    \n","# Define objective gradient\n","def dfdx(x):\n","    x1 = x[0]\n","    x2 = x[1]\n","    grad = []\n","    grad.append(1 + 2 * x2 + 4 * x1)\n","    grad.append(-1.0 + 2 * x1 + 2.0 * x2)\n","    return grad\n","\n","# Exact 2nd derivatives (hessian)\n","H = [[2.0, -2.0],[-2.0, 8.0]]\n","\n","# Start location/ Starting point\n","# x_start = [-3.0, 2.0]\n","x_start = [0.0, 0.0]"]},{"cell_type":"code","source":["# Design variables at mesh points\n","i1 = np.arange(-4.0, 4.0, 0.1)\n","i2 = np.arange(-4.0, 4.0, 0.1)\n","x1_mesh, x2_mesh = np.meshgrid(i1, i2)\n","#f_mesh = x1_mesh**2 - 2.0 * x1_mesh * x2_mesh + 4 * x2_mesh**2\n","f_mesh = x1_mesh - x2_mesh + 2.0 * x1_mesh * x2_mesh + 2 * x1_mesh**2 + x2_mesh**2\n","\n","# Create a contour plot\n","plt.figure()\n","\n","# Specify contour lines\n","lines = range(2,52,2)\n","\n","# Plot contours\n","CS = plt.contour(x1_mesh, x2_mesh, f_mesh, lines)\n","\n","# Label contours\n","plt.clabel(CS, inline=1, fontsize=10)\n","\n","# Add some text to the plot\n","plt.title(\"f(x) = x1 - x2 + 2.0 * x1 * x2 + 2 * x1**2 + x2**2\")\n","plt.xlabel(\"x1\")\n","plt.ylabel(\"x2\");"],"metadata":{"id":"y1ooxjdw96Pj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Steepest Descent Method"],"metadata":{"id":"EgHGgjZxHpFT"}},{"cell_type":"code","source":["# Number of iterations\n","n = 10\n","\n","# Use this alpha for every line search\n","from sympy import symbols, diff, solve\n","\n","alpha = symbols('a')\n","\n","# Initialize xs\n","xs = np.zeros((n+1, 2))\n","xs[0] = x_start\n","\n","# Get gradient at start location (df/dx or grad(f))\n","epsilon = 0.0025\n","for i in range(n):\n","    \n","    # gs = dfdx(xs[i])\n","    # Compute search direction and magnitude (dx)\n","    # with dx = - grad but no line searching\n","    print(\"Iteration:\", i)\n","    x = xs[i] - np.dot(alpha, dfdx(xs[i]))\n","    print(f\"x : {x}\")\n","    print(\"f(x)\", f(x))\n","\n","    if f(x).is_constant():\n","        print(\"Optimum value of alpha is:\", sol)\n","        print('')\n","        print(\"The derived optimal solution is:\", xs[i])\n","        print('')\n","        print(\"The minima of the given function is:\", f(xs[i+1]))\n","        print('')\n","        print(\"The total number iterations taken is:\", i+1)\n","        break\n","\n","    fx1 = f(x).diff(alpha)\n","    print(\"f(x1) = \", fx1)\n","\n","    sol = solve(fx1)\n","    print(\"alpha\", sol)\n","\n","    xs[i+1] = xs[i] - np.dot(sol[0], dfdx(xs[i]))\n","    print(\"New values =\", xs[i+1])\n","\n","    vec_norm = np.linalg.norm(dfdx(xs[i]))\n","    print(\"Vector norm:\", vec_norm)\n","    print('')\n","    if (vec_norm < epsilon):\n","        print(\"Optimum value of alpha is:\", sol)\n","        print('')\n","        print(\"The derived optimal solution is:\", xs[i+1])\n","        print('')\n","        print(\"The minima of the given function is:\", f(xs[i+1]))\n","        print('')\n","        print(\"The total number iterations taken is:\", i+1)\n","        break\n","\n","    print('')\n","    plt.plot(xs[:,0],xs[:,1], 'g-o')"],"metadata":{"id":"n1kX6tUTQngN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Multivariate Newton Optimization method"],"metadata":{"id":"-ERLOq0UX6up"}},{"cell_type":"code","source":["from sympy import Derivative\n","\n","x1, x2 = symbols('x1 x2')\n","expr = x1 - x2 + 2*x1*x2 + 2* x1**2 + x2**2\n","print(f\"Expression : {expr} \")"],"metadata":{"id":"sIS4odrwACab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In mathematics, the Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field.\n","\n","\n","  \\begin{equation*}H(x) = \\begin{bmatrix} f_{xx}\\,\\,\\, f_{xy} \\\\ f_{yx} \\,\\,\\,f_{yy}\\end{bmatrix} =   \\begin{bmatrix} \\frac{∂f} {∂x^{2} } \\frac{∂f} {∂x∂y}\\\\  \\frac{∂f}  {∂y∂x } \\frac{∂f} {∂x∂y}  \\end{bmatrix}\n","  \\end{equation*}"],"metadata":{"id":"EmM0Ca-g4vHU"}},{"cell_type":"code","source":["def Hessian_matrix(x01, x02):\n","  dfx = Derivative(expr, x1) \n","  dfyx = Derivative(dfx.doit(),x2)\n","  dfxx = Derivative(dfx.doit(),x1)\n","  dfy = Derivative(expr, x2) \n","  dfxy = Derivative(dfy.doit(),x1)\n","  dfyy = Derivative(dfy.doit(), x2)  \n","  DDfx = dfxx.doit().subs([(x1, x01), (x2, x02)])\n","  DDfxy = dfxy.doit().subs([(x1, x01), (x2, x02)])  \n","  DDfyx = dfyx.doit().subs([(x1, x01), (x2, x02)]) \n","  DDfy = dfyy.doit().subs([(x1, x01), (x2, x02)]) \n","  Hessian = np.array([[DDfx,DDfxy], [DDfyx, dfyy.doit().subs([(x1, x01), (x2, x02)]) ]])\n","  Hessian = Hessian.astype(float)\n","  inverse = np.linalg.inv(Hessian) \n","  Hxy = inverse\n","  return Hxy"],"metadata":{"id":"qNw74ltseXLI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Hessian = Hessian_matrix(0.1,0.2)\n","Hxy = Hessian\n","print(Hxy)"],"metadata":{"id":"4vCamUEVgG9X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The gradient of a function is a vector field. It is obtained by applying the vector operator V to the scalar function f(x, y). Such a vector field is called a gradient (or conservative) vector field.\n","\n","\\begin{equation*} \\bigtriangledown f = \\begin{bmatrix} \\frac {∂f}   {\\partial x} \\\\ \\frac {∂f}   {\\partial y}\\end{bmatrix} \\end{equation*}"],"metadata":{"id":"TtUWbP3DLQpJ"}},{"cell_type":"code","source":["def gradientf():\n","  gradf = []\n","  dfx = Derivative(expr, x1) \n","  dfy = Derivative(expr, x2) \n","  return dfx, dfy"],"metadata":{"id":"zI_a9pwsDupK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Newton Optimization for Multi Variate equation\n","\n","\n","\n","\n","\n","\n"," \\begin{equation*} X^{new} = X^{old} - H^{-1}  \\bigtriangledown f     \\end{equation*}  \n"," \\begin{equation*} where \\,\\, X = \\begin{bmatrix} x1 \\\\ x2 \\end{bmatrix}  \\end{equation*}"],"metadata":{"id":"H-_OwHMKMphi"}},{"cell_type":"code","source":["i = 10\n","\n","x0 = np.array([0.001,0.002])\n","print(type(x0[0]))\n","for m in range(i):\n","  print(f\"\\nIteration {m+1}\")\n","  gradientoff = []\n","  inverse = Hessian_matrix(x0[0], x0[1])\n","  \n","  print(f\"x0 : {x0}\")\n","  gradf = gradientf()\n","  \n","  gradientoff.append(gradf[0].doit().subs([(x1, x0[0]), (x2, x0[1])]))\n","  gradientoff.append(gradf[1].doit().subs([(x1, x0[0]), (x2, x0[1])]))\n","  \n","  h = np.matmul(Hxy, gradientoff)\n","  if h.all() == 0 :\n","    print(f\"Objective function converged\\nConverged in {m+1} iterations\")\n","    print(f\"Points at which the objective function is converged : {x0}\")\n","    break\n","\n","  xn = x0 - h\n","  # print(f\"xn : {xn}\")\n","  x0 = xn"],"metadata":{"id":"key8_vZgtfgj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VHfHdGCP_n6Y"},"source":["### Please answer the questions below to complete the experiment:\n","\n","\n"]},{"cell_type":"markdown","source":["\n","Q1. Suppose we are trying to minimize the function $f(x, y) = 4x^{2} – 4xy + 2y^{2}$. Starting from $x^{(0)} = (2, 3)^{T}$, compute the step size $\\alpha$ in the first iteration of steepest descent procedure with exact line search and also calculate $x^{(1)}$.\n","\n","Hint: Please solve the problem manually and if you want to use the exact line search algorithm, do make the necessary changes to the code given in the assignment.\n","\n","**Options**\n","\n","A. $\\alpha = 0.5$,  $x^{(1)} = (0, 1)^T$\n","\n","B. $\\alpha = 0.5$,  $x^{(1)} = (1, 0)^T$\n","\n","C. $\\alpha = 0.5$, $x^{(1)} = (0, -1)^T$\n","\n","D. $\\alpha = 0.5$, $x^{(1)} = (-1, -1)^T$\n","\n"],"metadata":{"id":"3aY0mwlsUP14"}},{"cell_type":"code","metadata":{"id":"kmvdJ4aNmGjR"},"source":["#@title Q.1. Solve the above problem { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer1 = \"\" #@param [\"\",\"Only A\",\"Only B\",\"Only C\", \"Only D\"]\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NacSL0FsDmv_"},"source":["#### Consider the following statements about Newton's Algorithm and answer Q.2.\n","\n","A. Convergence of the algorithm depends heavily on the starting point.\n","\n","B. The Hessian needs to be invertible at every iterate $x_{k}$.\n","\n","C. The Hessian need not be invertible at every iterate $x_{k}$.\n"]},{"cell_type":"code","metadata":{"id":"A3Wd-R9CDmv_"},"source":["#@title Q.2. Which of the above statements is/are drawbacks of Newton's Algorithm? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Answer2 = \"\" #@param [\"\",\"Only A\",\"Only B\",\"Only C\",\"Both A and C\",\"Both A and B\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4tpZisWbDZUI"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t1Z0w03yDZUJ"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LkiuHlpxDZUJ"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1iTafi4DZUJ"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7i9Lk8UjDZUK"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"i9ag4Eo0DZUK"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}