{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"10_Gradient_Descent.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"a3-AYqMRqHOD"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"myNyvBNXVgkX"},"source":["At the end of the experiment,  you will be able to :\n","\n","* Understand the intution of Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"XsuH046p_d1N"},"source":["## Overview \n","\n","In general terms Gradient means slope or slant of a surface. So gradient descent means descending a slope to reach the lowest point on that surface. Gradient Descent aims to minimize the cost function, a function reaches its minimum value when the slope is equal to 0. Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the minimum point of that function."]},{"cell_type":"markdown","metadata":{"id":"UQmzuXIChorp"},"source":["### Import required packages"]},{"cell_type":"code","metadata":{"id":"238GaEpo0MlR"},"source":["import matplotlib.pyplot as plt\n","from sympy import *\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuYgUJJy6GeU"},"source":["### Intution behind Gradient Descent\n","\n","Mathematical concepts before understanding the gradient descent\n","\n","* Let's take a function,  $f(x) = x^{2} - x + 3$ \n","\n","* Task to find the minimum value of the function. It means that find the value of X where the value of Y is minimum."]},{"cell_type":"code","metadata":{"id":"Bufkr_n2yrKe"},"source":["# Keep all the given values of x into the function and see the output values of y.\n","def func(x):\n","    return  x**2 -x + 3\n","\n","X = np.array([-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7]) # Input values\n","Y = func(X) # Call the function to get the y values\n","\n","print(\"X values: \" , X)\n","print(\"f(x) values: \\n\", Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RnaLCsM1Qtz"},"source":["Now let's plot the function $f(x) = x^{2} - x + 3$  for the given values of X"]},{"cell_type":"code","metadata":{"id":"JZ_5xJTR1m9h"},"source":["# Plotting x and y values\n","plt.plot(X, Y, marker='o',color='b',linestyle='-');\n","plt.plot(X, Y, 'o', color='r');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lp5no7O9A0zI"},"source":["## Gradient Descent \n","\n","It is an iterative optimization algorithm that finds the minimum value of a function. In this function the minimum value occurs at X = 0.5\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9MGhWQICrK4h"},"source":["### How to converge the gradient ?\n","\n","* Where to start ?\n","\n","* In which direction to move?\n","\n","* How to reach the minimum point?\n","\n","\n","The general idea is to start with a random starting point X, the Gradient of a function gives always the direction of greatest rate of increase, and move towards the negative direction which minimizes the value of the function.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DdTIiiflH6iX"},"source":["### The steps of the Gradient Descent algorithm \n","\n","1. Compute the gradient of the function (first order derivative of the function)\n","\n","2. Start from the random point by choosing learning rate ($\\eta$) and the no.of iterations\n","\n","3. Update the gradient and move towards negative slope to reach the minimum point.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hMUYPmgSSNlh"},"source":["The **learning rate ($\\eta$)** is a parameter which  influences the convergence of the algorithm. Larger learning rates make the algorithm take huge steps down the slope and it might jump across the minimum point thereby missing it. A low learning rate is more precise but calculating the gradient is time-consuming\n"]},{"cell_type":"code","source":["def f_prime(x1):\n","  var = np.poly1d([1,-1,3])\n","  derivative = var.deriv()\n","  f_derivative = derivative(x1)   \n","  return f_derivative"],"metadata":{"id":"QDwOGPdczsBx"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"11-C1DsL1-io"},"source":["# Below is the function to converge the gradient\n","def gradient_converge(eta , gradient_x, iterations):\n","  for i in range(0,iterations):\n","      \n","      # The derivative is the rate of change or the slope of a function at a given point\n","      deriv_x = f_prime(gradient_x)\n","      # Calculating the gradient_x = x - eta * f'(x)  \n","      gradient_x = gradient_x - eta * deriv_x \n","      \n","  return gradient_x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DEw2JGD91TBd"},"source":["Set the parameters for the gradient converge with iterations = 200, eta = 0.001"]},{"cell_type":"code","metadata":{"id":"2nYtA_Dr0-xo"},"source":["# Specify the number of iterations for the process to repeat.\n","iterations = 200\n","\n","# Set an initial value, to start with\n","Initial_point = 6\n","\n","# Learning rate  \n","eta = 0.001         "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["6-0.001*200"],"metadata":{"id":"u3G3IOk7Iz2_"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SeXO4jUM1GRq"},"source":["# Calling the gradient_converge() function\n","gradient_x = gradient_converge(eta, Initial_point, iterations)\n","print(\"iterations = \",iterations,\"\\ngradient_x = \",gradient_x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v3U-9ge4sMDX"},"source":["Visualization of the Gradient Convergence for the above parameters"]},{"cell_type":"code","metadata":{"id":"-jvqRt6Tq31f"},"source":["plt.plot(X, Y, marker='o',color='b',linestyle='-');\n","plt.plot(X, Y, 'o', color='r');\n","y_gradient = func(gradient_x)\n","plt.plot(gradient_x, y_gradient,'ko',  markersize = 10)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-dzU8Nu-Nv0"},"source":["From the above graph, with iterations = 200, eta = 0.001 the gradient has moved down to point 4, which is not the minimum point."]},{"cell_type":"markdown","metadata":{"id":"u-6ystdJ9BPk"},"source":["Now let's try changing the parameters for the gradient converge with, iterations = 9000, eta = 0.001 "]},{"cell_type":"code","metadata":{"id":"dysw7KeH9BPu"},"source":["# Change the no.of iterations to 9000\n","iterations = 9000\n","\n","# Start point\n","Initial_point = 6 \n","\n","# Learning rate\n","eta = 0.001 \n","\n","# Calling the gradient_converge() function\n","gradient_x = gradient_converge(eta, Initial_point, iterations)\n","print(\"iterations = \",iterations,\"\\ngradient_x = \",gradient_x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVmIPLWc97tE"},"source":["Visualize the Gradient Convergence for the changed parameters"]},{"cell_type":"code","metadata":{"id":"nL9OP4pq9fpk"},"source":["plt.plot(X, Y, marker='o',color='b',linestyle='-');\n","plt.plot(X, Y, 'o', color='r');\n","y_gradient = func(gradient_x)\n","plt.plot(gradient_x, y_gradient,'ko',  markersize = 10)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gb6jKI6Br1M"},"source":["From the above graph, observe that after 9000 iterations the gradient reaches the minimum point in the plot which is very close to 0.5"]},{"cell_type":"markdown","metadata":{"id":"4YlpByIaDqcf"},"source":["Also try with learning rate ($\\eta$) = 0.01 and observe how the gradient converges quickly within 1000 iteration and reaches the minimum point"]}]}